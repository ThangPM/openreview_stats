<ul class="list-unstyled submissions-list">
    <li class="note " data-id="6nblryHxVbO" data-number="316">
        <h4>
          <a href="/forum?id=6nblryHxVbO">
              WildfireDB: An Open-Source Dataset Connecting Wildfire Occurrence with Relevant Determinants
          </a>


            <a href="/pdf?id=6nblryHxVbO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Samriddhi_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samriddhi_Singla1">Samriddhi Singla</a>, <a href="/profile?id=~Ayan_Mukhopadhyay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ayan_Mukhopadhyay1">Ayan Mukhopadhyay</a>, <a href="/profile?email=michael.p.wilbur%40vanderbilt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.p.wilbur@vanderbilt.edu">Michael Wilbur</a>, <a href="/profile?email=tdiao%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tdiao@stanford.edu">Tina Diao</a>, <a href="/profile?email=vgajj002%40ucr.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="vgajj002@ucr.edu">Vinayak Gajjewar</a>, <a href="/profile?id=~Ahmed_Eldawy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Eldawy1">Ahmed Eldawy</a>, <a href="/profile?id=~Mykel_Kochenderfer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mykel_Kochenderfer1">Mykel Kochenderfer</a>, <a href="/profile?id=~Ross_D_Shachter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ross_D_Shachter1">Ross D Shachter</a>, <a href="/profile?id=~Abhishek_Dubey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Dubey1">Abhishek Dubey</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">24 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#6nblryHxVbO-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6nblryHxVbO-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">wildfire, open-source data, data-driven modeling, disaster management, emergency response</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An open-source dataset that links wildfire occurences with relevant covariates</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Modeling fire spread is critical in fire risk management. Creating data-driven models to forecast spread remains challenging due to the lack of comprehensive data sources that relate fires with relevant covariates. We present the first comprehensive and open-source dataset that relates historical fire data with relevant covariates such as weather, vegetation, and topography. Our dataset, named WildfireDB, contains over 17 million data points that capture how fires spread in continental USA in the last decade. In this paper, we describe the algorithmic approach used to process and integrate the data, describe the dataset, and present benchmark results regarding data-driven models that can be learned to forecast the spread of wildfires.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=6nblryHxVbO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://wildfire-modeling.github.io/" target="_blank" rel="nofollow noreferrer">https://wildfire-modeling.github.io/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="tyn3MYS_uDT" data-number="28">
        <h4>
          <a href="/forum?id=tyn3MYS_uDT">
              Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation
          </a>


            <a href="/pdf?id=tyn3MYS_uDT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuta_Saito1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuta_Saito1">Yuta Saito</a>, <a href="/profile?email=shunsuke.aihara%40zozo.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shunsuke.aihara@zozo.com">Shunsuke Aihara</a>, <a href="/profile?email=megumi.matsutani%40zozo.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="megumi.matsutani@zozo.com">Megumi Matsutani</a>, <a href="/profile?id=~Yusuke_Narita2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusuke_Narita2">Yusuke Narita</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">15 Aug 2021 (modified: 29 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#tyn3MYS_uDT-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tyn3MYS_uDT-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">off-policy evaluation, real-world dataset, open-source software, benchmark experiments, offline contextual bandits</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Large-scale public real dataset and open-source software to enable realistic and reproducible experiments and implementations of off-policy evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">\textit{Off-policy evaluation} (OPE) aims to estimate the performance of hypothetical policies using data generated by a different policy. Because of its huge potential impact in practice, there has been growing research interest in this field. There is, however, no real-world public dataset that enables the evaluation of OPE, making its experimental studies unrealistic and irreproducible. With the goal of enabling realistic and reproducible OPE research, we present \textit{Open Bandit Dataset}, a public logged bandit dataset collected on a large-scale fashion e-commerce platform, ZOZOTOWN. Our dataset is unique in that it contains a set of \textit{multiple} logged bandit datasets collected by running different policies on the same platform. This enables experimental comparisons of different OPE estimators for the first time. We also develop Python software called \textit{Open Bandit Pipeline} to streamline and standardize the implementation of batch bandit algorithms and OPE. Our open data and software will contribute to fair and transparent OPE research and help the community identify fruitful research directions. We provide extensive benchmark experiments of existing OPE estimators using our dataset and software. The results open up essential challenges and new avenues for future OPE research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=tyn3MYS_uDT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Public Real-World Dataset: https://research.zozo.com/data.html /  Open-Source Software (Open Bandit Pipeline): https://github.com/st-tech/zr-obp</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://research.zozo.com/data.html" target="_blank" rel="nofollow noreferrer">https://research.zozo.com/data.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is licensed under CC BY 4.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="vzb0f0TIVlI" data-number="22">
        <h4>
          <a href="/forum?id=vzb0f0TIVlI">
              A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer
          </a>


            <a href="/pdf?id=vzb0f0TIVlI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Weijia_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weijia_Wu2">Weijia Wu</a>, <a href="/profile?id=~Debing_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Debing_Zhang1">Debing Zhang</a>, <a href="/profile?id=~Yuanqiang_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanqiang_Cai1">Yuanqiang Cai</a>, <a href="/profile?id=~Sibo_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sibo_Wang2">Sibo Wang</a>, <a href="/profile?id=~Jiahong_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiahong_Li3">Jiahong Li</a>, <a href="/profile?id=~Zhuang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuang_Li2">Zhuang Li</a>, <a href="/profile?id=~Yejun_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yejun_Tang1">Yejun Tang</a>, <a href="/profile?id=~Hong_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hong_Zhou3">Hong Zhou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">13 Aug 2021 (modified: 28 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#vzb0f0TIVlI-details-834" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vzb0f0TIVlI-details-834"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">video text spotting, text detection and recognition</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Multilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most existing video text spotting benchmarks focus on evaluating a single language and scenario with limited data. In this work, we introduce a large-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There are four features for BOVText. Firstly, we provide 1,850+ videos with more than 1,600,000+ frames, 25 times larger than the existing largest dataset with incidental text in videos. Secondly, our dataset covers 30+ open categories with a wide selection of various scenarios, Life Vlog, Driving, Movie, etc. Thirdly, abundant text types annotation (i.e., title, caption, or scene text) are provided for the different representational meanings in the video. Fourthly, the MOVText provides multilingual text annotation to promote multiple cultures' live and communication.  Besides, we propose an end-to-end video text spotting framework with Transformer, termed TransVTSpotter, which solves the multi-orient text spotting in video with a simple, but efficient attention-based query-key mechanism. It applies object features from the previous frame as a tracking query for the current frame and introduces a rotation angle prediction to fit the multi-orient text instance. On ICDAR2015(video), TransVTSpotter achieves state-of-the-art performance with 44.2% MOTA, 13 fps. The dataset and code of TransVTSpotter can be found at https://github.com/weijiawu/BOVText-Benchmark and https://github.com/weijiawu/TransVTSpotter, respectively.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=vzb0f0TIVlI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://github.com/weijiawu/BOVText-Benchmark for the benchmark(BOVText), and https://github.com/weijiawu/TransVTSpotter for the proposed method(TransVTSpotter).</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">the URL of the dataset: https://github.com/weijiawu/BOVText-Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The released video dataset includes two parts: 1,494 videos from KuaiShou and 356 videos from YouTube. For those videos from KuaiShou, we mask the private information such as the human face, which has passed the examination of the legal department and copyright department of KuaiShou corporation. Thus, we own the copyright for these videos. For those videos from YouTube, to the best of our knowledge at the time of download, we have exercised caution to download only those videos that were available on YouTube with a Creative Commmons CC-BY (v3.0) License. We don't own the copyright of those videos and provide them for non-commercial research purposes only. All data in our project is open source under CC-by 4.0 license and only be used for research purposes.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bLBIbVaGDu" data-number="26">
        <h4>
          <a href="/forum?id=bLBIbVaGDu">
              LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation
          </a>


            <a href="/pdf?id=bLBIbVaGDu" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Junjue_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junjue_Wang3">Junjue Wang</a>, <a href="/profile?id=~Zhuo_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuo_Zheng1">Zhuo Zheng</a>, <a href="/profile?email=maailong007%40whu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="maailong007@whu.edu.cn">马爱龙</a>, <a href="/profile?id=~Xiaoyan_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyan_Lu1">Xiaoyan Lu</a>, <a href="/profile?id=~Yanfei_Zhong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanfei_Zhong1">Yanfei Zhong</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">14 Aug 2021 (modified: 12 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#bLBIbVaGDu-details-168" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bLBIbVaGDu-details-168"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Land-cover mapping, Unsupervised domain adaptation, Semantic segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A remote sensing land-cover domain adaptive semantic segmentation dataset is proposed with three considerable challenges in large-scale mapping: multi-scale objects, complex background samples, and inconsistent class distributions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets mainly promote the research of learning semantic representation, thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) dataset to advance semantic and transferable learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated objects from three different cities. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the:  1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on eleven semantic segmentation methods and eight UDA methods. Some exploratory studies including multi-scale architectures and strategies, additional background supervision, and pseudo-label analysis were also carried out to address these challenges. The code and data are available at https://github.com/Junjue-Wang/LoveDA.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bLBIbVaGDu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">The code and data are available at https://github.com/Junjue-Wang/LoveDA.</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Junjue-Wang/LoveDA" target="_blank" rel="nofollow noreferrer">https://github.com/Junjue-Wang/LoveDA</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The owners of the data and of the copyright on the data are RSIDEA, Wuhan University. Use of the Google Earth images must respect the "Google Earth" terms of use. All images and their associated annotations in LoveDA can be used for academic purposes only, and any commercial use is prohibited. (CC BY-NC-SA 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Fkpr2RYDvI1" data-number="133">
        <h4>
          <a href="/forum?id=Fkpr2RYDvI1">
              SynthBio: A Case Study in Faster Curation of Text Datasets
          </a>


            <a href="/pdf?id=Fkpr2RYDvI1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ann_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ann_Yuan1">Ann Yuan</a>, <a href="/profile?id=~Daphne_Ippolito1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daphne_Ippolito1">Daphne Ippolito</a>, <a href="/profile?email=vitalyn%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vitalyn@google.com">Vitaly Nikolaev</a>, <a href="/profile?id=~Chris_Callison-Burch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Callison-Burch1">Chris Callison-Burch</a>, <a href="/profile?id=~Andy_Coenen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andy_Coenen1">Andy Coenen</a>, <a href="/profile?id=~Sebastian_Gehrmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Gehrmann1">Sebastian Gehrmann</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#Fkpr2RYDvI1-details-693" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Fkpr2RYDvI1-details-693"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">NLP, machine learning, datasets, NLG</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset curation method in which a language model generates seed text which human raters edit - and use the method to curate SynthBio - a new structure-to-text dataset of biographies describing fictional individuals.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">NLP researchers need more, higher-quality text datasets. Human-labeled datasets are expensive to collect, while datasets collected via automatic retrieval from the web such as WikiBio [Lebret 2016] are noisy and can include undesired biases. Moreover, data sourced from the web is often included in datasets used to pretrain models, leading to inadvertent cross-contamination of training and test sets. In this work we introduce a novel method for efficient dataset curation: we use a large language model to provide seed generations to human raters, thereby changing dataset authoring from a writing task to an editing task. We use our method to curate SynthBio - a new evaluation set for WikiBio - comprised of structured attribute lists describing fictional individuals, mapped to natural language biographies. We show that our dataset of fictional biographies is less noisy than WikiBio, and also more balanced with respect to gender and nationality.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Fkpr2RYDvI1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://storage.googleapis.com/gem-benchmark/SynthBio.json" target="_blank" rel="nofollow noreferrer">https://storage.googleapis.com/gem-benchmark/SynthBio.json</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://storage.googleapis.com/gem-benchmark/SynthBio.json" target="_blank" rel="nofollow noreferrer">https://storage.googleapis.com/gem-benchmark/SynthBio.json</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bgWHz41FMB7" data-number="214">
        <h4>
          <a href="/forum?id=bgWHz41FMB7">
              RAFT: A Real-World Few-Shot Text Classification Benchmark
          </a>


            <a href="/pdf?id=bgWHz41FMB7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Neel_Alex1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neel_Alex1">Neel Alex</a>, <a href="/profile?id=~Eli_Lifland1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eli_Lifland1">Eli Lifland</a>, <a href="/profile?email=lewis%40huggingface.co" class="profile-link" data-toggle="tooltip" data-placement="top" title="lewis@huggingface.co">Lewis Tunstall</a>, <a href="/profile?email=abhishek%40huggingface.co" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhishek@huggingface.co">Abhishek Thakur</a>, <a href="/profile?email=pmaham%40stiftung-nv.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="pmaham@stiftung-nv.de">Pegah Maham</a>, <a href="/profile?email=jessriedel%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jessriedel@gmail.com">C. Jess Riedel</a>, <a href="/profile?email=emmie.e.hine%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="emmie.e.hine@gmail.com">Emmie Hine</a>, <a href="/profile?id=~Carolyn_Ashurst1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carolyn_Ashurst1">Carolyn Ashurst</a>, <a href="/profile?email=paul.sedille%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="paul.sedille@gmail.com">Paul Sedille</a>, <a href="/profile?email=alexis.carlier%40governance.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="alexis.carlier@governance.ai">Alexis Carlier</a>, <a href="/profile?email=michael.noetel%40acu.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.noetel@acu.edu.au">Michael Noetel</a>, <a href="/profile?id=~Andreas_Stuhlm%C3%BCller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Stuhlmüller1">Andreas Stuhlmüller</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#bgWHz41FMB7-details-357" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bgWHz41FMB7-details-357"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bgWHz41FMB7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://raft.elicit.org/" target="_blank" rel="nofollow noreferrer">https://raft.elicit.org/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="K7ke_GZ_6N" data-number="130">
        <h4>
          <a href="/forum?id=K7ke_GZ_6N">
              Artsheets for Art Datasets
          </a>


            <a href="/pdf?id=K7ke_GZ_6N" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ramya_Srinivasan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ramya_Srinivasan1">Ramya Srinivasan</a>, <a href="/profile?id=~Emily_Denton2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Denton2">Emily Denton</a>, <a href="/profile?id=~Jordan_Famularo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jordan_Famularo1">Jordan Famularo</a>, <a href="/profile?id=~Negar_Rostamzadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Negar_Rostamzadeh1">Negar Rostamzadeh</a>, <a href="/profile?id=~Fernando_Diaz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fernando_Diaz1">Fernando Diaz</a>, <a href="/profile?id=~Beth_Coleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beth_Coleman1">Beth Coleman</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 30 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#K7ke_GZ_6N-details-970" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="K7ke_GZ_6N-details-970"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">art datasets, checklists, questionnaire, dataset curation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Checklists for art datasets</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning (ML) techniques are increasingly being employed within a variety of creative domains. For example, ML tools are being used to analyze the authenticity of artworks, to simulate artistic styles, and to augment human creative processes. While this progress has opened up new creative avenues, it has also paved the way for adverse downstream effects such as cultural appropriation (e.g., cultural misrepresentation, offense, and undervaluing) and representational harm. Many such concerning issues stem from the training data in ways that diligent evaluation can uncover, prevent, and mitigate. We posit that, when developing an arts-based dataset, it is essential to consider the social factors that influenced the process of conception and design, and the resulting gaps must be examined in order to maximize understanding of the dataset's meaning and future impact. Each dataset creator's decision produces opportunities, but also omissions. Each choice, moreover, builds on preexisting histories of the data's formation and handling across time by prior actors including, but not limited to, art collectors, galleries, libraries, archives, museums, and digital repositories. To illuminate the aforementioned aspects, we provide a checklist of questions customized for use with art datasets in order to help guide assessment of the ways that dataset design may either perpetuate or shift exclusions found in repositories of art data. The checklist is organized to address the dataset creator's motivation together with dataset provenance, composition, collection, pre-processing, cleaning, labeling, use (including data generation), distribution, and maintenance. Two case studies exemplify the value and application of our questionnaire.</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KVMS3fl4Rsv" data-number="213">
        <h4>
          <a href="/forum?id=KVMS3fl4Rsv">
              Neural Latents Benchmark ‘21: Evaluating latent variable models of neural population activity
          </a>


            <a href="/pdf?id=KVMS3fl4Rsv" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Felix_C_Pei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_C_Pei1">Felix C Pei</a>, <a href="/profile?id=~Joel_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joel_Ye1">Joel Ye</a>, <a href="/profile?id=~David_M._Zoltowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_M._Zoltowski1">David M. Zoltowski</a>, <a href="/profile?id=~Anqi_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anqi_Wu2">Anqi Wu</a>, <a href="/profile?email=raeed.chowdhury%40pitt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="raeed.chowdhury@pitt.edu">Raeed Hasan Chowdhury</a>, <a href="/profile?id=~Hansem_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hansem_Sohn1">Hansem Sohn</a>, <a href="/profile?id=~Joseph_E_O%26%23x27%3BDoherty1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_E_O'Doherty1">Joseph E O'Doherty</a>, <a href="/profile?id=~Krishna_V._Shenoy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krishna_V._Shenoy1">Krishna V. Shenoy</a>, <a href="/profile?id=~Matthew_Kaufman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Kaufman1">Matthew Kaufman</a>, <a href="/profile?id=~Mark_M_Churchland1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_M_Churchland1">Mark M Churchland</a>, <a href="/profile?email=mjaz%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mjaz@mit.edu">Mehrdad Jazayeri</a>, <a href="/profile?id=~Lee_E._Miller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lee_E._Miller1">Lee E. Miller</a>, <a href="/profile?id=~Jonathan_W._Pillow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_W._Pillow1">Jonathan W. Pillow</a>, <a href="/profile?id=~Il_Memming_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Il_Memming_Park1">Il Memming Park</a>, <a href="/profile?id=~Eva_L_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eva_L_Dyer1">Eva L Dyer</a>, <a href="/profile?id=~Chethan_Pandarinath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chethan_Pandarinath1">Chethan Pandarinath</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#KVMS3fl4Rsv-details-966" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KVMS3fl4Rsv-details-966"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmark, computational neuroscience, latent variable models</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a benchmark for evaluating latent variable models across the brain.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external experimental variables. However, progress with LVMs for neuronal population activity is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce a benchmark suite for latent variable modeling of neural population activity. We curate four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas. We identify unsupervised evaluation as a common framework for evaluating models across datasets, and apply several baselines that demonstrate the variety of the benchmarked datasets. We release this benchmark through EvalAI. http://neurallatents.github.io</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KVMS3fl4Rsv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://neurallatents.github.io/" target="_blank" rel="nofollow noreferrer">https://neurallatents.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://neurallatents.github.io/" target="_blank" rel="nofollow noreferrer">https://neurallatents.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Data license: CC-BY-4.0. Code license: MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="G1muTb5zuO7" data-number="270">
        <h4>
          <a href="/forum?id=G1muTb5zuO7">
              What Would Jiminy Cricket Do? Towards Agents That Behave Morally
          </a>


            <a href="/pdf?id=G1muTb5zuO7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>, <a href="/profile?id=~Mantas_Mazeika3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mantas_Mazeika3">Mantas Mazeika</a>, <a href="/profile?id=~Andy_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andy_Zou1">Andy Zou</a>, <a href="/profile?email=sahil.patelsp%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sahil.patelsp@berkeley.edu">Sahil Patel</a>, <a href="/profile?email=czhu43%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="czhu43@berkeley.edu">Christine Zhu</a>, <a href="/profile?email=navjesus%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="navjesus@berkeley.edu">Jesus Navarro</a>, <a href="/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>, <a href="/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li19">Bo Li</a>, <a href="/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 08 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#G1muTb5zuO7-details-210" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="G1muTb5zuO7-details-210"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Transformers, RL, data bias, reward bias, machine ethics, value learning, safe exploration</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a benchmark for evaluating the moral behavior of artificial agents in 25 semantically rich text-based environments and show how commonsense understanding in language models can encourage moral behavior.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong, to behave morally. By contrast, artificial agents may behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, mitigating inherited biases towards immoral behavior will become necessary. However, prior work on aligning agents with human values and morals focuses on small-scale settings lacking in semantic complexity. To enable research in larger, more realistic settings, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of semantically rich, morally salient scenarios. Via dense annotations for every possible action, Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. To improve moral behavior, we leverage language models with commonsense moral knowledge and develop strategies to mediate this knowledge into actions. In extensive experiments, we find that our artificial conscience approach can steer agents towards moral behavior without sacrificing performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=G1muTb5zuO7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hendrycks/jiminy-cricket" target="_blank" rel="nofollow noreferrer">https://github.com/hendrycks/jiminy-cricket</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NfTU-wN8Uo" data-number="104">
        <h4>
          <a href="/forum?id=NfTU-wN8Uo">
              <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="30" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-ty"><mjx-c class="mjx-c1D681 TEX-T"></mjx-c><mjx-c class="mjx-c1D67F TEX-T"></mjx-c><mjx-c class="mjx-c2D TEX-T"></mjx-c><mjx-c class="mjx-c1D67C TEX-T"></mjx-c><mjx-c class="mjx-c1D698 TEX-T"></mjx-c><mjx-c class="mjx-c1D68D TEX-T"></mjx-c></mjx-mtext><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mi class="mjx-n"><mjx-c class="mjx-c26"></mjx-c></mjx-mi><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mtext class="mjx-ty"><mjx-c class="mjx-c1D681 TEX-T"></mjx-c><mjx-c class="mjx-c1D67F TEX-T"></mjx-c><mjx-c class="mjx-c2D TEX-T"></mjx-c><mjx-c class="mjx-c1D672 TEX-T"></mjx-c><mjx-c class="mjx-c1D69B TEX-T"></mjx-c><mjx-c class="mjx-c1D698 TEX-T"></mjx-c><mjx-c class="mjx-c1D6A0 TEX-T"></mjx-c><mjx-c class="mjx-c1D68D TEX-T"></mjx-c><mjx-c class="mjx-c3A TEX-T"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="monospace">RP-Mod</mtext><mtext>&nbsp;</mtext><mi mathvariant="normal">&amp;</mi><mtext>&nbsp;</mtext><mtext mathvariant="monospace">RP-Crowd:</mtext></math></mjx-assistive-mml></mjx-container> Moderator- and Crowd-Annotated German News Comment Datasets
          </a>


            <a href="/pdf?id=NfTU-wN8Uo" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dennis_Assenmacher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dennis_Assenmacher1">Dennis Assenmacher</a>, <a href="/profile?id=~Marco_Niemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marco_Niemann1">Marco Niemann</a>, <a href="/profile?id=~Kilian_M%C3%BCller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kilian_Müller1">Kilian Müller</a>, <a href="/profile?id=~Moritz_Seiler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moritz_Seiler1">Moritz Seiler</a>, <a href="/profile?id=~Dennis_M_Riehle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dennis_M_Riehle1">Dennis M Riehle</a>, <a href="/profile?id=~Heike_Trautmann2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heike_Trautmann2">Heike Trautmann</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">20 Replies</span>


        </div>

          <a href="#NfTU-wN8Uo-details-904" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NfTU-wN8Uo-details-904"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Abusive Language Detection, Newspaper, Comment Moderation, Crowd Study, NLP</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Introducing the largest annotated German dataset for comment moderation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Abuse and hate are penetrating social media and many comment sections of news media companies. To prevent losing readers who get appalled by inappropriate texts, these platform providers invest considerable efforts to moderate user-generated contributions. This is further enforced by legislative actions, which make non-clearance of these comments a punishable action. While (semi-)automated solutions using Natural Language Processing and advanced Machine Learning techniques are getting increasingly sophisticated, the domain of abusive language detection still struggles as large non-English and well-curated datasets are scarce or not publicly available. With this work, we publish and analyse the largest annotated German abusive language comment datasets to date. In contrast to existing datasets, we achieve a high labeling standard by conducting a thorough crowd-based annotation study that complements professional moderators' decisions, which are also included in the dataset. We compare and cross-evaluate the performance of baseline algorithms and state-of-the-art transformer-based language models, which are fine-tuned on our datasets and an existing alternative, showing the usefulness for the community.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NfTU-wN8Uo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5291339" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5291339</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5242915" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5242915</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution Non Commercial Share Alike 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="701FtuyLlAd" data-number="319">
        <h4>
          <a href="/forum?id=701FtuyLlAd">
              FS-Mol: A Few-Shot Learning Dataset of Molecules
          </a>


            <a href="/pdf?id=701FtuyLlAd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Megan_Stanley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Megan_Stanley1">Megan Stanley</a>, <a href="/profile?id=~John_F_Bronskill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_F_Bronskill1">John F Bronskill</a>, <a href="/profile?id=~Krzysztof_Maziarz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krzysztof_Maziarz1">Krzysztof Maziarz</a>, <a href="/profile?id=~Hubert_Misztela1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hubert_Misztela1">Hubert Misztela</a>, <a href="/profile?id=~Jessica_Lanini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jessica_Lanini1">Jessica Lanini</a>, <a href="/profile?id=~Marwin_Segler2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marwin_Segler2">Marwin Segler</a>, <a href="/profile?id=~Nadine_Schneider1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nadine_Schneider1">Nadine Schneider</a>, <a href="/profile?id=~Marc_Brockschmidt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Brockschmidt1">Marc Brockschmidt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#701FtuyLlAd-details-883" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="701FtuyLlAd-details-883"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Few-shot learning, Meta-learning, Molecular Data, GNNs, QSAR, Prototypical Networks, Drug-Discovery</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present FS-Mol, an up-to-date molecular dataset and benchmarking system with reference baselines, to enable and inspire few-shot learning method development in an important domain outside of computer vision and NLP. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Small datasets are ubiquitous in drug discovery as data generation is expensive and can be restricted for ethical reasons (e.g. in vivo experiments). A widely applied technique in early drug discovery to identify novel active molecules against a protein target is modelling quantitative structure-activity relationships (QSAR). It is known to be extremely challenging, as available measurements of compound activities range in the low dozens or hundreds. However, many such related datasets exist, each with a small number of datapoints, opening up the opportunity for few-shot learning after pre-training on a substantially larger corpus of data. At the same time, many few-shot learning methods are currently evaluated in the computer-vision domain. We propose that expansion into a new application, as well as the possibility to use explicitly graph-structured data, will drive exciting progress in few-shot learning. Here, we provide a few-shot learning dataset (FS-Mol) and complementary benchmarking procedure. We define a set of tasks on which few-shot learning methods can be evaluated, with a separate set of tasks for use in pre-training. In addition, we implement and evaluate a number of existing single-task, multi-task, and meta-learning approaches as baselines for the community. We hope that our dataset, support code release, and baselines will encourage future work on this extremely challenging new domain for few-shot learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=701FtuyLlAd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/microsoft/FS-Mol" target="_blank" rel="nofollow noreferrer">https://github.com/microsoft/FS-Mol</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jI_BbL-qjJN" data-number="139">
        <h4>
          <a href="/forum?id=jI_BbL-qjJN">
              An Information Retrieval Approach to Building Datasets for Hate Speech Detection
          </a>


            <a href="/pdf?id=jI_BbL-qjJN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Md_Mustafizur_Rahman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Md_Mustafizur_Rahman1">Md Mustafizur Rahman</a>, <a href="/profile?email=dinesh.k.balakrishnan%40utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dinesh.k.balakrishnan@utexas.edu">Dinesh Balakrishnan</a>, <a href="/profile?id=~Dhiraj_Murthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dhiraj_Murthy1">Dhiraj Murthy</a>, <a href="/profile?id=~Mucahid_Kutlu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mucahid_Kutlu1">Mucahid Kutlu</a>, <a href="/profile?id=~Matthew_Lease1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Lease1">Matthew Lease</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#jI_BbL-qjJN-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jI_BbL-qjJN-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hate Speech, Test Collections, Pooling, Active Learning, Rationales</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building a benchmark dataset for hate speech detection presents various challenges. Firstly, because hate speech is relatively rare, random sampling of tweets to annotate is very inefficient in finding hate speech. To address this, prior datasets often include only tweets matching known ``hate words''. However, restricting data to a pre-defined vocabulary may exclude portions of the real-world phenomenon we seek to model. A second challenge is that definitions of hate speech tend to be highly varying and subjective. Annotators having diverse prior notions of hate speech may not only disagree with one another but also struggle to conform to specified labeling guidelines. Our key insight is that the rarity and subjectivity of hate speech are akin to that of relevance in information retrieval (IR). This connection suggests that well-established methodologies for creating IR test collections can be usefully applied to create better benchmark datasets for hate speech. To intelligently and efficiently select which tweets to annotate, we apply standard IR techniques of {\em pooling} and {\em active learning}. To improve both consistency and value of annotations, we apply {\em task decomposition} and {\em annotator rationale} techniques. We share a new benchmark dataset for hate speech detection on Twitter that provides broader coverage of hate than prior datasets. We also show a dramatic drop in accuracy of existing detection models when tested on these broader forms of hate. Annotator rationales we collect not only justify labeling decisions but also enable future work opportunities for dual-supervision and/or explanation generation in modeling. Further details of our approach can be found in the supplementary materials \cite{rahman21-neurips21-supplementary}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jI_BbL-qjJN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mdmustafizurrahman/An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection" target="_blank" rel="nofollow noreferrer">https://github.com/mdmustafizurrahman/An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wCu6T5xFjeJ" data-number="105">
        <h4>
          <a href="/forum?id=wCu6T5xFjeJ">
              BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models
          </a>


            <a href="/pdf?id=wCu6T5xFjeJ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nandan_Thakur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nandan_Thakur1">Nandan Thakur</a>, <a href="/profile?id=~Nils_Reimers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nils_Reimers1">Nils Reimers</a>, <a href="/profile?id=~Andreas_R%C3%BCckl%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Rücklé1">Andreas Rücklé</a>, <a href="/profile?email=abhesrivastava%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhesrivastava@gmail.com">Abhishek Srivastava</a>, <a href="/profile?id=~Iryna_Gurevych1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iryna_Gurevych1">Iryna Gurevych</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#wCu6T5xFjeJ-details-658" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wCu6T5xFjeJ-details-658"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">information-retrieval, zero-shot, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel heterogeneous zero-shot retrieval benchmark containing 18 datasets from diverse text retrieval tasks and domains.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction, and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems and contributes to accelerating progress towards more robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wCu6T5xFjeJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/UKPLab/beir" target="_blank" rel="nofollow noreferrer">https://github.com/UKPLab/beir</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fgFBtYgJQX_" data-number="77">
        <h4>
          <a href="/forum?id=fgFBtYgJQX_">
              Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning
          </a>


            <a href="/pdf?id=fgFBtYgJQX_" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Viktor_Makoviychuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Viktor_Makoviychuk1">Viktor Makoviychuk</a>, <a href="/profile?email=lwawrzyniak%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lwawrzyniak@nvidia.com">Lukasz Wawrzyniak</a>, <a href="/profile?id=~Yunrong_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunrong_Guo1">Yunrong Guo</a>, <a href="/profile?email=michellel%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="michellel@nvidia.com">Michelle Lu</a>, <a href="/profile?email=kstorey%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kstorey@nvidia.com">Kier Storey</a>, <a href="/profile?id=~Miles_Macklin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Miles_Macklin1">Miles Macklin</a>, <a href="/profile?id=~David_Hoeller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Hoeller1">David Hoeller</a>, <a href="/profile?id=~Nikita_Rudin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikita_Rudin1">Nikita Rudin</a>, <a href="/profile?id=~Arthur_Allshire1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Allshire1">Arthur Allshire</a>, <a href="/profile?id=~Ankur_Handa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankur_Handa1">Ankur Handa</a>, <a href="/profile?id=~Gavriel_State1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gavriel_State1">Gavriel State</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#fgFBtYgJQX_-details-697" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fgFBtYgJQX_-details-697"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Simulation, Physics Engine, Reinforcement Learning, Robot Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a new GPU based physics simulation for large scale high performance robot learning </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Isaac Gym offers a high-performance learning platform to train policies for a wide variety of robotics tasks entirely on GPU. Both physics simulation and neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU-based simulator and GPUs for neural networks. We host the results and videos at https://sites.google.com/view/isaacgym-nvidia and Isaac Gym can be downloaded at https://developer.nvidia.com/isaac-gym. The benchmark and environments are available at https://github.com/NVIDIA-Omniverse/IsaacGymEnvs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fgFBtYgJQX_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs" target="_blank" rel="nofollow noreferrer">https://github.com/NVIDIA-Omniverse/IsaacGymEnvs</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="TSvj5dmuSd" data-number="153">
        <h4>
          <a href="/forum?id=TSvj5dmuSd">
              Task Agnostic and Task Specific Self-Supervised Learning from Speech with LeBenchmark
          </a>


            <a href="/pdf?id=TSvj5dmuSd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=solene.evain%40univ-grenoble-alpes.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="solene.evain@univ-grenoble-alpes.fr">Solène Evain</a>, <a href="/profile?id=~Ha_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ha_Nguyen1">Ha Nguyen</a>, <a href="/profile?id=~Hang_Le1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Le1">Hang Le</a>, <a href="/profile?id=~Marcely_Zanon_Boito2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcely_Zanon_Boito2">Marcely Zanon Boito</a>, <a href="/profile?id=~Salima_Mdhaffar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Salima_Mdhaffar1">Salima Mdhaffar</a>, <a href="/profile?id=~Sina_Alisamir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sina_Alisamir1">Sina Alisamir</a>, <a href="/profile?email=ziyi.tong%40univ-grenoble-alpes.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="ziyi.tong@univ-grenoble-alpes.fr">Ziyi Tong</a>, <a href="/profile?id=~Natalia_Tomashenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Natalia_Tomashenko1">Natalia Tomashenko</a>, <a href="/profile?id=~Marco_Dinarelli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marco_Dinarelli1">Marco Dinarelli</a>, <a href="/profile?id=~Titouan_Parcollet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Titouan_Parcollet1">Titouan Parcollet</a>, <a href="/profile?id=~Alexandre_Allauzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_Allauzen1">Alexandre Allauzen</a>, <a href="/profile?id=~Yannick_Est%C3%A8ve1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yannick_Estève1">Yannick Estève</a>, <a href="/profile?id=~Benjamin_Lecouteux1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Lecouteux1">Benjamin Lecouteux</a>, <a href="/profile?id=~Fran%C3%A7ois_Portet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~François_Portet1">François Portet</a>, <a href="/profile?id=~Solange_Rossato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Solange_Rossato1">Solange Rossato</a>, <a href="/profile?id=~Fabien_Ringeval1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabien_Ringeval1">Fabien Ringeval</a>, <a href="/profile?email=didier.schwab%40univ-grenoble-alpes.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="didier.schwab@univ-grenoble-alpes.fr">Didier Schwab</a>, <a href="/profile?id=~laurent_besacier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~laurent_besacier1">laurent besacier</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#TSvj5dmuSd-details-690" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TSvj5dmuSd-details-690"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">French language, speech benchmark, ssl, self-supervised learning, asr, automatic speech recognition, slu, spoken language understanding, speech translation, speech-to-text, emotion recognition</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Self-Supervised Learning (SSL) has yielded remarkable improvements in many different domains including computer vision, natural language processing and speech processing by leveraging large amounts of unlabeled data. In the specific context of speech, however, and despite promising results, there exists a clear lack of standardization in the evaluation process for comprehensive comparisons of these models. This issue gets even worse with the investigation of SSL approaches for other languages than English. We present LeBenchmark, an open-source and reproducible framework for assessing SSL from French speech data. It includes documented, large-scale and heterogeneous corpora, seven pretrained SSL wav2vec 2.0 models shared with the community, and a clear evaluation protocol made of four downstream tasks along with their scoring scripts: automatic speech recognition, spoken language understanding, automatic speech translation and automatic emotion recognition. For the first time, SSL models are analyzed and compared on the latter domains both from a task-agnostic (i.e. frozen) and task-specific (i.e. fine-tuned w.r.t the downstream task) perspectives. We report state-of-the-art performance on most considered French tasks and provide a readable evaluation set-up for the development of future SSL models for speech processing.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=TSvj5dmuSd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://lebenchmark.com/" target="_blank" rel="nofollow noreferrer">http://lebenchmark.com/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">http://lebenchmark.com
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zQIvkXHS_U5" data-number="267">
        <h4>
          <a href="/forum?id=zQIvkXHS_U5">
              ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations
          </a>


            <a href="/pdf?id=zQIvkXHS_U5" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tongzhou_Mu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tongzhou_Mu1">Tongzhou Mu</a>, <a href="/profile?id=~Zhan_Ling2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhan_Ling2">Zhan Ling</a>, <a href="/profile?id=~Fanbo_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fanbo_Xiang1">Fanbo Xiang</a>, <a href="/profile?id=~Derek_Cathera_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Cathera_Yang1">Derek Cathera Yang</a>, <a href="/profile?id=~Xuanlin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanlin_Li1">Xuanlin Li</a>, <a href="/profile?id=~Stone_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stone_Tao1">Stone Tao</a>, <a href="/profile?id=~Zhiao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiao_Huang1">Zhiao Huang</a>, <a href="/profile?id=~Zhiwei_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwei_Jia1">Zhiwei Jia</a>, <a href="/profile?id=~Hao_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Su1">Hao Su</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 04 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#zQIvkXHS_U5-details-131" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zQIvkXHS_U5-details-131"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalization, object manipulation, 3D deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A benchmark for generalizable physics-rich manipulation skill with large-scale demonstrations.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions,  we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced (\href{https://github.com/haosulab/ManiSkill}{Github repo}), and a challenge facing interdisciplinary researchers will be held based on the benchmark. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zQIvkXHS_U5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/haosulab/ManiSkill" target="_blank" rel="nofollow noreferrer">https://github.com/haosulab/ManiSkill</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-or413Lh_aF" data-number="263">
        <h4>
          <a href="/forum?id=-or413Lh_aF">
              Benchmarking Data-driven Surrogate Simulators for Artificial Electromagnetic Materials
          </a>


            <a href="/pdf?id=-or413Lh_aF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yang_Deng3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Deng3">Yang Deng</a>, <a href="/profile?id=~Juncheng_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juncheng_Dong1">Juncheng Dong</a>, <a href="/profile?id=~Simiao_Ren2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simiao_Ren2">Simiao Ren</a>, <a href="/profile?id=~Omar_Khatib1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Omar_Khatib1">Omar Khatib</a>, <a href="/profile?id=~Mohammadreza_Soltani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammadreza_Soltani1">Mohammadreza Soltani</a>, <a href="/profile?id=~Vahid_Tarokh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vahid_Tarokh1">Vahid Tarokh</a>, <a href="/profile?id=~Willie_Padilla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Willie_Padilla1">Willie Padilla</a>, <a href="/profile?id=~Jordan_Malof1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jordan_Malof1">Jordan Malof</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#-or413Lh_aF-details-274" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-or413Lh_aF-details-274"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Artificial electromagnetic materials, Benchmark, MLP-mixer, Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial electromagnetic materials (AEMs), including metamaterials, derive their electromagnetic properties from geometry rather than chemistry. With the appropriate geometric design, AEMs have achieved exotic properties not realizable with conventional materials (e.g., cloaking or negative refractive index). However, understanding the relationship between the AEM structure and its properties is often poorly understood. While computational electromagnetic simulation (CEMS) may help design new AEMs, its use is limited due to its long computational time. Recently, it has been shown that deep learning can be an alternative solution to infer the relationship between an AEM geometry and its properties using a (relatively) small pool of CEMS data. However, the limited publicly released datasets and models and no widely-used benchmark for comparison have made using deep learning approaches even more difficult. Furthermore, configuring CEMS for a specific problem requires substantial expertise and time, making reproducibility challenging. Here, we develop a collection of three classes of AEM problems: metamaterials, nanophotonics, and color filter designs. We also publicly release software, allowing other researchers to conduct additional simulations for each system easily. Finally, we conduct experiments on our benchmark datasets with three recent neural network architectures: the multilayer perceptron (MLP), MLP-mixer, and transformer. We identify the methods and models that generalize best over the three problems to establish the best practice and baseline results upon which future research can build.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-or413Lh_aF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ydeng-MLM/ML_MM_Benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/ydeng-MLM/ML_MM_Benchmark</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FZBtIpEAb5J" data-number="256">
        <h4>
          <a href="/forum?id=FZBtIpEAb5J">
              ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models
          </a>


            <a href="/pdf?id=FZBtIpEAb5J" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Salva_R%C3%BChling_Cachay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Salva_Rühling_Cachay1">Salva Rühling Cachay</a>, <a href="/profile?id=~Venkatesh_Ramesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Venkatesh_Ramesh1">Venkatesh Ramesh</a>, <a href="/profile?email=jason.cole%40ec.gc.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="jason.cole@ec.gc.ca">Jason N. S. Cole</a>, <a href="/profile?email=howard.barker%40ec.gc.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="howard.barker@ec.gc.ca">Howard Barker</a>, <a href="/profile?id=~David_Rolnick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Rolnick1">David Rolnick</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 10 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#FZBtIpEAb5J-details-826" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FZBtIpEAb5J-details-826"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">climate modeling, atmospheric modeling, radiative transfer, climate change, emulators</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A comprehensive, large-scale dataset for benchmarking neural network emulators of the radiation component in climate and weather models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive.
          This has made them a popular target for neural network-based emulators.
          However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking.
          To fill this gap, we build a large dataset, ClimART, with more than 10 million samples from present, pre-industrial, and future climate conditions, based on the Canadian Earth System Model.
          ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed.
          We also present several novel baselines that indicate shortcomings of datasets and network architectures used in prior work.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FZBtIpEAb5J&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/RolnickLab/climart" target="_blank" rel="nofollow noreferrer">https://github.com/RolnickLab/climart</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/RolnickLab/climart" target="_blank" rel="nofollow noreferrer">https://github.com/RolnickLab/climart</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution 4.0 International (CC BY 4.0) </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="lwrPkQP_is" data-number="208">
        <h4>
          <a href="/forum?id=lwrPkQP_is">
              URLB: Unsupervised Reinforcement Learning Benchmark
          </a>


            <a href="/pdf?id=lwrPkQP_is" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Michael_Laskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Laskin1">Michael Laskin</a>, <a href="/profile?id=~Denis_Yarats1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denis_Yarats1">Denis Yarats</a>, <a href="/profile?id=~Hao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Liu1">Hao Liu</a>, <a href="/profile?id=~Kimin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kimin_Lee1">Kimin Lee</a>, <a href="/profile?id=~Albert_Zhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Zhan1">Albert Zhan</a>, <a href="/profile?id=~Kevin_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Lu2">Kevin Lu</a>, <a href="/profile?id=~Catherine_Cang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_Cang1">Catherine Cang</a>, <a href="/profile?id=~Lerrel_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lerrel_Pinto1">Lerrel Pinto</a>, <a href="/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 31 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">20 Replies</span>


        </div>

          <a href="#lwrPkQP_is-details-515" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lwrPkQP_is-details-515"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised learning, reinforcement learning, benchmark, open-source code</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a benchmark for Unsupervised Reinforcement Learning, open-source code for eight leading unsupervised RL methods, standardize pre-training &amp; evaluation, and benchmark across twelve downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep Reinforcement Learning (RL) has emerged as a powerful paradigm to solve a range of complex yet specific control tasks. Training generalist agents that can quickly adapt to new tasks remains an outstanding challenge. Recent advances in unsupervised RL have shown that pre-training RL agents with self-supervised intrinsic rewards can result in efficient adaptation. However, these algorithms have been hard to compare and develop due to the lack of a unified benchmark. To this end, we introduce the Unsupervised Reinforcement Learning Benchmark (URLB). URLB consists of two phases: reward-free pre-training and downstream task adaptation with extrinsic rewards. Building on the DeepMind Control Suite, we provide twelve continuous control tasks from three domains for evaluation and open-source code for eight leading unsupervised RL methods. We find that the implemented baselines make progress but are not able to solve URLB and propose directions for future research.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=lwrPkQP_is&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/rll-research/url_benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/rll-research/url_benchmark</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="aqCD8RINP54" data-number="150">
        <h4>
          <a href="/forum?id=aqCD8RINP54">
              RELLISUR: A Real Low-Light Image Super-Resolution Dataset
          </a>


            <a href="/pdf?id=aqCD8RINP54" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andreas_Aakerberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Aakerberg1">Andreas Aakerberg</a>, <a href="/profile?id=~Kamal_Nasrollahi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kamal_Nasrollahi1">Kamal Nasrollahi</a>, <a href="/profile?id=~Thomas_B._Moeslund1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_B._Moeslund1">Thomas B. Moeslund</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#aqCD8RINP54-details-429" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aqCD8RINP54-details-429"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Low-light enhancement, super-resolution, image processing, image enhancement, image restoration</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">RELLISUR: A new large-scale dataset for joint low-light image enhancement and super-resolution.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce RELLISUR, a novel dataset of real low-light low-resolution images paired with normal-light high-resolution reference image counterparts. With this dataset, we seek to fill the gap between low-light image enhancement and low-resolution image enhancement (Super-Resolution (SR)) which is currently only being addressed separately in the literature, even though the visibility of real-world images are often limited by both low-light and low-resolution. Part of the reason for this, is the lack of a large-scale dataset. To this end, we release a dataset with 12750 paired images of different resolutions and degrees of low-light illumination, to facilitate learning of deep-learning based models that can perform a direct mapping from degraded images with low visibility to sharp and detail rich images of high resolution. Additionally, we provide a benchmark of the existing methods for separate Low Light Enhancement (LLE) and SR on the proposed dataset along with experiments with joint LLE and SR. The latter shows that joint processing results in more accurate reconstructions with better perceptual quality compared to sequential processing of the images. With this, we confirm that the new RELLISUR dataset can be useful for future machine learning research aimed at solving simultaneous image LLE and SR.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=aqCD8RINP54&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5234969" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5234969</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5234969" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5234969</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="cXCZnLjDm4s" data-number="18">
        <h4>
          <a href="/forum?id=cXCZnLjDm4s">
              Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection
          </a>


            <a href="/pdf?id=cXCZnLjDm4s" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~John_Lambert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Lambert1">John Lambert</a>, <a href="/profile?id=~James_Hays1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Hays1">James Hays</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">12 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">19 Replies</span>


        </div>

          <a href="#cXCZnLjDm4s-details-473" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cXCZnLjDm4s-details-473"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">autonomous driving, hd maps, change detection, self-driving vehicles, scene understanding</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Strong baselines on the first dataset for high-definition (HD) map change detection demonstrate models trained on synthetically manipulated real-world maps can generalize to real-world distributions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">High-definition (HD) map change detection is the task of determining when sensor data and map data are no longer in agreement with one another due to real-world changes. We collect the first dataset for the task, which we entitle the Trust, but Verify (TbV) dataset, by mining thousands of hours of data from over 9 months of autonomous vehicle fleet operations. We present learning-based formulations for solving the problem in the bird's eye view and ego-view. Because real map changes are infrequent and vector maps are easy to synthetically manipulate, we lean on simulated data to train our model. Perhaps surprisingly, we show that such models can generalize to real world distributions. The dataset consists of maps and logs collected in six North American cities, is one of the largest AV datasets to date with more than 7.9 million images. We make the data available to the public, along with code and models under the the CC BY-NC-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=cXCZnLjDm4s&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.argoverse.org/" target="_blank" rel="nofollow noreferrer">https://www.argoverse.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.argoverse.org/" target="_blank" rel="nofollow noreferrer">https://www.argoverse.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uUa4jNMLjrL" data-number="317">
        <h4>
          <a href="/forum?id=uUa4jNMLjrL">
              DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring from space
          </a>


            <a href="/pdf?id=uUa4jNMLjrL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lukas_Kondmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Kondmann1">Lukas Kondmann</a>, <a href="/profile?id=~Aysim_Toker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aysim_Toker1">Aysim Toker</a>, <a href="/profile?id=~Marc_Ru%C3%9Fwurm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Rußwurm1">Marc Rußwurm</a>, <a href="/profile?id=~Andr%C3%A9s_Camero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrés_Camero1">Andrés Camero</a>, <a href="/profile?email=devis.peressutti%40sinergise.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="devis.peressutti@sinergise.com">Devis Peressuti</a>, <a href="/profile?email=grega.milcinski%40sinergise.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="grega.milcinski@sinergise.com">Grega Milcinski</a>, <a href="/profile?email=pierre.philippe.mathieu%40esa.int" class="profile-link" data-toggle="tooltip" data-placement="top" title="pierre.philippe.mathieu@esa.int">Pierre-Philippe Mathieu</a>, <a href="/profile?email=nicolas.longepe%40esa.int" class="profile-link" data-toggle="tooltip" data-placement="top" title="nicolas.longepe@esa.int">Nicolas Longépé</a>, <a href="/profile?email=tim.davis%40planet.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tim.davis@planet.com">Timothy Davis</a>, <a href="/profile?id=~Giovanni_Marchisio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giovanni_Marchisio1">Giovanni Marchisio</a>, <a href="/profile?id=~Laura_Leal-Taix%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laura_Leal-Taixé1">Laura Leal-Taixé</a>, <a href="/profile?id=~Xiao_Xiang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Xiang_Zhu1">Xiao Xiang Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 Aug 2021 (modified: 04 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#uUa4jNMLjrL-details-309" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uUa4jNMLjrL-details-309"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Agriculture, Crop Type Classification, Satellite Image Time Series, Food Security, Data Fusion</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A dataset for crop type mapping from daily, analysis-ready satellite time-series data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances in remote sensing products allow near-real time monitoring of the Earth’s surface. Despite increasing availability of near-daily time-series of satellite imagery, there has been little exploration of deep learning methods to utilize the unprecedented temporal density of observations. This is particularly interesting in crop monitoring where time-series remote sensing data has been used frequently to exploit phenological differences of crops in the growing cycle over time. In this work, we present DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operabel, analysis-Ready, daily crop monitoring from space. Our dataset contains daily, analysis-ready Planet Fusion data together with Sentinel-1 radar and Sentinel-2 optical time-series for crop type classification in Northern Germany. Our baseline experiments underline that incorporating the available spatial and temporal information fully may not be straightforward and could require the design of tailored architectures. The dataset presents two main challenges to the community: Exploit the temporal dimension for improved crop classification and ensure that models can handle a domain shift to a different year.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uUa4jNMLjrL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/lukaskondmann/DENETHOR" target="_blank" rel="nofollow noreferrer">https://github.com/lukaskondmann/DENETHOR</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="BlcUQYxknbX" data-number="167">
        <h4>
          <a href="/forum?id=BlcUQYxknbX">
              What Ails One-Shot Image Segmentation: A Data Perspective
          </a>


            <a href="/pdf?id=BlcUQYxknbX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mayur_Hemani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mayur_Hemani1">Mayur Hemani</a>, <a href="/profile?id=~Abhinav_Patel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Patel1">Abhinav Patel</a>, <a href="/profile?email=tej.shimpi%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tej.shimpi@gmail.com">Tejas Shimpi</a>, <a href="/profile?id=~Anirudha_Ramesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudha_Ramesh1">Anirudha Ramesh</a>, <a href="/profile?id=~Balaji_Krishnamurthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balaji_Krishnamurthy1">Balaji Krishnamurthy</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 07 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#BlcUQYxknbX-details-879" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BlcUQYxknbX-details-879"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">one-shot image segmentation negative inductive biases, evaluation dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose training and evaluation improvements for One-shot image segmentation with evidence for negative inductive biases in existing methods as well as a new evaluation dataset for nuanced reporting.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One-shot image segmentation (OSS) methods enable semantic labeling of image pixels without supervised training with an extensive dataset. They require just one example (image, mask) pair per target class. Most neural-network-based methods train on a large subset of dataset classes and are evaluated on a disjoint subset of classes. We posit that the data used for training induces negative biases and affects the accuracy of these methods. Specifically, we present evidence for a \textit{Class Negative Bias} (CNB) arising from treating non-target objects as background during training, and \textit{Salience Bias} (SB), affecting the segmentation accuracy for non-salient target class pixels. We also demonstrate that by eliminating CNB and SB, significant gains can be made over the existing state-of-the-art. Next, we argue that there is a significant disparity between real-world expectations from an OSS method and its accuracy reported on existing benchmarks. To this end, we propose a new evaluation dataset - Tiered One-shot Segmentation (TOSS) - based on the PASCAL <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="31" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>5</mn><mi>i</mi></msup></math></mjx-assistive-mml></mjx-container> and FSS-1000 datasets, and associated metrics for each tier. The dataset enforces uniformity in the measurement of accuracy for existing methods and affords fine-grained insights into the applicability of a method to real applications. The paper includes extensive experiments with the TOSS dataset on several existing OSS methods. The intended impact of this work is to point to biases in training and introduce nuances and uniformity in reporting results for the OSS problem. The evaluation splits of the TOSS dataset and instructions for use are available at \url{https://github.com/fewshotseg/toss}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=BlcUQYxknbX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/fewshotseg/toss" target="_blank" rel="nofollow noreferrer">https://github.com/fewshotseg/toss</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ByiVJWsHVKc" data-number="48">
        <h4>
          <a href="/forum?id=ByiVJWsHVKc">
              SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset for Autonomous Driving
          </a>


            <a href="/pdf?id=ByiVJWsHVKc" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jianhua_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhua_Han1">Jianhua Han</a>, <a href="/profile?id=~Xiwen_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiwen_Liang1">Xiwen Liang</a>, <a href="/profile?id=~Hang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Xu1">Hang Xu</a>, <a href="/profile?id=~Kai_Chen11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Chen11">Kai Chen</a>, <a href="/profile?id=~Lanqing_HONG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lanqing_HONG1">Lanqing HONG</a>, <a href="/profile?id=~Jiageng_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiageng_Mao1">Jiageng Mao</a>, <a href="/profile?id=~Chaoqiang_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaoqiang_Ye1">Chaoqiang Ye</a>, <a href="/profile?id=~Wei_Zhang45" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Zhang45">Wei Zhang</a>, <a href="/profile?id=~Zhenguo_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenguo_Li1">Zhenguo Li</a>, <a href="/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>, <a href="/profile?id=~Chunjing_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chunjing_Xu1">Chunjing Xu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 12 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#ByiVJWsHVKc-details-130" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ByiVJWsHVKc-details-130"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">autonomous driving, object detection, dataset, benchmark, self-supervised learning, semi-supervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale dataset for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest dataset to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (i.e., detection) trained using extensive annotated data to ensure safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (i.e., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing datasets (i.e., BDD100K, Waymo) either provide only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale 2D Self/semi-supervised Object Detection dataset for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected within 27833 driving hours under different weather conditions, periods and location scenes of 32 different cities. We provide extensive experiments and deep analyses of existing popular self-supervised and semi-supervised approaches, and some interesting findings in autonomous driving scope. Experiments show that SODA10M can serve as a promising pre-training dataset for different self-supervised learning methods, which gives superior performance when finetuning with different downstream tasks (i.e., detection, semantic/instance segmentation) in autonomous driving domain. This dataset has been used to hold the ICCV2021 SSLAD challenge. More information can refer to https://soda-2d.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ByiVJWsHVKc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://soda-2d.github.io" target="_blank" rel="nofollow noreferrer">https://soda-2d.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://soda-2d.github.io" target="_blank" rel="nofollow noreferrer">https://soda-2d.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Unless specifically labeled otherwise, these Datasets are provided to You under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”), with the additional terms included herein. The CC BY-NC-SA 4.0 may be accessed at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode. When You download or use the Datasets from the Website or elsewhere, You are agreeing to comply with the terms of CC BY-NC-SA 4.0, and also agreeing to the Dataset Terms. Where these Dataset Terms conflict with the terms of CC BY-NC-SA 4.0, these Dataset Terms shall prevail. We reiterate once again that this dataset is used only for non-commercial purposes such as academic research, teaching, or scientific publications. We prohibits You from using the dataset or any derivative works for commercial purposes, such as selling data or using it for commercial gain.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="gN35BGa1Rt" data-number="313">
        <h4>
          <a href="/forum?id=gN35BGa1Rt">
              A sandbox for prediction and integration of DNA, RNA, and proteins in single cells
          </a>


            <a href="/pdf?id=gN35BGa1Rt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Malte_D_Luecken1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Malte_D_Luecken1">Malte D Luecken</a>, <a href="/profile?id=~Daniel_Bernard_Burkhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Bernard_Burkhardt1">Daniel Bernard Burkhardt</a>, <a href="/profile?id=~Robrecht_Cannoodt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robrecht_Cannoodt1">Robrecht Cannoodt</a>, <a href="/profile?id=~Christopher_Lance1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Lance1">Christopher Lance</a>, <a href="/profile?id=~Aditi_Agrawal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditi_Agrawal1">Aditi Agrawal</a>, <a href="/profile?id=~Hananeh_Aliee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hananeh_Aliee1">Hananeh Aliee</a>, <a href="/profile?id=~Ann_T_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ann_T_Chen1">Ann T Chen</a>, <a href="/profile?id=~Louise_Deconinck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louise_Deconinck1">Louise Deconinck</a>, <a href="/profile?email=angela.detweiler%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="angela.detweiler@czbiohub.org">Angela M Detweiler</a>, <a href="/profile?id=~Alejandro_A_Granados1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alejandro_A_Granados1">Alejandro A Granados</a>, <a href="/profile?id=~Shelly_Huynh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shelly_Huynh1">Shelly Huynh</a>, <a href="/profile?id=~Laura_Isacco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laura_Isacco1">Laura Isacco</a>, <a href="/profile?email=yang-joon.kim%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="yang-joon.kim@czbiohub.org">Yang Joon Kim</a>, <a href="/profile?email=dominik.klein%40helmholtz-muenchen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="dominik.klein@helmholtz-muenchen.de">Dominik Klein</a>, <a href="/profile?id=~BONY_DE_KUMAR1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~BONY_DE_KUMAR1">BONY DE KUMAR</a>, <a href="/profile?id=~Sunil_Kuppasani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sunil_Kuppasani1">Sunil Kuppasani</a>, <a href="/profile?email=heiko.lickert%40helmholtz-muenchen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="heiko.lickert@helmholtz-muenchen.de">Heiko Lickert</a>, <a href="/profile?email=aaron.mcgeever%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="aaron.mcgeever@czbiohub.org">Aaron McGeever</a>, <a href="/profile?email=honey.mekonen%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="honey.mekonen@czbiohub.org">Honey Mekonen</a>, <a href="/profile?email=joaquin.caceresmelgarejo%40yale.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="joaquin.caceresmelgarejo@yale.edu">Joaquin Caceres Melgarejo</a>, <a href="/profile?email=maurizio.morri%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="maurizio.morri@czbiohub.org">Maurizio Morri</a>, <a href="/profile?email=mic.uni.muc%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mic.uni.muc@gmail.com">Michaela Müller</a>, <a href="/profile?email=norma.neff%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="norma.neff@czbiohub.org">Norma Neff</a>, <a href="/profile?email=sheryl.paul%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="sheryl.paul@czbiohub.org">Sheryl Paul</a>, <a href="/profile?id=~Bastian_Rieck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bastian_Rieck1">Bastian Rieck</a>, <a href="/profile?id=~Kaylie_Schneider1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaylie_Schneider1">Kaylie Schneider</a>, <a href="/profile?id=~Scott_Steelman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Steelman1">Scott Steelman</a>, <a href="/profile?email=michael.sterr%40helmholtz-muenchen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.sterr@helmholtz-muenchen.de">Michael Sterr</a>, <a href="/profile?id=~Daniel_J._Treacy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_J._Treacy1">Daniel J. Treacy</a>, <a href="/profile?id=~Alexander_Tong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Tong1">Alexander Tong</a>, <a href="/profile?id=~Alexandra-Chloe_Villani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandra-Chloe_Villani1">Alexandra-Chloe Villani</a>, <a href="/profile?id=~Guilin_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guilin_Wang2">Guilin Wang</a>, <a href="/profile?email=rose.yan%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="rose.yan@czbiohub.org">Jia Yan</a>, <a href="/profile?id=~Ce_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ce_Zhang4">Ce Zhang</a>, <a href="/profile?email=angela.pisco%40czbiohub.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="angela.pisco@czbiohub.org">Angela Oliveira Pisco</a>, <a href="/profile?id=~Smita_Krishnaswamy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Smita_Krishnaswamy1">Smita Krishnaswamy</a>, <a href="/profile?id=~Fabian_J_Theis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabian_J_Theis1">Fabian J Theis</a>, <a href="/profile?id=~Jonathan_M._Bloom1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_M._Bloom1">Jonathan M. Bloom</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">23 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#gN35BGa1Rt-details-708" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gN35BGa1Rt-details-708"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmarks, single-cell, computational biology, genomics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A multimodal single-cell benchmarking dataset and data integration challenge </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The last decade has witnessed a technological arms race to encode the molecular states of cells into DNA libraries, turning DNA sequencers into scalable single-cell microscopes. Single-cell measurement of chromatin accessibility (DNA), gene expression (RNA), and proteins has revealed rich cellular diversity across tissues, organisms, and disease states. However, single-cell data poses a unique set of challenges. A dataset may comprise millions of cells with tens of thousands of sparse features. Identifying biologically relevant signals from the background sources of technical noise requires innovation in predictive and representational learning. Furthermore, unlike in machine vision or natural language processing, biological ground truth is limited. Here we leverage recent advances in multi-modal single-cell technologies which, by simultaneously measuring two layers of cellular processing in each cell, provide ground truth analogous to language translation. We define three key tasks to predict one modality from another and learn integrated representations of cellular state. We also generate a novel dataset of the human bone marrow specifically designed for benchmarking studies. The dataset and tasks are accessible through an open-source framework that facilitates centralized evaluation of community-submitted methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=gN35BGa1Rt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://openproblems.bio/benchmark_dataset" target="_blank" rel="nofollow noreferrer">https://openproblems.bio/benchmark_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://openproblems.bio/benchmark_dataset" target="_blank" rel="nofollow noreferrer">https://openproblems.bio/benchmark_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Ayf90B1yESX" data-number="298">
        <h4>
          <a href="/forum?id=Ayf90B1yESX">
              The Medkit-Learn(ing) Environment: Medical Decision Modelling through Simulation
          </a>


            <a href="/pdf?id=Ayf90B1yESX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Alex_Chan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Chan2">Alex Chan</a>, <a href="/profile?id=~Ioana_Bica1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioana_Bica1">Ioana Bica</a>, <a href="/profile?id=~Alihan_H%C3%BCy%C3%BCk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alihan_Hüyük1">Alihan Hüyük</a>, <a href="/profile?id=~Daniel_Jarrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jarrett1">Daniel Jarrett</a>, <a href="/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">21 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#Ayf90B1yESX-details-661" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ayf90B1yESX-details-661"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">decision modelling, imitation learning, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The goal of understanding decision-making behaviours in clinical environments is of paramount importance if we are to bring the strengths of machine learning to ultimately improve patient outcomes. Mainstream development of algorithms is often geared towards optimal performance in tasks that do not necessarily translate well into the medical regime---due to several factors including the lack of public availability of realistic data, the intrinsically offline nature of the problem, as well as the complexity and variety of human behaviours. We therefore present a new benchmarking suite designed specifically for medical sequential decision modelling: the Medkit-Learn(ing) Environment, a publicly available Python package providing simple and easy access to high-fidelity synthetic medical data. While providing a standardised way to compare algorithms in a realistic medical setting, we employ a generating process that disentangles the policy and environment dynamics to allow for a range of customisations, thus enabling systematic evaluation of algorithms’ robustness against specific challenges prevalent in healthcare.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Ayf90B1yESX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/XanderJC/medkit-learn" target="_blank" rel="nofollow noreferrer">https://github.com/XanderJC/medkit-learn</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NxWUnvwFV4" data-number="184">
        <h4>
          <a href="/forum?id=NxWUnvwFV4">
              Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning
          </a>


            <a href="/pdf?id=NxWUnvwFV4" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Qinkai_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qinkai_Zheng2">Qinkai Zheng</a>, <a href="/profile?id=~Xu_Zou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xu_Zou2">Xu Zou</a>, <a href="/profile?id=~Yuxiao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxiao_Dong1">Yuxiao Dong</a>, <a href="/profile?id=~Yukuo_Cen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yukuo_Cen1">Yukuo Cen</a>, <a href="/profile?id=~Da_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Yin1">Da Yin</a>, <a href="/profile?id=~Jiarong_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiarong_Xu2">Jiarong Xu</a>, <a href="/profile?email=yangya%40zju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="yangya@zju.edu.cn">Yang Yang</a>, <a href="/profile?id=~Jie_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Tang1">Jie Tang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#NxWUnvwFV4-details-668" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NxWUnvwFV4-details-668"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial robustness, graph machine learning, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Benchmark for better evaluating the adversarial robustness of graph machine learning models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial attacks on graphs have posed a major threat to the robustness of graph machine learning (GML) models. Naturally, there is an ever-escalating arms race between attackers and defenders. However, the strategies behind both sides are often not fairly compared under the same and realistic conditions. To bridge this gap, we present the Graph Robustness Benchmark (GRB) with the goal of providing a scalable, unified, modular, and reproducible evaluation for the adversarial robustness of GML models. GRB standardizes the process of attacks and defenses by 1) developing scalable and diverse datasets, 2) modularizing the attack and defense implementations, and 3) unifying the evaluation protocol in refined scenarios. By leveraging the modular GRB pipeline,  the end-users can focus on the development of robust GML models with automated data processing and experimental evaluations. To support open and reproducible research on graph adversarial learning, GRB also hosts public leaderboards for different scenarios.
        As a starting point, we provide various baseline experiments to benchmark the state-of-the-art techniques. GRB is an open-source benchmark and all datasets, code, and leaderboards are available at https://cogdl.ai/grb/home. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NxWUnvwFV4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Homepage: https://cogdl.ai/grb/home ; Code: https://github.com/THUDM/grb</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://cogdl.ai/grb/home" target="_blank" rel="nofollow noreferrer">https://cogdl.ai/grb/home</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FQLzQqGEAH" data-number="155">
        <h4>
          <a href="/forum?id=FQLzQqGEAH">
              Really Doing Great at Estimating CATE? A Critical Look at ML Benchmarking Practices in Treatment Effect Estimation
          </a>


            <a href="/pdf?id=FQLzQqGEAH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Alicia_Curth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alicia_Curth1">Alicia Curth</a>, <a href="/profile?id=~David_Svensson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Svensson1">David Svensson</a>, <a href="/profile?id=~Jim_Weatherall1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jim_Weatherall1">Jim Weatherall</a>, <a href="/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">21 Replies</span>


        </div>

          <a href="#FQLzQqGEAH-details-259" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FQLzQqGEAH-details-259"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Treatment effect heterogeneity, Causal Inference, Benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We argue that CATE estimator benchmarking results based on popular semi-synthetic datasets should be interpreted more carefully, and discuss alternatives to current practice.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The machine learning (ML) toolbox for estimation of heterogeneous treatment effects from observational data is expanding rapidly, yet many of its algorithms have been evaluated only on a very limited set of semi-synthetic benchmark datasets. In this paper, we investigate current benchmarking practices for ML-based conditional average treatment effect (CATE) estimators, with special focus on empirical evaluation based on the popular semi-synthetic IHDP benchmark. We identify problems with current practice and highlight that semi-synthetic benchmark datasets, which (unlike real-world benchmarks used elsewhere in ML) do not necessarily reflect properties of real data, can systematically favor some algorithms over others  -- a fact that is rarely acknowledged but of immense relevance for interpretation of empirical results. Further, we argue that current evaluation metrics evaluate performance only for a small subset of possible use cases of CATE estimators, and discuss alternative metrics relevant for applications in personalized medicine. Additionally, we discuss alternatives for current benchmark datasets, and implications of our findings for benchmarking in CATE estimation. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FQLzQqGEAH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fnuAjFL7MXy" data-number="110">
        <h4>
          <a href="/forum?id=fnuAjFL7MXy">
              The Met Dataset: Instance-level Recognition for Artworks
          </a>


            <a href="/pdf?id=fnuAjFL7MXy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nikolaos-Antonios_Ypsilantis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikolaos-Antonios_Ypsilantis1">Nikolaos-Antonios Ypsilantis</a>, <a href="/profile?id=~Noa_Garcia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noa_Garcia1">Noa Garcia</a>, <a href="/profile?id=~Guangxing_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guangxing_Han1">Guangxing Han</a>, <a href="/profile?id=~Sarah_Ibrahimi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_Ibrahimi1">Sarah Ibrahimi</a>, <a href="/profile?id=~Nanne_Van_Noord1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanne_Van_Noord1">Nanne Van Noord</a>, <a href="/profile?id=~Giorgos_Tolias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giorgos_Tolias1">Giorgos Tolias</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 05 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#fnuAjFL7MXy-details-900" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fnuAjFL7MXy-details-900"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">instance-level recognition, artwork recognition, large-scale recognition dataset, knn classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A large-scale dataset for instance-level recognition for artworks is introduced.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This work introduces a dataset for large-scale instance-level recognition in the domain of artworks. The proposed benchmark exhibits a number of different challenges such as large inter-class similarity, long tail distribution, and many classes. We rely on the open access collection of The Met museum to form a large training set of about 224k classes, where each class corresponds to a museum exhibit with photos taken under studio conditions. Testing is primarily performed on photos taken by museum guests depicting exhibits, which introduces a distribution shift between training and testing. Testing is additionally performed on a set of images not related to Met exhibits making the task resemble an out-of-distribution detection problem. The proposed benchmark follows the paradigm of other recent datasets for instance level recognition on different domains to encourage research on domain independent approaches. A number of suitable approaches are evaluated to offer a testbed for future comparisons. Self-supervised and supervised contrastive learning are effectively combined to train the backbone which is used for non-parametric classification that is shown as a promising direction. Dataset webpage: http://cmp.felk.cvut.cz/met/</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fnuAjFL7MXy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">http://cmp.felk.cvut.cz/met/  https://github.com/nikosips/met</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">http://cmp.felk.cvut.cz/met/  https://github.com/nikosips/met</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset license:
        The annotations are licensed under CC BY 4.0 license. The images included in the dataset are either publicly available on the web, and come from three sources, i.e. the Met open collection, Flickr, and WikiMedia commons, or are created by us. The corresponding licenses for the ones that are available on the web are public domain, Creative Commons, and public domain, respectively. We do not own their copyright. For the ones created by us, we release them to the public domain.

        Code license:
        The code that accompanies the dataset and is hosted at (https://github.com/nikosips/met) is licensed under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MQlMIrm3Hv5" data-number="88">
        <h4>
          <a href="/forum?id=MQlMIrm3Hv5">
              Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions
          </a>


            <a href="/pdf?id=MQlMIrm3Hv5" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chenyu_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenyu_Yi1">Chenyu Yi</a>, <a href="/profile?id=~SIYUAN_YANG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SIYUAN_YANG1">SIYUAN YANG</a>, <a href="/profile?id=~Haoliang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoliang_Li2">Haoliang Li</a>, <a href="/profile?id=~Yap-peng_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yap-peng_Tan1">Yap-peng Tan</a>, <a href="/profile?id=~Alex_Kot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Kot1">Alex Kot</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#MQlMIrm3Hv5-details-819" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MQlMIrm3Hv5-details-819"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Robustness</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The state-of-the-art deep neural networks are vulnerable to common corruptions (e.g., input data degradations, distortions, and disturbances caused by weather changes, system error, and processing). While much progress has been made in analyzing and improving the robustness of models in image understanding, the robustness in video understanding is largely unexplored. In this paper, we establish a corruption robustness benchmark, Mini Kinetics-C and Mini SSV2-C, which considers temporal corruptions beyond spatial corruptions in images. We make the first attempt to conduct an exhaustive study on the corruption robustness of established CNN-based and Transformer-based spatial-temporal models. The study provides some guidance on robust model design and training: Transformer-based model performs better than CNN-based models on corruption robustness; the generalization ability of spatial-temporal models implies robustness against temporal corruptions; model corruption robustness (especially robustness in the temporal domain) enhances with computational cost and model capacity, which may contradict the current trend of improving the computational efficiency of models. Moreover, we find the robustness intervention for image-related tasks (e.g., training models with noise) may not work for spatial-temporal models. Our codes are available on https://github.com/Newbeeyoung/Video-Corruption-Robustness.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MQlMIrm3Hv5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Newbeeyoung/Video-Corruption-Robustness" target="_blank" rel="nofollow noreferrer">https://github.com/Newbeeyoung/Video-Corruption-Robustness</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Newbeeyoung/Video-Corruption-Robustness" target="_blank" rel="nofollow noreferrer">https://github.com/Newbeeyoung/Video-Corruption-Robustness</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="H-d5634yVi" data-number="43">
        <h4>
          <a href="/forum?id=H-d5634yVi">
              Chest ImaGenome Dataset for Clinical Reasoning
          </a>


            <a href="/pdf?id=H-d5634yVi" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Joy_T_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joy_T_Wu1">Joy T Wu</a>, <a href="/profile?id=~Nkechinyere_Nneka_Agu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nkechinyere_Nneka_Agu1">Nkechinyere Nneka Agu</a>, <a href="/profile?id=~Ismini_Lourentzou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ismini_Lourentzou1">Ismini Lourentzou</a>, <a href="/profile?id=~Arjun_Sharma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arjun_Sharma3">Arjun Sharma</a>, <a href="/profile?id=~Joseph_Alexander_Paguio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_Alexander_Paguio1">Joseph Alexander Paguio</a>, <a href="/profile?id=~Jasper_Seth_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jasper_Seth_Yao1">Jasper Seth Yao</a>, <a href="/profile?id=~Edward_Christopher_Dee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Christopher_Dee1">Edward Christopher Dee</a>, <a href="/profile?id=~William_G_Mitchell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_G_Mitchell1">William G Mitchell</a>, <a href="/profile?id=~Satyananda_Kashyap2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Satyananda_Kashyap2">Satyananda Kashyap</a>, <a href="/profile?id=~Andrea_Giovannini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrea_Giovannini1">Andrea Giovannini</a>, <a href="/profile?id=~Leo_Anthony_Celi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leo_Anthony_Celi1">Leo Anthony Celi</a>, <a href="/profile?id=~Mehdi_Moradi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mehdi_Moradi1">Mehdi Moradi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">16 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#H-d5634yVi-details-913" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H-d5634yVi-details-913"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Chest X-ray, Scene Graph, CXR Dataset, Medical Radiology, Medical Imaging</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the first VisualGenome-style Chest X-Ray (CXR) dataset to facilitate clinical reasoning research.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite the progress in automatic detection of radiologic findings from Chest X-Ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global "weak" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe 242,072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) 1,256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from 500 unique patients.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=H-d5634yVi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://www.physionet.org/content/chest-imagenome/1.0.0/ [This is a public link. Full access for downloading requires credentials. Reviewers please see Credentialized access link below. We have created a pass phrase for you. Ask the chairs]. </span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">PhysioNet credentials are required. A review link with full access will be provided in a note to the reviewers.</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://physionet.org/content/chest-imagenome/1.0.0/" target="_blank" rel="nofollow noreferrer">https://physionet.org/content/chest-imagenome/1.0.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value "><a href="https://physionet.org/content/chest-imagenome/view-license/1.0.0/" target="_blank" rel="nofollow noreferrer">https://physionet.org/content/chest-imagenome/view-license/1.0.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KGeAHDH4njY" data-number="249">
        <h4>
          <a href="/forum?id=KGeAHDH4njY">
              Mitigating dataset harms requires stewardship: Lessons from 1000 papers
          </a>


            <a href="/pdf?id=KGeAHDH4njY" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kenneth_L_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenneth_L_Peng1">Kenneth L Peng</a>, <a href="/profile?id=~Arunesh_Mathur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arunesh_Mathur1">Arunesh Mathur</a>, <a href="/profile?id=~Arvind_Narayanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arvind_Narayanan1">Arvind Narayanan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#KGeAHDH4njY-details-704" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KGeAHDH4njY-details-704"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">datasets, ethics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We analyzed nearly 1000 papers citing three controversial datasets to better understand the ethics of ML datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning datasets have elicited concerns about privacy, bias, and unethical applications, leading to the retraction of prominent datasets such as DukeMTMC, MS-Celeb-1M, and Tiny Images. In response, the machine learning community has called for higher ethical standards in dataset creation. To help inform these efforts, we studied three influential but ethically problematic face and person recognition datasets---Labeled Faces in the Wild (LFW), MS-Celeb-1M, and DukeMTMC---by analyzing nearly 1000 papers that cite them. We found that the creation of derivative datasets and models, broader technological and social change, the lack of clarity of licenses, and
        dataset management practices can introduce a wide range of ethical concerns. We conclude by suggesting a distributed approach to harm mitigation that considers the entire life cycle of a dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KGeAHDH4njY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Our paper does not present a specific dataset. However, supplemental data (explained in the supplementary material) is available at: https://github.com/citp/mitigating-dataset-harms.</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="79shW3z5Eaq" data-number="204">
        <h4>
          <a href="/forum?id=79shW3z5Eaq">
              Datasets for Online Controlled Experiments
          </a>


            <a href="/pdf?id=79shW3z5Eaq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~C._H._Bryan_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~C._H._Bryan_Liu2">C. H. Bryan Liu</a>, <a href="/profile?email=angelo.cardoso%40asos.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="angelo.cardoso@asos.com">Ângelo Cardoso</a>, <a href="/profile?email=paul.couturier20%40imperial.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="paul.couturier20@imperial.ac.uk">Paul Couturier</a>, <a href="/profile?email=e.mccoy%40imperial.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="e.mccoy@imperial.ac.uk">Emma J. McCoy</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#79shW3z5Eaq-details-862" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="79shW3z5Eaq-details-862"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Online controlled experiments, A/B test, Dataset survey, Dataset taxonomy, Statistical tests</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The first survey and taxonomy for online controlled experiment datasets, together with the first public dataset that supports the design and running of experiments with adaptive stopping.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Online Controlled Experiments (OCE) are the gold standard to measure impact and guide decisions for digital products and services. Despite many methodological advances in this area, the scarcity of public datasets and the lack of a systematic review and categorization hinder its development. We present the first survey and taxonomy for OCE datasets, which highlight the lack of a public dataset to support the design and running of experiments with adaptive stopping, an increasingly popular approach to enable quickly deploying improvements or rolling back degrading changes. We release the first such dataset, containing daily checkpoints of decision metrics from multiple, real experiments run on a global e-commerce platform. The dataset design is guided by a broader discussion on data requirements for common statistical tests used in digital experimentation. We demonstrate how to use the dataset in the adaptive stopping scenario using sequential and Bayesian hypothesis tests and learn the relevant parameters for each approach. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=79shW3z5Eaq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">The dataset, its schema and accompanying datasheet (with the intended uses, plus the hosting, licensing, and maintenance plan), and the experiment code are available on Open Science Framework: https://osf.io/64jsb/ (DOI 10.17605/OSF.IO/64JSB). The project (and dataset) is open-sourced under a CC-By Attribution 4.0 International license.</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset, its schema and accompanying datasheet (with the intended uses, plus the hosting, licensing, and maintenance plan), and the experiment code are available on Open Science Framework: https://osf.io/64jsb/</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The project (including the dataset) is open-sourced under a CC-By Attribution 4.0 International license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="OCrD8ycKjG" data-number="87">
        <h4>
          <a href="/forum?id=OCrD8ycKjG">
              OpenML Benchmarking Suites
          </a>


            <a href="/pdf?id=OCrD8ycKjG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Bernd_Bischl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernd_Bischl1">Bernd Bischl</a>, <a href="/profile?id=~Giuseppe_Casalicchio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giuseppe_Casalicchio1">Giuseppe Casalicchio</a>, <a href="/profile?id=~Matthias_Feurer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Feurer2">Matthias Feurer</a>, <a href="/profile?id=~Pieter_Gijsbers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Gijsbers1">Pieter Gijsbers</a>, <a href="/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>, <a href="/profile?id=~Michel_Lang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michel_Lang1">Michel Lang</a>, <a href="/profile?id=~Rafael_Gomes_Mantovani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rafael_Gomes_Mantovani1">Rafael Gomes Mantovani</a>, <a href="/profile?id=~Jan_N._van_Rijn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_N._van_Rijn1">Jan N. van Rijn</a>, <a href="/profile?id=~Joaquin_Vanschoren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joaquin_Vanschoren1">Joaquin Vanschoren</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">14 Replies</span>


        </div>

          <a href="#OCrD8ycKjG-details-437" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OCrD8ycKjG-details-437"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Benchmarking, OpenML, Reproducible Research</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A benchmarking layer on top of OpenML to quickly create, download, and share systematic benchmarks </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. We advocate the use of curated, comprehensive suites of machine learning tasks to standardize the setup, execution, and reporting of benchmarks. We enable this through software tools that help to create and leverage these benchmarking suites. These are seamlessly integrated into the OpenML platform, and accessible through interfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to use through standardized data formats, APIs, and client libraries; (b) come with extensive meta-information on the included datasets; and (c) allow benchmarks to be shared and reused in future studies. We then present a first, carefully curated and practical benchmarking suite for classification: the OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally, we discuss use cases and applications which demonstrate the usefulness of OpenML benchmarking suites and the OpenML-CC18 in particular.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=OCrD8ycKjG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="GF9cSKI3A_q" data-number="240">
        <h4>
          <a href="/forum?id=GF9cSKI3A_q">
              Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models
          </a>


            <a href="/pdf?id=GF9cSKI3A_q" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Boxin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boxin_Wang1">Boxin Wang</a>, <a href="/profile?id=~Chejian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chejian_Xu1">Chejian Xu</a>, <a href="/profile?id=~Shuohang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuohang_Wang1">Shuohang Wang</a>, <a href="/profile?id=~Zhe_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhe_Gan1">Zhe Gan</a>, <a href="/profile?id=~Yu_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Cheng1">Yu Cheng</a>, <a href="/profile?id=~Jianfeng_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Gao1">Jianfeng Gao</a>, <a href="/profile?id=~Ahmed_Hassan_Awadallah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Hassan_Awadallah1">Ahmed Hassan Awadallah</a>, <a href="/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li19">Bo Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 10 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#GF9cSKI3A_q-details-318" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GF9cSKI3A_q-details-318"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-task benchmark dataset, adversarial robustness, language models, natural language understanding</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose an adversarial robustness benchmark dataset Adversarial GLUE (AdvGLUE) to quantitatively and thoroughly understand the model vulnerabilities to different types of adversarial transformations.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our ﬁndings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful ﬁltering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=GF9cSKI3A_q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://adversarialglue.github.io." target="_blank" rel="nofollow noreferrer">https://adversarialglue.github.io.</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://adversarialglue.github.io." target="_blank" rel="nofollow noreferrer">https://adversarialglue.github.io.</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset will be distributed under the CC BY-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="qM45LHaWM6E" data-number="74">
        <h4>
          <a href="/forum?id=qM45LHaWM6E">
              Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks
          </a>


            <a href="/pdf?id=qM45LHaWM6E" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andrey_Malinin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Malinin1">Andrey Malinin</a>, <a href="/profile?id=~Neil_Band1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Band1">Neil Band</a>, <a href="/profile?id=~Yarin_Gal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yarin_Gal1">Yarin Gal</a>, <a href="/profile?id=~Mark_Gales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Gales1">Mark Gales</a>, <a href="/profile?id=~Alexander_Ganshin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Ganshin1">Alexander Ganshin</a>, <a href="/profile?email=gerrok%40yandex-team.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="gerrok@yandex-team.ru">German Chesnokov</a>, <a href="/profile?email=alexeynoskov%40yandex-team.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="alexeynoskov@yandex-team.ru">Alexey Noskov</a>, <a href="/profile?id=~Andrey_Ploskonosov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Ploskonosov1">Andrey Ploskonosov</a>, <a href="/profile?id=~Liudmila_Prokhorenkova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liudmila_Prokhorenkova1">Liudmila Prokhorenkova</a>, <a href="/profile?id=~Ivan_Provilkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Provilkov1">Ivan Provilkov</a>, <a href="/profile?email=vr311%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="vr311@cam.ac.uk">Vatsal Raina</a>, <a href="/profile?email=vr313%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="vr313@cam.ac.uk">Vyas Raina</a>, <a href="/profile?email=droginskiy%40yandex-team.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="droginskiy@yandex-team.ru">Denis Roginskiy</a>, <a href="/profile?email=mashashma%40yandex-team.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="mashashma@yandex-team.ru">Mariya Shmatova</a>, <a href="/profile?id=~Panagiotis_Tigas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Panagiotis_Tigas1">Panagiotis Tigas</a>, <a href="/profile?id=~Boris_Yangel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Yangel1">Boris Yangel</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 09 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">19 Replies</span>


        </div>

          <a href="#qM45LHaWM6E-details-924" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qM45LHaWM6E-details-924"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Distributional shift, uncertainty, robustness, tabular data, machine translation, vehicle motion prediction, dataset and baselines</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the Shifts Dataset for investigating distributional shift and uncertainty estimation estimation on a range of industrial scale tasks across multiple data modalities.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> There has been significant research done on developing methods for improving robustness to distributional shift and uncertainty estimation. In contrast, only limited work has examined developing standard datasets and benchmarks for assessing these approaches. Additionally, most work on uncertainty estimation and robustness has developed new techniques based on small-scale regression or image classification tasks. However, many tasks of practical interest have different modalities, such as tabular data, audio, text, or sensor data,  which offer significant challenges involving regression and discrete or continuous structured prediction. Thus, given the current state of the field, a standardized large-scale dataset of tasks across a range of modalities affected by distributional shifts is necessary. This will enable researchers to meaningfully evaluate the plethora of recently developed uncertainty quantification methods, as well as assessment criteria and state-of-the-art baselines. In this work, we propose the \emph{Shifts Dataset} for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation. In this work we provide a description of the dataset and baseline results for all tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=qM45LHaWM6E&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://github.com/yandex-research/shifts - Links to the dataset and metadata will be provided here. However, currently only the canonically partitioned training and dev data is provided, as the data is used in the currently on-going Shifts Challenge. Full dataset and meta will be released 1.11.2021 after the Challenge concludes. </span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wEc1mgAjU-" data-number="309">
        <h4>
          <a href="/forum?id=wEc1mgAjU-">
              Monash Time Series Forecasting Archive
          </a>


            <a href="/pdf?id=wEc1mgAjU-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rakshitha_Wathsadini_Godahewa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rakshitha_Wathsadini_Godahewa1">Rakshitha Wathsadini Godahewa</a>, <a href="/profile?id=~Christoph_Bergmeir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christoph_Bergmeir1">Christoph Bergmeir</a>, <a href="/profile?id=~Geoffrey_I._Webb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Geoffrey_I._Webb1">Geoffrey I. Webb</a>, <a href="/profile?email=rob.hyndman%40monash.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rob.hyndman@monash.edu">Rob Hyndman</a>, <a href="/profile?email=pmontm%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pmontm@gmail.com">Pablo Montero-Manso</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">21 Aug 2021 (modified: 28 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#wEc1mgAjU--details-690" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wEc1mgAjU--details-690"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">global time series forecasting, forecasting archive, feature analysis, baseline evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Our paper introduces the first official time series benchmarking repository for global and multivariate time series forecasting.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many businesses nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models and multivariate models that are trained across sets of time series have shown huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series forecasting archives that contain datasets of time series from similar sources available for researchers to evaluate the performance of new global or multivariate forecasting algorithms over varied datasets. In this paper, we present such a comprehensive forecasting archive containing 25 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across ten error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wEc1mgAjU-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">All information related to our datasets repository are available at https://forecastingdata.org/. The URLs of datasets repository, individual datasets and code repository are mentioned in the paper and supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Our forecasting archive is publicly available at: https://forecastingdata.org/. The tab "Datasets" contains a table representing the summary of all datasets in our archive. This table has a column named "Download", and by clicking the corresponding links mentioned under that column, the users will be redirected to the Zenodo platform (https://zenodo.org/communities/forecasting) where we have uploaded the datasets. After that, the datasets can be simply downloaded by clicking on the "Download" button on the corresponding landing page.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">Not applicable to our work. All datasets in our archive are already publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The datasets (https://forecastingdata.org/) and code (https://github.com/rakshitha123/TSForecasting) are licensed under a Creative Commons Attribution 4.0 International License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uKv5inrWeld" data-number="180">
        <h4>
          <a href="/forum?id=uKv5inrWeld">
              Evaluating Bayes Error Estimators on Real-World Datasets with FeeBee
          </a>


            <a href="/pdf?id=uKv5inrWeld" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cedric_Renggli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cedric_Renggli1">Cedric Renggli</a>, <a href="/profile?id=~Luka_Rimanic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luka_Rimanic1">Luka Rimanic</a>, <a href="/profile?id=~Nora_Hollenstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nora_Hollenstein1">Nora Hollenstein</a>, <a href="/profile?id=~Ce_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ce_Zhang1">Ce Zhang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 04 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#uKv5inrWeld-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uKv5inrWeld-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayes error rate, BER, Irreducible error</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose FeeBee, the first principled framework for analyzing and comparing Bayes error rate estimators on real-world datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Bayes error rate (BER) is a fundamental concept in machine learning that quantifies the best possible accuracy any classifier can achieve on a fixed probability distribution. Despite years of research on building estimators of lower and upper bounds for the BER, these were usually compared only on synthetic datasets with known probability distributions, leaving two key questions unanswered: (1) How well do they perform on realistic, non-synthetic datasets?, and (2) How practical are they? Answering these is not trivial. Apart from the obvious challenge of an unknown BER for real-world datasets, there are two main aspects any BER estimator needs to overcome in order to be applicable in real-world settings: (1) the computational and sample complexity, and (2) the sensitivity and selection of hyper-parameters.
        In this work, we propose FeeBee, the first principled framework for analyzing and comparing BER estimators on modern real-world datasets with unknown probability distribution. We achieve this by injecting a controlled amount of label noise and performing multiple evaluations on a series of different noise levels, supported by a theoretical result which allows drawing conclusions about the evolution of the BER. By implementing and analyzing 7 multi-class BER estimators on 6 commonly used datasets of the computer vision and NLP domains, FeeBee allows a thorough study of these estimators, clearly identifying strengths and weaknesses of each, whilst being easily deployable on any future BER estimator.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uKv5inrWeld&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/DS3Lab/feebee" target="_blank" rel="nofollow noreferrer">https://github.com/DS3Lab/feebee</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="O24OhmqpJtP" data-number="102">
        <h4>
          <a href="/forum?id=O24OhmqpJtP">
              HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML
          </a>


            <a href="/pdf?id=O24OhmqpJtP" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sebastian_Pineda_Arango1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Pineda_Arango1">Sebastian Pineda Arango</a>, <a href="/profile?id=~Hadi_Samer_Jomaa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hadi_Samer_Jomaa1">Hadi Samer Jomaa</a>, <a href="/profile?id=~Martin_Wistuba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Wistuba1">Martin Wistuba</a>, <a href="/profile?id=~Josif_Grabocka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Josif_Grabocka1">Josif Grabocka</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 10 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#O24OhmqpJtP-details-284" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O24OhmqpJtP-details-284"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Meta-dataset, Hyperparameter Optimization, OpenML, Transfer-learning, Meta-learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Hyperparameter optimization (HPO) is a core problem for the machine learning community and remains largely unsolved due to the significant computational resources required to evaluate hyperparameter configurations. As a result, a series of recent related works have focused on the direction of transfer learning for quickly fine-tuning hyperparameters on a dataset. Unfortunately, the community does not have a common large-scale benchmark for comparing HPO algorithms. Instead, the de facto practice consists of empirical protocols on arbitrary small-scale meta-datasets that vary inconsistently across publications, making reproducibility a challenge. To resolve this major bottleneck and enable a fair and fast comparison of black-box HPO methods on a level playing field, we propose HPO-B, a new large-scale benchmark in the form of a collection of meta-datasets. Our benchmark is assembled and preprocessed from the OpenML repository and consists of 176 search spaces (algorithms) evaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter evaluations. For ensuring reproducibility on our benchmark, we detail explicit experimental protocols, splits, and evaluation measures for comparing methods for both non-transfer, as well as, transfer learning HPO.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=O24OhmqpJtP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/releaunifreiburg/HPO-B" target="_blank" rel="nofollow noreferrer">https://github.com/releaunifreiburg/HPO-B</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/releaunifreiburg/HPO-B" target="_blank" rel="nofollow noreferrer">https://github.com/releaunifreiburg/HPO-B</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released under license CC-BY.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1k4rJYEwda-" data-number="86">
        <h4>
          <a href="/forum?id=1k4rJYEwda-">
              HPOBench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for HPO
          </a>


            <a href="/pdf?id=1k4rJYEwda-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Katharina_Eggensperger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katharina_Eggensperger1">Katharina Eggensperger</a>, <a href="/profile?id=~Philipp_M%C3%BCller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Müller1">Philipp Müller</a>, <a href="/profile?id=~Neeratyoy_Mallik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neeratyoy_Mallik1">Neeratyoy Mallik</a>, <a href="/profile?id=~Matthias_Feurer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Feurer2">Matthias Feurer</a>, <a href="/profile?id=~Rene_Sass1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rene_Sass1">Rene Sass</a>, <a href="/profile?id=~Aaron_Klein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Klein1">Aaron Klein</a>, <a href="/profile?email=awad%40cs.uni-freiburg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="awad@cs.uni-freiburg.de">Noor Awad</a>, <a href="/profile?id=~Marius_Lindauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Lindauer1">Marius Lindauer</a>, <a href="/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 08 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">24 Replies</span>


        </div>

          <a href="#1k4rJYEwda--details-606" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1k4rJYEwda--details-606"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hyperparameter Optimization, Benchmarking, Multi-Fidelity, Bayesian Optimization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efficient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-fidelity HPO methods. To close this gap, we propose HPOBench, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multi-fidelity benchmark problems. HPOBench allows to run this extendable set of multi-fidelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench’s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https://github.com/automl/HPOBench.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1k4rJYEwda-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/automl/HPOBench" target="_blank" rel="nofollow noreferrer">https://github.com/automl/HPOBench</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NYgt9vcdyjm" data-number="72">
        <h4>
          <a href="/forum?id=NYgt9vcdyjm">
              GraphGT: Machine Learning Datasets for Graph Generation and Transformation
          </a>


            <a href="/pdf?id=NYgt9vcdyjm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuanqi_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanqi_Du1">Yuanqi Du</a>, <a href="/profile?id=~Shiyu_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiyu_Wang2">Shiyu Wang</a>, <a href="/profile?id=~Xiaojie_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojie_Guo1">Xiaojie Guo</a>, <a href="/profile?id=~Hengning_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hengning_Cao1">Hengning Cao</a>, <a href="/profile?id=~Shujie_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shujie_Hu1">Shujie Hu</a>, <a href="/profile?id=~Junji_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junji_Jiang1">Junji Jiang</a>, <a href="/profile?id=~Aishwarya_Varala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aishwarya_Varala1">Aishwarya Varala</a>, <a href="/profile?id=~Abhinav_Angirekula1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Angirekula1">Abhinav Angirekula</a>, <a href="/profile?id=~Liang_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Zhao1">Liang Zhao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 01 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">19 Replies</span>


        </div>

          <a href="#NYgt9vcdyjm-details-822" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NYgt9vcdyjm-details-822"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Graph Learning, Graph Generation, Graph Transformation, Datasets and Benchmarks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">GraphGT is a systematic machine learning dataset collection for graph generation and transformation. It provides 36 datasets from 9 domains across 6 subjects with easy-to-use data loading, experiment set-up, evaluation Python APIs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph generation has shown great potential in applications like network design and mobility synthesis and is one of the fastest-growing domains in machine learning for graphs. Despite the success of graph generation, the corresponding real-world datasets are few and limited to areas such as molecules and citation networks. To fill the gap, we introduce GraphGT, a large dataset collection for graph generation and transformation problem, which contains 36 datasets from 9 domains across 6 subjects. To assist the researchers with better explorations of the datasets, we provide a systemic review and classification of the datasets based on research tasks, graph types, and application domains. We have significantly (re)processed all the data from different domains to fit the unified framework of graph generation and transformation problems. In addition, GraphGT provides an easy-to-use graph generation pipeline that simplifies the process for graph data loading, experimental setup and model evaluation. Finally, we compare the performance of popular graph generative models in 16 graph generation and 17 graph transformation datasets, showing the great power of GraphGT in differentiating and evaluating model capabilities and drawbacks. GraphGT has been regularly updated and welcomes inputs from the community. GraphGT is publicly available at \url{https://graphgt.github.io/} and can also be accessed via an open Python library.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NYgt9vcdyjm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://graphgt.github.io/" target="_blank" rel="nofollow noreferrer">https://graphgt.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://graphgt.github.io/" target="_blank" rel="nofollow noreferrer">https://graphgt.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="WcY35wjmCBA" data-number="55">
        <h4>
          <a href="/forum?id=WcY35wjmCBA">
              Dynamic Environments with Deformable Objects
          </a>


            <a href="/pdf?id=WcY35wjmCBA" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rika_Antonova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rika_Antonova1">Rika Antonova</a>, <a href="/profile?id=~Peiyang_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peiyang_Shi1">Peiyang Shi</a>, <a href="/profile?id=~Hang_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Yin1">Hang Yin</a>, <a href="/profile?id=~Zehang_Weng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zehang_Weng1">Zehang Weng</a>, <a href="/profile?id=~Danica_Kragic_Jensfelt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danica_Kragic_Jensfelt1">Danica Kragic Jensfelt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 11 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">19 Replies</span>


        </div>

          <a href="#WcY35wjmCBA-details-146" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WcY35wjmCBA-details-146"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Simulation with Deformable Objects, Reinforcement Learning Environments</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a set of environments with dynamic tasks that involve highly deformable topologically non-trivial objects, with support for large-scale parallel data generation and reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose a set of environments with dynamic tasks that involve highly deformable topologically non-trivial objects. These environments facilitate easy experimentation: offer fast runtime, support large-scale parallel data generation, are easy to connect to reinforcement learning frameworks with OpenAI Gym API. We offer several types of benchmark tasks with varying levels of complexity, provide variants with procedurally generated cloth objects and randomized material textures. Moreover, we allow users to customize the tasks: import custom objects and textures, adjust size and material properties of deformable objects.
        We prioritize dynamic aspects of the tasks: forgoing 2D tabletop manipulation in favor of 3D tasks, with gravity and inertia playing a non-negligible role. Such advanced challenges require insights from multiple fields: machine learning and computer vision to process high-dimensional inputs, methods from computer graphics and topology to inspire structured and interpretable representations, insights from robotics to learn advanced control policies. We aim to help researches from these fields contribute their insights and simplify establishing interdisciplinary collaborations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=WcY35wjmCBA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/contactrika/dedo" target="_blank" rel="nofollow noreferrer">https://github.com/contactrika/dedo</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/contactrika/dedo" target="_blank" rel="nofollow noreferrer">https://github.com/contactrika/dedo</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="6vZVBkCDrHT" data-number="37">
        <h4>
          <a href="/forum?id=6vZVBkCDrHT">
              CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks
          </a>


            <a href="/pdf?id=6vZVBkCDrHT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ruchir_Puri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruchir_Puri1">Ruchir Puri</a>, <a href="/profile?id=~David_S_Kung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_S_Kung1">David S Kung</a>, <a href="/profile?id=~Geert_Janssen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Geert_Janssen1">Geert Janssen</a>, <a href="/profile?id=~Wei_Zhang33" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Zhang33">Wei Zhang</a>, <a href="/profile?id=~Giacomo_Domeniconi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giacomo_Domeniconi1">Giacomo Domeniconi</a>, <a href="/profile?id=~Vladimir_Zolotov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladimir_Zolotov1">Vladimir Zolotov</a>, <a href="/profile?email=dolby%40us.ibm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dolby@us.ibm.com">Julian Dolby</a>, <a href="/profile?id=~Jie_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Chen1">Jie Chen</a>, <a href="/profile?id=~Mihir_Choudhury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihir_Choudhury1">Mihir Choudhury</a>, <a href="/profile?id=~Lindsey_Decker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lindsey_Decker1">Lindsey Decker</a>, <a href="/profile?id=~Veronika_Thost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Veronika_Thost1">Veronika Thost</a>, <a href="/profile?id=~Luca_Buratti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luca_Buratti1">Luca Buratti</a>, <a href="/profile?id=~Saurabh_Pujar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saurabh_Pujar1">Saurabh Pujar</a>, <a href="/profile?id=~Shyam_Ramji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shyam_Ramji1">Shyam Ramji</a>, <a href="/profile?id=~Ulrich_Finkler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ulrich_Finkler1">Ulrich Finkler</a>, <a href="/profile?id=~Susan_Malaika2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Susan_Malaika2">Susan Malaika</a>, <a href="/profile?email=frreiss%40us.ibm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="frreiss@us.ibm.com">Frederick Reiss</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">16 Aug 2021 (modified: 06 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#6vZVBkCDrHT-details-130" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6vZVBkCDrHT-details-130"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep Learning, Software Development, AI for Code</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">"We present CodeNet, a large scale, diverse and high-quality AI for code dataset with baseline experiments"</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled breakthroughs in computer vision, speech recognition, natural language processing and beyond, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of “AI for Code” has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset \textit{CodeNet}, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5\% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=6vZVBkCDrHT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Source code: https://github.com/IBM/Project_CodeNet     Dataset: https://developer.ibm.com/exchanges/data/all/project-codenet</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">source code repo - https://github.com/IBM/Project_CodeNet
        CodeNet dataset landing page - https://developer.ibm.com/exchanges/data/all/project-codenet/</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CodeNet dataset license - https://cdla.dev/permissive-2-0/
        Source code license -  https://www.apache.org/licenses/LICENSE-2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mbW_GT3ZN-" data-number="322">
        <h4>
          <a href="/forum?id=mbW_GT3ZN-">
              CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge
          </a>


            <a href="/pdf?id=mbW_GT3ZN-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yasumasa_Onoe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yasumasa_Onoe1">Yasumasa Onoe</a>, <a href="/profile?id=~Michael_JQ_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_JQ_Zhang1">Michael JQ Zhang</a>, <a href="/profile?id=~Eunsol_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunsol_Choi1">Eunsol Choi</a>, <a href="/profile?id=~Greg_Durrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Greg_Durrett1">Greg Durrett</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">27 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#mbW_GT3ZN--details-296" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mbW_GT3ZN--details-296"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">NLP, commonsense reasoning, probing, pre-trained models</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities with commonsense inferences.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall, social knowledge like bumping into someone is awkward, and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim Harry Potter can teach classes on how to fly on a broomstick. Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that pre-trained language models (LMs) should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles with the type of inferences in CREAK. Training a model on CREAK improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mbW_GT3ZN-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.cs.utexas.edu/~yasumasa/creak" target="_blank" rel="nofollow noreferrer">https://www.cs.utexas.edu/~yasumasa/creak</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="c20jiJ5K2H" data-number="255">
        <h4>
          <a href="/forum?id=c20jiJ5K2H">
              Multilingual Spoken Words Corpus
          </a>


            <a href="/pdf?id=c20jiJ5K2H" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mark_Mazumder1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Mazumder1">Mark Mazumder</a>, <a href="/profile?id=~Sharad_Chitlangia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sharad_Chitlangia1">Sharad Chitlangia</a>, <a href="/profile?id=~Colby_Banbury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colby_Banbury1">Colby Banbury</a>, <a href="/profile?email=ypkang%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ypkang@umich.edu">Yiping Kang</a>, <a href="/profile?id=~Juan_Manuel_Ciro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juan_Manuel_Ciro1">Juan Manuel Ciro</a>, <a href="/profile?id=~Keith_Achorn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keith_Achorn1">Keith Achorn</a>, <a href="/profile?id=~Daniel_Galvez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Galvez1">Daniel Galvez</a>, <a href="/profile?email=mark%40landing.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="mark@landing.ai">Mark Sabini</a>, <a href="/profile?id=~Peter_Mattson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Mattson1">Peter Mattson</a>, <a href="/profile?id=~David_Kanter2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Kanter2">David Kanter</a>, <a href="/profile?id=~Greg_Diamos2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Greg_Diamos2">Greg Diamos</a>, <a href="/profile?id=~Pete_Warden1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pete_Warden1">Pete Warden</a>, <a href="/profile?email=josh%40coqui.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="josh@coqui.ai">Josh Meyer</a>, <a href="/profile?id=~Vijay_Janapa_Reddi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vijay_Janapa_Reddi1">Vijay Janapa Reddi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#c20jiJ5K2H-details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c20jiJ5K2H-details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">keyword spotting, speech recognition, low resource languages</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Multilingual Spoken Words Corpus is a speech dataset of over 340,000 spoken words in 50 languages, with over 23.4 million examples.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multilingual Spoken Words Corpus is a large and growing audio dataset of spoken words in 50 languages collectively spoken by over 5 billion people, for academic research and commercial applications in keyword spotting and spoken term search, licensed under CC-BY 4.0. The dataset contains more than 340,000 keywords, totaling 23.4 million 1-second spoken examples (over 6,000 hours). The dataset has many use cases, ranging from voice-enabled consumer devices to call center automation. We generate this dataset by applying forced alignment on crowd-sourced sentence-level audio to produce per-word timing estimates for extraction. All alignments are included in the dataset. We provide a detailed analysis of the contents of the data and contribute methods for detecting potential outliers. We report baseline accuracy metrics on keyword spotting models trained from our dataset compared to models trained on a manually-recorded keyword dataset. We conclude with our plans for dataset maintenance, updates, and open-sourced code.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=c20jiJ5K2H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://mlcommons.org/en/multilingual-spoken-words (the dataset will be available during and after the NeurIPS 2021 conference)</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="sD93GOzH3i5" data-number="253">
        <h4>
          <a href="/forum?id=sD93GOzH3i5">
              Measuring Coding Challenge Competence With APPS
          </a>


            <a href="/pdf?id=sD93GOzH3i5" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>, <a href="/profile?id=~Steven_Basart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Basart1">Steven Basart</a>, <a href="/profile?id=~Saurav_Kadavath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saurav_Kadavath1">Saurav Kadavath</a>, <a href="/profile?id=~Mantas_Mazeika3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mantas_Mazeika3">Mantas Mazeika</a>, <a href="/profile?email=akularora%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="akularora@berkeley.edu">Akul Arora</a>, <a href="/profile?id=~Ethan_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Guo1">Ethan Guo</a>, <a href="/profile?id=~Collin_Burns1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Collin_Burns1">Collin Burns</a>, <a href="/profile?email=samir.puranik%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="samir.puranik@berkeley.edu">Samir Puranik</a>, <a href="/profile?id=~Horace_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Horace_He1">Horace He</a>, <a href="/profile?id=~Dawn_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song2">Dawn Song</a>, <a href="/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 08 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#sD93GOzH3i5-details-241" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sD93GOzH3i5-details-241"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">code, programming, transformers, program synthesis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Can Transformers crack the coding interview? We collected 10,000 programming problems to find out. GPT-3 does not perform well, but new models like GPT-Neo are starting to be able to solve introductory coding challenges.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While programming is one of the most broadly applicable skills in modern society, it is unclear how well state-of-the-art machine learning models can write code. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to assess code generation performance in an accurate and rigorous manner. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an objective measure for tracking advancements.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=sD93GOzH3i5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hendrycks/apps" target="_blank" rel="nofollow noreferrer">https://github.com/hendrycks/apps</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="UuUbIYnHKO" data-number="285">
        <h4>
          <a href="/forum?id=UuUbIYnHKO">
              An Empirical Study of Graph Contrastive Learning
          </a>


            <a href="/pdf?id=UuUbIYnHKO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yanqiao_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanqiao_Zhu1">Yanqiao Zhu</a>, <a href="/profile?id=~Yichen_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yichen_Xu1">Yichen Xu</a>, <a href="/profile?id=~Qiang_Liu8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Liu8">Qiang Liu</a>, <a href="/profile?id=~Shu_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shu_Wu1">Shu Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">21 Aug 2021 (modified: 05 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">14 Replies</span>


        </div>

          <a href="#UuUbIYnHKO-details-349" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UuUbIYnHKO-details-349"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph contrastive learning, self-supervised learning, graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An empirical analysis of graph contrastive learning and an open-source library</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Contrastive Learning (GCL) establishes a new paradigm for learning graph representations without human annotations. Although remarkable progress has been witnessed recently, the success behind GCL is still left somewhat mysterious. In this work, we first identify several critical design considerations within a general GCL paradigm, including augmentation functions, contrasting modes, contrastive objectives, and negative mining strategies. Then, to understand the interplay of different GCL components, we conduct comprehensive, controlled experiments over benchmark tasks on datasets across various domains. Our empirical studies suggest a set of general receipts for effective GCL, e.g., simple topology augmentations that produce sparse graph views bring promising performance improvements; contrasting modes should be aligned with the granularities of end tasks. In addition, to foster future research and ease the implementation of GCL algorithms, we develop an easy-to-use library PyGCL, featuring modularized CL components, standardized evaluation, and experiment management. We envision this work to provide useful empirical evidence of effective GCL algorithms and offer several insights for future research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=UuUbIYnHKO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/GraphCL/PyGCL" target="_blank" rel="nofollow noreferrer">https://github.com/GraphCL/PyGCL</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">PyGCL is licensed under the Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bNL5VlTfe3p" data-number="75">
        <h4>
          <a href="/forum?id=bNL5VlTfe3p">
              Hardware Design and Accurate Simulation of Structured-Light Scanning for Benchmarking of 3D Reconstruction Algorithms
          </a>


            <a href="/pdf?id=bNL5VlTfe3p" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sebastian_Koch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Koch1">Sebastian Koch</a>, <a href="/profile?id=~Yurii_Piadyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yurii_Piadyk1">Yurii Piadyk</a>, <a href="/profile?id=~Markus_Worchel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Worchel1">Markus Worchel</a>, <a href="/profile?id=~Marc_Alexa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Alexa1">Marc Alexa</a>, <a href="/profile?id=~Cl%C3%A1udio_Silva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cláudio_Silva1">Cláudio Silva</a>, <a href="/profile?id=~Denis_Zorin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denis_Zorin2">Denis Zorin</a>, <a href="/profile?id=~Daniele_Panozzo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniele_Panozzo1">Daniele Panozzo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#bNL5VlTfe3p-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bNL5VlTfe3p-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Structured-Light Scanning, Structured Light Scanning, 3D Scanning, Surface Reconstruction, 3D Scanning Benchmark, Range Scan Completion, Shape Completion, Scan Denoising</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We co-develop a 3D structured-light scanning hardware setup together with a corresponding light transport simulation. This provides an ideal framework for developing and benchmarking data-driven algorithms in the area of 3D reconstruction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Images of a real scene taken with a camera commonly differ from synthetic images of a virtual replica of the same scene, despite advances in light transport simulation and calibration. By explicitly co-developing the Structured-Light Scanning (SLS) hardware and rendering pipeline we are able to achieve negligible per-pixel difference between the real image and the synthesized image on geometrically complex calibration objects with known material properties. This approach provides an ideal test-bed for developing and evaluating data-driven algorithms in the area of 3D reconstruction, as the synthetic data is indistinguishable from real data and can be generated at large scale by simulation. We propose three benchmark challenges using a combination of acquired and synthetic data generated with our system: (1) a denoising benchmark tailored to structured-light scanning, (2) a shape completion benchmark to fill in missing data, and (3) a benchmark for surface reconstruction from dense point clouds. Besides, we provide a large collection of high-resolution scans that allow to use our system and benchmarks without reproduction of the hardware setup on our website: https://geometryprocessing.github.io/scanner-sim</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bNL5VlTfe3p&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://geometryprocessing.github.io/scanner-sim" target="_blank" rel="nofollow noreferrer">https://geometryprocessing.github.io/scanner-sim</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://geometryprocessing.github.io/scanner-sim" target="_blank" rel="nofollow noreferrer">https://geometryprocessing.github.io/scanner-sim</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our datasets are available under the CC BY 4.0 license except for the 3D models of 7 colored 3D printed objects which are licensed by their respective creators under various Creative Commons licenses as listed above. Our code is published partly under the BSD 3-Clause Clear license and partly under the GPL 3.0 license. The parts are clearly separated in our code repository and marked with the respective license. The rendering engine we used and modified is GPL-licensed, therefore the simulation framework is licensed under GPL and the calibration and physical scanner software is licensed under BSD 3-Clause Clear. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="TAXFsg6ZaOl" data-number="46">
        <h4>
          <a href="/forum?id=TAXFsg6ZaOl">
              FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset
          </a>


            <a href="/pdf?id=TAXFsg6ZaOl" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hasam_Khalid1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hasam_Khalid1">Hasam Khalid</a>, <a href="/profile?id=~Shahroz_Tariq1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shahroz_Tariq1">Shahroz Tariq</a>, <a href="/profile?id=~Minha_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minha_Kim2">Minha Kim</a>, <a href="/profile?id=~Simon_S._Woo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_S._Woo1">Simon S. Woo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">18 Replies</span>


        </div>

          <a href="#TAXFsg6ZaOl-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TAXFsg6ZaOl-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Audio-Video Deepfakes, Media Forensics, Multimodal dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel multimodal racial unbias deepfake dataset containing three types of deepfakes Type1 (Fake audio) Type2 (Fake video) Type 3 (Fake Audio &amp; Video).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios.
        Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the current most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-of-the-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=TAXFsg6ZaOl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/fakeavcelebdash-lab/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/fakeavcelebdash-lab/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/fakeavcelebdash-lab/home" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/fakeavcelebdash-lab/home</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uXa9oBDZ9V1" data-number="10">
        <h4>
          <a href="/forum?id=uXa9oBDZ9V1">
              IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning
          </a>


            <a href="/pdf?id=uXa9oBDZ9V1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Pan_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pan_Lu2">Pan Lu</a>, <a href="/profile?id=~Liang_Qiu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Qiu2">Liang Qiu</a>, <a href="/profile?id=~Jiaqi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaqi_Chen1">Jiaqi Chen</a>, <a href="/profile?id=~Tony_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tony_Xia1">Tony Xia</a>, <a href="/profile?id=~Yizhou_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhou_Zhao1">Yizhou Zhao</a>, <a href="/profile?id=~Wei_Zhang27" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Zhang27">Wei Zhang</a>, <a href="/profile?id=~Zhou_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhou_Yu1">Zhou Yu</a>, <a href="/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>, <a href="/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">09 Aug 2021 (modified: 31 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#uXa9oBDZ9V1-details-772" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uXa9oBDZ9V1-details-772"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">question answering, diagram understanding, visual reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a new dataset of icon question answering for diagram understanding and reasoning along with an icon dataset for model pre-training.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uXa9oBDZ9V1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://iconqa.github.io" target="_blank" rel="nofollow noreferrer">https://iconqa.github.io</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="QVeMBoRXAo_" data-number="3">
        <h4>
          <a href="/forum?id=QVeMBoRXAo_">
              WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges
          </a>


            <a href="/pdf?id=QVeMBoRXAo_" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Bj%C3%B6rn_Barz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Björn_Barz1">Björn Barz</a>, <a href="/profile?id=~Joachim_Denzler2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joachim_Denzler2">Joachim Denzler</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Aug 2021 (modified: 05 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#QVeMBoRXAo_-details-75" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QVeMBoRXAo_-details-75"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Architectural Style Recognition, Computer Vision, Image Dataset, Fine-Grained Visual Categorization, Fine-Grained Visual Recognition, Hierarchical Classification, Class Hierarchy, Data-efficient Deep Learning, Imprecise Labels, Imbalanced Classes</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset comprising images of church buildings, labeled with their architectural style and bounding boxes denoting distinctive architectural elements.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce a novel dataset for architectural style classification, consisting of 9,485 images of church buildings. Both images and style labels were sourced from Wikipedia. The dataset can serve as a benchmark for various research fields, as it combines numerous real-world challenges: fine-grained distinctions between classes based on subtle visual features, a comparatively small sample size, a highly imbalanced class distribution, a high variance of viewpoints, and a hierarchical organization of labels, where only some images are labeled at the most precise level.
        In addition, we provide 631 bounding box annotations of characteristic visual features for 139 churches from four major categories. These annotations can, for example, be useful for research on fine-grained classification, where additional expert knowledge about distinctive object parts is often available.
        Images and annotations are available at: https://doi.org/10.5281/zenodo.5166986</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=QVeMBoRXAo_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5166986" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5166986</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5166986" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5166986</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution Share Alike 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="gWIbXsrtOCc" data-number="268">
        <h4>
          <a href="/forum?id=gWIbXsrtOCc">
              Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning
          </a>


            <a href="/pdf?id=gWIbXsrtOCc" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nan_Rosemary_Ke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nan_Rosemary_Ke1">Nan Rosemary Ke</a>, <a href="/profile?id=~Aniket_Rajiv_Didolkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aniket_Rajiv_Didolkar1">Aniket Rajiv Didolkar</a>, <a href="/profile?id=~Sarthak_Mittal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarthak_Mittal1">Sarthak Mittal</a>, <a href="/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="/profile?id=~Guillaume_Lajoie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guillaume_Lajoie1">Guillaume Lajoie</a>, <a href="/profile?id=~Stefan_Bauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Bauer1">Stefan Bauer</a>, <a href="/profile?id=~Danilo_Jimenez_Rezende2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danilo_Jimenez_Rezende2">Danilo Jimenez Rezende</a>, <a href="/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="/profile?id=~Christopher_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Pal1">Christopher Pal</a>, <a href="/profile?id=~Michael_Curtis_Mozer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Curtis_Mozer1">Michael Curtis Mozer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 09 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#gWIbXsrtOCc-details-992" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gWIbXsrtOCc-details-992"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">model-based RL, causal discovery</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves  are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this  objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order  to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=gWIbXsrtOCc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/dido1998/CausalMBRL" target="_blank" rel="nofollow noreferrer">https://github.com/dido1998/CausalMBRL</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="kBNhgqXatI" data-number="164">
        <h4>
          <a href="/forum?id=kBNhgqXatI">
              An Empirical Investigation of Representation Learning for Imitation
          </a>


            <a href="/pdf?id=kBNhgqXatI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xin_Chen15" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Chen15">Xin Chen</a>, <a href="/profile?id=~Sam_Toyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sam_Toyer1">Sam Toyer</a>, <a href="/profile?id=~Cody_Wild1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cody_Wild1">Cody Wild</a>, <a href="/profile?id=~Scott_Emmons1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Emmons1">Scott Emmons</a>, <a href="/profile?id=~Ian_Fischer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Fischer1">Ian Fischer</a>, <a href="/profile?id=~Kuang-Huei_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuang-Huei_Lee1">Kuang-Huei Lee</a>, <a href="/profile?id=~Neel_Alex1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neel_Alex1">Neel Alex</a>, <a href="/profile?id=~Steven_H_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_H_Wang1">Steven H Wang</a>, <a href="/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>, <a href="/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stuart_Russell1">Stuart Russell</a>, <a href="/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="/profile?id=~Rohin_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rohin_Shah1">Rohin Shah</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 11 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#kBNhgqXatI-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kBNhgqXatI-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">imitation learning, representation learning, reinforcement learning, image augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Imitation learning often needs a large demonstration set in order to handle the full range of situations that an agent might find itself in during deployment. However, collecting expert demonstrations can be expensive. Recent work in vision, reinforcement learning, and NLP has shown that auxiliary representation learning objectives can reduce the need for large amounts of expensive, task-specific data. Our Empirical Investigation of Representation Learning for Imitation (EIRLI) investigates whether similar benefits apply to imitation learning. We propose a modular framework for constructing representation learning algorithms, then use our framework to evaluate the utility of representation learning for imitation across several environment suites. In the settings we evaluate, we find that existing algorithms for image-based representation learning provide limited value relative to a well-tuned baseline with image augmentations. To explain this result, we investigate differences between imitation learning and other settings where representation learning *has* provided significant benefit, such as image classification. Finally, we release a well-documented codebase which both replicates our findings and provides a modular framework for creating new representation learning algorithms out of reusable components.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=kBNhgqXatI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/HumanCompatibleAI/eirli" target="_blank" rel="nofollow noreferrer">https://github.com/HumanCompatibleAI/eirli</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="eZu4BZxlRnX" data-number="68">
        <h4>
          <a href="/forum?id=eZu4BZxlRnX">
              Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents
          </a>


            <a href="/pdf?id=eZu4BZxlRnX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jane_X_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jane_X_Wang1">Jane X Wang</a>, <a href="/profile?id=~Michael_King4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_King4">Michael King</a>, <a href="/profile?id=~Nicolas_Pierre_Mickael_Porcel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Pierre_Mickael_Porcel1">Nicolas Pierre Mickael Porcel</a>, <a href="/profile?id=~Zeb_Kurth-Nelson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeb_Kurth-Nelson1">Zeb Kurth-Nelson</a>, <a href="/profile?id=~Tina_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tina_Zhu1">Tina Zhu</a>, <a href="/profile?email=cdeck%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="cdeck@google.com">Charlie Deck</a>, <a href="/profile?id=~Peter_Choy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Choy1">Peter Choy</a>, <a href="/profile?id=~Mary_Cassin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mary_Cassin1">Mary Cassin</a>, <a href="/profile?id=~Malcolm_Reynolds1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Malcolm_Reynolds1">Malcolm Reynolds</a>, <a href="/profile?id=~H._Francis_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~H._Francis_Song1">H. Francis Song</a>, <a href="/profile?id=~Gavin_Buttimore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gavin_Buttimore1">Gavin Buttimore</a>, <a href="/profile?id=~David_P_Reichert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_P_Reichert1">David P Reichert</a>, <a href="/profile?id=~Neil_Charles_Rabinowitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Charles_Rabinowitz1">Neil Charles Rabinowitz</a>, <a href="/profile?id=~Loic_Matthey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Loic_Matthey1">Loic Matthey</a>, <a href="/profile?id=~Demis_Hassabis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Demis_Hassabis1">Demis Hassabis</a>, <a href="/profile?id=~Alexander_Lerchner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Lerchner1">Alexander Lerchner</a>, <a href="/profile?id=~Matthew_Botvinick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Botvinick1">Matthew Botvinick</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 02 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#eZu4BZxlRnX-details-1" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eZu4BZxlRnX-details-1"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-reinforcement learning, benchmark, deep RL, agent analysis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a new meta-reinforcement learning benchmark that tests and analyzes deep RL agents' abilities to perform structured latent inference.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been rapidly growing interest in meta-learning as a method for increasing the flexibility and sample efficiency of reinforcement learning. One problem in this area of research, however, has been a scarcity of adequate benchmark tasks. In general, the structure underlying past benchmarks has either been too simple to be inherently interesting, or too ill-defined to support principled analysis. In the present work, we introduce a new benchmark for meta-RL research, emphasizing transparency and potential for in-depth analysis as well as structural richness. Alchemy is a 3D video game, implemented in Unity, which involves a latent causal structure that is resampled procedurally from episode to episode, affording structure learning, online inference, hypothesis testing and action sequencing based on abstract domain knowledge. We evaluate a pair of powerful RL agents on Alchemy and present an in-depth analysis of one of these agents. Results clearly indicate a frank and specific failure of meta-learning, providing validation for Alchemy as a challenging benchmark for meta-RL. Concurrent with this report, we are releasing Alchemy as public resource, together with a suite of analysis tools and sample agent trajectories.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=eZu4BZxlRnX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/deepmind/dm_alchemy" target="_blank" rel="nofollow noreferrer">https://github.com/deepmind/dm_alchemy</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The Alchemy environment and analysis tools can be found at https://github.com/deepmind/
        dm_alchemy and are released under the Apache License 2.0. Further licensing details and all
        documentation can be found at this repository.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YDMFgD_qJuA" data-number="252">
        <h4>
          <a href="/forum?id=YDMFgD_qJuA">
              SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation
          </a>


            <a href="/pdf?id=YDMFgD_qJuA" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Arjun_D_Desai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arjun_D_Desai1">Arjun D Desai</a>, <a href="/profile?email=aschmid2%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="aschmid2@stanford.edu">Andrew M Schmidt</a>, <a href="/profile?email=erubin3%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="erubin3@stanford.edu">Elka B Rubin</a>, <a href="/profile?id=~Christopher_Michael_Sandino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Michael_Sandino1">Christopher Michael Sandino</a>, <a href="/profile?id=~Marianne_Susan_Black1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marianne_Susan_Black1">Marianne Susan Black</a>, <a href="/profile?id=~Valentina_Mazzoli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valentina_Mazzoli1">Valentina Mazzoli</a>, <a href="/profile?email=kate.stevens%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kate.stevens@stanford.edu">Kathryn J Stevens</a>, <a href="/profile?email=boutin%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="boutin@stanford.edu">Robert Boutin</a>, <a href="/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>, <a href="/profile?email=gold%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gold@stanford.edu">Garry E Gold</a>, <a href="/profile?id=~Brian_Hargreaves1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_Hargreaves1">Brian Hargreaves</a>, <a href="/profile?id=~Akshay_Chaudhari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akshay_Chaudhari1">Akshay Chaudhari</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#YDMFgD_qJuA-details-591" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YDMFgD_qJuA-details-591"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">medical imaging, MRI, radiology, reconstruction, segmentation, detection, inverse problems, multi-task, computer vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present SKM-TEA, a dataset of raw MRI data, images, and dense labels that enables clinically-relevant, quantitative evaluation for accelerated MRI reconstruction, segmentation, and detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging. However, long image acquisition times, the need for qualitative expert analysis, and the lack of (and difficulty extracting) quantitative indicators that are sensitive to tissue health have curtailed widespread clinical and research studies. While recent machine learning methods for MRI reconstruction and analysis have shown promise for reducing this burden, these techniques are primarily validated with imperfect image quality metrics, which are discordant with clinically-relevant measures that ultimately hamper clinical deployment and clinician trust. To mitigate this challenge, we present the Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) dataset, a collection of quantitative knee MRI (qMRI) scans that enables end-to-end, clinically-relevant evaluation of MRI reconstruction and analysis tools. This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies. We provide a framework for using qMRI parameter maps, along with image reconstructions and dense image labels, for measuring the quality of qMRI biomarker estimates extracted from MRI reconstruction, segmentation, and detection techniques. Finally, we use this framework to benchmark state-of-the-art baselines on this dataset. We hope our SKM-TEA dataset and code can enable a broad spectrum of research for modular image reconstruction and image analysis in a clinically informed manner. Dataset access, code, and benchmarks are available at https://github.com/StanfordMIMI/skm-tea.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YDMFgD_qJuA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/StanfordMIMI/skm-tea" target="_blank" rel="nofollow noreferrer">https://github.com/StanfordMIMI/skm-tea</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jyd4Lyjr2iB" data-number="238">
        <h4>
          <a href="/forum?id=jyd4Lyjr2iB">
              Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks
          </a>


            <a href="/pdf?id=jyd4Lyjr2iB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Neil_Band1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Band1">Neil Band</a>, <a href="/profile?id=~Tim_G._J._Rudner2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_G._J._Rudner2">Tim G. J. Rudner</a>, <a href="/profile?id=~Qixuan_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qixuan_Feng1">Qixuan Feng</a>, <a href="/profile?id=~Angelos_Filos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angelos_Filos1">Angelos Filos</a>, <a href="/profile?id=~Zachary_Nado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Nado1">Zachary Nado</a>, <a href="/profile?id=~Michael_W_Dusenberry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W_Dusenberry1">Michael W Dusenberry</a>, <a href="/profile?id=~Ghassen_Jerfel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ghassen_Jerfel1">Ghassen Jerfel</a>, <a href="/profile?id=~Dustin_Tran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dustin_Tran1">Dustin Tran</a>, <a href="/profile?id=~Yarin_Gal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yarin_Gal1">Yarin Gal</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#jyd4Lyjr2iB-details-947" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jyd4Lyjr2iB-details-947"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Bayesian Deep Learning, Bayesian Neural Networks, Variational Inference, Uncertainty Quantification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents an easy-to-use, expert-guided, open-source suite of diabetic retinopathy detection benchmarking tasks for Bayesian deep learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose the RETINA Benchmark, a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jyd4Lyjr2iB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection" target="_blank" rel="nofollow noreferrer">https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection" target="_blank" rel="nofollow noreferrer">https://github.com/google/uncertainty-baselines/tree/main/baselines/diabetic_retinopathy_detection</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code: Apache License 2.0
        EyePACS and APTOS Datasets: Public Access on Kaggle</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="p2dMLEwL8tF" data-number="115">
        <h4>
          <a href="/forum?id=p2dMLEwL8tF">
              FLIP: Benchmark tasks in fitness landscape inference for proteins
          </a>


            <a href="/pdf?id=p2dMLEwL8tF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Christian_Dallago1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Dallago1">Christian Dallago</a>, <a href="/profile?id=~Jody_Mou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jody_Mou1">Jody Mou</a>, <a href="/profile?id=~Kadina_E_Johnston1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kadina_E_Johnston1">Kadina E Johnston</a>, <a href="/profile?email=bwittman%40caltech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bwittman@caltech.edu">Bruce Wittmann</a>, <a href="/profile?id=~Nick_Bhattacharya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nick_Bhattacharya1">Nick Bhattacharya</a>, <a href="/profile?email=samlg%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="samlg@mit.edu">Samuel Goldman</a>, <a href="/profile?id=~Ali_Madani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Madani1">Ali Madani</a>, <a href="/profile?id=~Kevin_K_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_K_Yang1">Kevin K Yang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">29 Replies</span>


        </div>

          <a href="#p2dMLEwL8tF-details-251" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="p2dMLEwL8tF-details-251"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">protein design, protein landscape prediction, protein representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A set of tasks to probe protein representations for protein engineering.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning could enable an unprecedented level of control in protein engineering for therapeutic and industrial applications. Critical to its use in designing proteins with desired properties, machine learning models must capture the protein sequence-function relationship, often termed fitness landscape. Existing benchmarks like CASP or CAFA assess structure and function predictions of proteins, respectively, yet they do not target metrics relevant for protein engineering. In this work, we introduce Fitness Landscape Inference for Proteins (FLIP), a benchmark for function prediction to encourage rapid scoring of representation learning for protein engineering. Our curated splits, baselines, and metrics probe model generalization in settings relevant for protein engineering, e.g. low-resource and extrapolative. Currently, FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families. In order to enable ease of use and future expansion to new splits, all data are presented in a standard format. FLIP scripts and data are freely accessible at https://benchmark.protein.properties.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=p2dMLEwL8tF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://benchmark.protein.properties" target="_blank" rel="nofollow noreferrer">https://benchmark.protein.properties</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://benchmark.protein.properties" target="_blank" rel="nofollow noreferrer">https://benchmark.protein.properties</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="vKQGe36av4k" data-number="273">
        <h4>
          <a href="/forum?id=vKQGe36av4k">
              Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting
          </a>


            <a href="/pdf?id=vKQGe36av4k" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Benjamin_Wilson3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Wilson3">Benjamin Wilson</a>, <a href="/profile?id=~William_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Qi1">William Qi</a>, <a href="/profile?email=tagarawal%40argo.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="tagarawal@argo.ai">Tanmay Agarwal</a>, <a href="/profile?id=~John_Lambert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Lambert1">John Lambert</a>, <a href="/profile?id=~Jagjeet_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jagjeet_Singh2">Jagjeet Singh</a>, <a href="/profile?id=~Siddhesh_Khandelwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddhesh_Khandelwal1">Siddhesh Khandelwal</a>, <a href="/profile?id=~Bowen_Pan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bowen_Pan2">Bowen Pan</a>, <a href="/profile?id=~Ratnesh_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ratnesh_Kumar2">Ratnesh Kumar</a>, <a href="/profile?id=~Andrew_Hartnett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Hartnett1">Andrew Hartnett</a>, <a href="/profile?id=~Jhony_Kaesemodel_Pontes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jhony_Kaesemodel_Pontes1">Jhony Kaesemodel Pontes</a>, <a href="/profile?id=~Deva_Ramanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deva_Ramanan1">Deva Ramanan</a>, <a href="/profile?id=~Peter_Carr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Carr1">Peter Carr</a>, <a href="/profile?id=~James_Hays1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Hays1">James Hays</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#vKQGe36av4k-details-403" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vKQGe36av4k-details-403"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Argoverse 2 (AV2) — a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for “scored actors" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry — sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=vKQGe36av4k&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.argoverse.org/" target="_blank" rel="nofollow noreferrer">https://www.argoverse.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.argoverse.org/data.html" target="_blank" rel="nofollow noreferrer">https://www.argoverse.org/data.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Q9SKS5k8io" data-number="156">
        <h4>
          <a href="/forum?id=Q9SKS5k8io">
              WRENCH: A Comprehensive Benchmark for Weak Supervision
          </a>


            <a href="/pdf?id=Q9SKS5k8io" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jieyu_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jieyu_Zhang1">Jieyu Zhang</a>, <a href="/profile?id=~Yue_Yu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Yu2">Yue Yu</a>, <a href="/profile?email=yinghaoli%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yinghaoli@gatech.edu">Yinghao Li</a>, <a href="/profile?id=~Yujing_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yujing_Wang1">Yujing Wang</a>, <a href="/profile?id=~Yaming_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaming_Yang1">Yaming Yang</a>, <a href="/profile?id=~Mao_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mao_Yang1">Mao Yang</a>, <a href="/profile?id=~Alexander_Ratner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Ratner1">Alexander Ratner</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#Q9SKS5k8io-details-385" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q9SKS5k8io-details-385"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weak supervision, data programming</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A comprehensive benchmark for weak supervision</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent Weak Supervision (WS)  approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a significant "hidden" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efficacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Q9SKS5k8io&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/JieyuZ2/wrench" target="_blank" rel="nofollow noreferrer">https://github.com/JieyuZ2/wrench</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/JieyuZ2/wrench" target="_blank" rel="nofollow noreferrer">https://github.com/JieyuZ2/wrench</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache-2.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="EfgNF5-ZAjM" data-number="118">
        <h4>
          <a href="/forum?id=EfgNF5-ZAjM">
              STAR: A Benchmark for Situated Reasoning in Real-World Videos
          </a>


            <a href="/pdf?id=EfgNF5-ZAjM" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Bo_Wu6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Wu6">Bo Wu</a>, <a href="/profile?id=~Shoubin_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoubin_Yu1">Shoubin Yu</a>, <a href="/profile?id=~Zhenfang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenfang_Chen1">Zhenfang Chen</a>, <a href="/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">27 Replies</span>


        </div>

          <a href="#EfgNF5-ZAjM-details-734" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EfgNF5-ZAjM-details-734"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Situated Reasoning, Visual Reasoning, Action, Benchmark, Question Answering</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">STAR is a novel benchmark for Situated Reasoning in real-world videos, which provides challenging question-answering tasks, structured situation abstraction and logic-grounded programs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=EfgNF5-ZAjM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">http://star.csail.mit.edu or https://bobbywu.com/STAR </span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">STAR Benchmark: http://star.csail.mit.edu, Code Repository: https://github.com/csbobby/STAR_Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset and Code License: https://github.com/csbobby/STAR_Benchmark/blob/main/LICENSE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="IfzTefIU_3j" data-number="61">
        <h4>
          <a href="/forum?id=IfzTefIU_3j">
              Occluded Video Instance Segmentation: Dataset and ICCV 2021 Challenge
          </a>


            <a href="/pdf?id=IfzTefIU_3j" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiyang_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiyang_Qi1">Jiyang Qi</a>, <a href="/profile?id=~Yan_Gao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yan_Gao2">Yan Gao</a>, <a href="/profile?id=~Yao_Hu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao_Hu2">Yao Hu</a>, <a href="/profile?id=~Xinggang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinggang_Wang1">Xinggang Wang</a>, <a href="/profile?id=~Xiaoyu_Liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyu_Liu4">Xiaoyu Liu</a>, <a href="/profile?id=~Xiang_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Bai1">Xiang Bai</a>, <a href="/profile?id=~Serge_Belongie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Serge_Belongie1">Serge Belongie</a>, <a href="/profile?id=~Alan_Yuille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Yuille1">Alan Yuille</a>, <a href="/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="/profile?id=~Song_Bai3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song_Bai3">Song Bai</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#IfzTefIU_3j-details-884" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IfzTefIU_3j-details-884"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Video instance segmentation, Occlusion understanding, Dataset, Challenge, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the OVIS (Occluded Video Instance Segmentation) dataset and discuss the challenge we launched basing on it.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although deep learning methods have achieved advanced video object recognition performance in recent years, perceiving heavily occluded objects in a video is still a very challenging task. To promote the development of occlusion understanding, we collect a large-scale dataset called OVIS for video instance segmentation in the occluded scenario. OVIS consists of 296k high-quality instance masks and 901 occluded scenes. While our human vision systems can perceive those occluded objects by contextual reasoning and association, our experiments suggest that current video understanding systems cannot. On the OVIS dataset, all baseline methods encounter a significant performance degradation of about 80\% in the heavily occluded object group, which demonstrates that there is still a long way to go in understanding obscured objects and videos in a complex real-world scenario. To facilitate the research on new paradigms for video understanding systems, we launched a challenge basing on the OVIS dataset. The submitted top-performing algorithms have achieved much higher performance than our baselines. In this paper, we will introduce the OVIS dataset and further dissect it by analyzing the results of baselines and submitted methods. The OVIS dataset and challenge information can be found at \url{http://songbai.site/ovis}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=IfzTefIU_3j&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://songbai.site/ovis" target="_blank" rel="nofollow noreferrer">http://songbai.site/ovis</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="43mYF598ZDB" data-number="51">
        <h4>
          <a href="/forum?id=43mYF598ZDB">
              The CLEAR Benchmark: Continual LEArning on Real-World Imagery
          </a>


            <a href="/pdf?id=43mYF598ZDB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhiqiu_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiqiu_Lin1">Zhiqiu Lin</a>, <a href="/profile?id=~Jia_Shi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia_Shi2">Jia Shi</a>, <a href="/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepak_Pathak1">Deepak Pathak</a>, <a href="/profile?id=~Deva_Ramanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deva_Ramanan1">Deva Ramanan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#43mYF598ZDB-details-586" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="43mYF598ZDB-details-586"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">computer vision, machine learning, ML datasets and benchmark, continual learning, image recognition, online continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present CLEAR, the first benchmark for naturally-evolving continual image classification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continual learning (CL) is widely regarded as crucial challenge for lifelong AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make use of artificial temporal variation and do not align with or generalize to the real- world. In this paper, we introduce CLEAR, the first continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). We build CLEAR from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. Our pipeline makes use of pretrained vision-language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning. We find that a simple unsupervised pre-training step can already boost state-of-the-art CL algorithms that only utilize fully-supervised data. Our analysis also reveals that mainstream CL evaluation protocols that train and test on iid data artificially inflate performance of CL system. To address this, we propose novel "streaming" protocols for CL that always test on the (near) future. Interestingly, streaming protocols (a) can simplify dataset curation since today’s testset can be repurposed for tomorrow’s trainset and (b) can produce more generalizable models with more accurate estimates of performance since all labeled data from each time-period is used for both training and testing (unlike classic iid train-test splits).</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=43mYF598ZDB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://clear-benchmark.github.io/" target="_blank" rel="nofollow noreferrer">https://clear-benchmark.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://clear-benchmark.github.io/" target="_blank" rel="nofollow noreferrer">https://clear-benchmark.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">na</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset is under CC BY license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-v4OuqNs5P" data-number="30">
        <h4>
          <a href="/forum?id=-v4OuqNs5P">
              Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI
          </a>


            <a href="/pdf?id=-v4OuqNs5P" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Santhosh_Kumar_Ramakrishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santhosh_Kumar_Ramakrishnan1">Santhosh Kumar Ramakrishnan</a>, <a href="/profile?id=~Aaron_Gokaslan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Gokaslan1">Aaron Gokaslan</a>, <a href="/profile?id=~Erik_Wijmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Wijmans1">Erik Wijmans</a>, <a href="/profile?id=~Oleksandr_Maksymets1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oleksandr_Maksymets1">Oleksandr Maksymets</a>, <a href="/profile?id=~Alexander_Clegg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Clegg1">Alexander Clegg</a>, <a href="/profile?id=~John_M_Turner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_M_Turner1">John M Turner</a>, <a href="/profile?id=~Eric_Undersander2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Undersander2">Eric Undersander</a>, <a href="/profile?id=~Wojciech_Galuba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Galuba1">Wojciech Galuba</a>, <a href="/profile?email=awestbury%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="awestbury@fb.com">Andrew Westbury</a>, <a href="/profile?id=~Angel_X_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angel_X_Chang1">Angel X Chang</a>, <a href="/profile?id=~Manolis_Savva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manolis_Savva1">Manolis Savva</a>, <a href="/profile?id=~Yili_Zhao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yili_Zhao2">Yili Zhao</a>, <a href="/profile?id=~Dhruv_Batra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dhruv_Batra1">Dhruv Batra</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">15 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">14 Replies</span>


        </div>

          <a href="#-v4OuqNs5P-details-478" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-v4OuqNs5P-details-478"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Embodied AI, Navigation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the Habitat-Matterport 3D (HM3D) dataset which contains 1000 high quality 3D scans of real-world buildings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-ﬂoor residences, stores, and other private indoor spaces.

        HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual ﬁdelity. HM3D contains 112.5k m^2 of navigable space, which is 1.4 - 3.7× larger than other building-scale datasets (MP3D, Gibson). When compared to existing photorealistic 3D datasets (Replica, MP3D, Gibson, ScanNet), rendered images from HM3D have 20 - 85% higher visual ﬁdelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91% fewer artifacts due to incomplete surface reconstruction.

        The increased scale, ﬁdelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we ﬁnd that HM3D is ‘pareto optimal’ in the following sense – agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on other datasets. HM3D-trained PointNav agents achieve 100% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset. The HM3D dataset, analysis code, and pre-trained models are publicly released: https://aihabitat.org/datasets/hm3d/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-v4OuqNs5P&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://matterport.com/habitat-matterport-3d-research-dataset" target="_blank" rel="nofollow noreferrer">https://matterport.com/habitat-matterport-3d-research-dataset</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="hjsJraoqn1Y" data-number="301">
        <h4>
          <a href="/forum?id=hjsJraoqn1Y">
              A realistic approach to generate masked faces applied on two novel masked face recognition data sets
          </a>


            <a href="/pdf?id=hjsJraoqn1Y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tudor-Alexandru_Mare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tudor-Alexandru_Mare1">Tudor-Alexandru Mare</a>, <a href="/profile?email=georgian.duta%40s.unibuc.ro" class="profile-link" data-toggle="tooltip" data-placement="top" title="georgian.duta@s.unibuc.ro">Georgian Duta</a>, <a href="/profile?id=~Iuliana_Georgescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iuliana_Georgescu1">Iuliana Georgescu</a>, <a href="/profile?email=adrian.sandru%40securifai.ro" class="profile-link" data-toggle="tooltip" data-placement="top" title="adrian.sandru@securifai.ro">Adrian Sandru</a>, <a href="/profile?id=~Bogdan_Alexe3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bogdan_Alexe3">Bogdan Alexe</a>, <a href="/profile?id=~Marius_Popescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Popescu1">Marius Popescu</a>, <a href="/profile?id=~Radu_Tudor_Ionescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Radu_Tudor_Ionescu1">Radu Tudor Ionescu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">21 Aug 2021 (modified: 29 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#hjsJraoqn1Y-details-50" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hjsJraoqn1Y-details-50"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">face recognition, masked face recognition</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a realistic approach to generate masked faces and apply it on two novel masked face recognition data sets</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on SparkAR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8%) masks for the CelebA data set, releasing the mask images at https://github.com/securifai/masked_faces. We show that our method produces significantly more realistic training examples of masks overlaid on faces by asking volunteers to qualitatively compare it to other methods or data sets designed for the same task. We also demonstrate the usefulness of our method by evaluating state-of-the-art face recognition systems (FaceNet, VGG-face, ArcFace) trained on our enhanced data sets and showing that they outperform equivalent systems trained on original data sets (containing faces without masks) or competing data sets (containing masks generated by related methods), when the test benchmarks contain masked faces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=hjsJraoqn1Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/securifai/masked_faces" target="_blank" rel="nofollow noreferrer">https://github.com/securifai/masked_faces</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/securifai/masked_faces" target="_blank" rel="nofollow noreferrer">https://github.com/securifai/masked_faces</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license (https://creativecommons.org/licenses/by-nc-sa/4.0/)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9KArJb4r5ZQ" data-number="203">
        <h4>
          <a href="/forum?id=9KArJb4r5ZQ">
              COVID-19 Sounds: A Large-Scale Audio Dataset for Digital Respiratory Screening
          </a>


            <a href="/pdf?id=9KArJb4r5ZQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tong_Xia2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tong_Xia2">Tong Xia</a>, <a href="/profile?id=~Dimitris_Spathis2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dimitris_Spathis2">Dimitris Spathis</a>, <a href="/profile?email=clb76%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="clb76@cam.ac.uk">Chlo{\"e} Brown</a>, <a href="/profile?id=~J_Ch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Ch1">J Ch</a>, <a href="/profile?id=~Andreas_Grammenos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Grammenos1">Andreas Grammenos</a>, <a href="/profile?id=~Jing_Han4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Han4">Jing Han</a>, <a href="/profile?id=~Apinan_Hasthanasombat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Apinan_Hasthanasombat1">Apinan Hasthanasombat</a>, <a href="/profile?email=eb729%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="eb729@cam.ac.uk">Erika Bondareva</a>, <a href="/profile?id=~Ting_Dang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ting_Dang1">Ting Dang</a>, <a href="/profile?email=arf27%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="arf27@cam.ac.uk">Andres Floto</a>, <a href="/profile?email=pc245%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="pc245@cam.ac.uk">Pietro Cicuta</a>, <a href="/profile?email=cm542%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="cm542@cam.ac.uk">Cecilia Mascolo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#9KArJb4r5ZQ-details-603" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9KArJb4r5ZQ-details-603"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">audio dataset, respiratory sounds, COVID-19, mobile health, machine learning for health</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Audio signals are widely recognised as powerful indicators of overall health status, and there has been increasing interest in leveraging sound for affordable COVID-19 screening through machine learning. However, there has also been scepticism regarding the initial efforts, due to perhaps the lack of reproducibility, large datasets and transparency which unfortunately is often an issue with machine learning for health. To facilitate the advancement and openness of audio-based machine learning for respiratory health, we release a dataset consisting of 53,449 audio samples (over 552 hours in total) crowd-sourced from 36,116 participants through our COVID-19 Sounds app. Given its scale, this dataset is comprehensive in terms of demographics and spectrum of health conditions. It also provides participants' self-reported COVID-19 testing status with 2,106 samples tested positive. To the best of our knowledge, COVID-19 Sounds is the largest multi-modal dataset of COVID-19 respiratory sounds: it consists of three modalities including breathing, cough, and voice recordings. Additionally, in this paper, we report on several benchmarks for two principal research tasks: respiratory symptoms prediction and COVID-19 prediction. For these tasks we demonstrate performance with a ROC-AUC of over 0.7, confirming both the promise of machine learning approaches based on these types of datasets as well as the usability of our data for such tasks.  We describe a realistic experimental setting that hopes to pave the way to a fair performance evaluation of future models. In addition, we reflect on how the released dataset can help to scale some existing studies and enable new research directions, which inspire and benefit a wide range of future works.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9KArJb4r5ZQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">This dataset is available on request. </span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="eLYinD0TtIt" data-number="166">
        <h4>
          <a href="/forum?id=eLYinD0TtIt">
              Pl@ntNet-300K: a plant image dataset with high label ambiguity and a long-tailed distribution
          </a>


            <a href="/pdf?id=eLYinD0TtIt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Camille_Garcin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Camille_Garcin1">Camille Garcin</a>, <a href="/profile?id=~alexis_joly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~alexis_joly1">alexis joly</a>, <a href="/profile?id=~Pierre_Bonnet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre_Bonnet1">Pierre Bonnet</a>, <a href="/profile?id=~Antoine_Affouard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antoine_Affouard1">Antoine Affouard</a>, <a href="/profile?id=~Jean-Christophe_Lombardo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Christophe_Lombardo1">Jean-Christophe Lombardo</a>, <a href="/profile?email=mathias.chouet%40inria.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="mathias.chouet@inria.fr">Mathias Chouet</a>, <a href="/profile?id=~Maximilien_Servajean2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilien_Servajean2">Maximilien Servajean</a>, <a href="/profile?id=~Titouan_Lorieul1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Titouan_Lorieul1">Titouan Lorieul</a>, <a href="/profile?id=~Joseph_Salmon2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_Salmon2">Joseph Salmon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#eLYinD0TtIt-details-482" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eLYinD0TtIt-details-482"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, ambiguity, top-k, set-valued classification, long tail, plant recognition, image classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents a novel image dataset with high intrinsic ambiguity specifically built for evaluating and comparing set-valued classifers.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a novel image dataset with high intrinsic ambiguity specifically built for evaluating and comparing set-valued classifiers. This dataset, built from the database of Pl@ntnet citizen observatory, consists of 306,146 images covering 1,081 species. We highlight two particular features of the dataset, inherent to the way the images are acquired and to the intrinsic diversity of plants morphology:
            i) The dataset has a strong class imbalance, meaning that a few species account for most of the images.
            ii) Many species are visually similar, making identification difficult even for the expert eye.
        These two characteristics make the present dataset well suited for the evaluation of set-valued classification methods and algorithms. Therefore, we recommend two set-valued evaluation metrics associated with the dataset (mean top-k accuracy and mean average-k accuracy) and we provide the results of a baseline approach based on a deep neural network trained with the cross-entropy loss.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=eLYinD0TtIt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.5645731" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.5645731</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Y6sH0l4PExm" data-number="135">
        <h4>
          <a href="/forum?id=Y6sH0l4PExm">
              Intelligent Sight and Sound: A Chronic Cancer Facial Pain Dataset
          </a>


            <a href="/pdf?id=Y6sH0l4PExm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Catherine_Ordun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_Ordun1">Catherine Ordun</a>, <a href="/profile?email=zan_alexandra%40bah.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zan_alexandra@bah.com">Alexandra Cha</a>, <a href="/profile?id=~Edward_Raff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Raff1">Edward Raff</a>, <a href="/profile?email=gaskin_byron%40bah.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gaskin_byron@bah.com">Byron Gaskin</a>, <a href="/profile?email=hanson_alexander%40bah.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hanson_alexander@bah.com">Alexander Hanson</a>, <a href="/profile?email=mason.rule%40nih.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="mason.rule@nih.gov">Mason Rule</a>, <a href="/profile?id=~Sanjay_Purushotham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjay_Purushotham1">Sanjay Purushotham</a>, <a href="/profile?email=gulleyj%40mail.nih.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="gulleyj@mail.nih.gov">James Gulley</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 29 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#Y6sH0l4PExm-details-876" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y6sH0l4PExm-details-876"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">pain detection, facial emotion recognition, multimodal, healthcare, cancer</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset for chronic cancer pain facial detection collected from smartphone videos, collected in an ongoing clinical trial of cancer patients at the National Institute of Health.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Cancer patients experience high rates of chronic pain throughout the treatment process. Assessing pain for this patient population is a vital component of psychological and functional well-being, as it can cause a rapid deterioration of quality of life. Existing work in facial pain detection often have deficiencies in labeling or methodology that prevent them from being clinically relevant.  This paper introduces the first chronic cancer pain dataset, collected as part of the Intelligent Sight and Sound (ISS) clinical trial, guided by clinicians to help ensure that model findings yield clinically relevant results. The data collected to date consists of 29 patients, 509 smartphone videos, 189,999 frames, and self-reported affective and activity pain scores adopted from the Brief Pain Inventory (BPI). Using static images and multi-modal data to predict self-reported pain levels, early models show significant gaps between current methods available to predict pain today, with room for improvement. Due to the especially sensitive nature of the inherent Personally Identifiable Information (PII) of facial images, the dataset will be released under the guidance and control of the National Institutes of Health (NIH). </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Y6sH0l4PExm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="vhjsBtq9OxO" data-number="132">
        <h4>
          <a href="/forum?id=vhjsBtq9OxO">
              HumBugDB: A Large-scale Acoustic Mosquito Dataset
          </a>


            <a href="/pdf?id=vhjsBtq9OxO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ivan_Kiskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Kiskin1">Ivan Kiskin</a>, <a href="/profile?id=~Marianne_Sinka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marianne_Sinka1">Marianne Sinka</a>, <a href="/profile?id=~Adam_D._Cobb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_D._Cobb1">Adam D. Cobb</a>, <a href="/profile?id=~Waqas_Rafique1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Waqas_Rafique1">Waqas Rafique</a>, <a href="/profile?email=lawrence.wang%40eng.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="lawrence.wang@eng.ox.ac.uk">Lawrence Wang</a>, <a href="/profile?email=davide.zilli%40mindfoundry.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="davide.zilli@mindfoundry.ai">Davide Zilli</a>, <a href="/profile?email=beng%40robots.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="beng@robots.ox.ac.uk">Benjamin Gutteridge</a>, <a href="/profile?email=rinita.dam%40zoo.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="rinita.dam@zoo.ox.ac.uk">Rinita Dam</a>, <a href="/profile?email=tm00591%40surrey.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="tm00591@surrey.ac.uk">Theodoros Marinos</a>, <a href="/profile?id=~Yunpeng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunpeng_Li1">Yunpeng Li</a>, <a href="/profile?email=dmsaky%40ihi.or.otz" class="profile-link" data-toggle="tooltip" data-placement="top" title="dmsaky@ihi.or.otz">Dickson Msaky</a>, <a href="/profile?id=~Emmanuel_Kaindoa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emmanuel_Kaindoa1">Emmanuel Kaindoa</a>, <a href="/profile?email=gerard.killeen%40ucc.ie" class="profile-link" data-toggle="tooltip" data-placement="top" title="gerard.killeen@ucc.ie">Gerard Killeen</a>, <a href="/profile?email=eva.herreros-moya%40zoo.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="eva.herreros-moya@zoo.ox.ac.uk">Eva Herreros-Moya</a>, <a href="/profile?id=~Kathy_Willis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kathy_Willis1">Kathy Willis</a>, <a href="/profile?id=~Stephen_J._Roberts1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_J._Roberts1">Stephen J. Roberts</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#vhjsBtq9OxO-details-760" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vhjsBtq9OxO-details-760"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Acoustic machine learning, audio event detection, audio classification, mosquito detection, Bayesian deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Large-scale multi-species dataset of acoustic recordings of mosquitoes, with Bayesian convolutional neural network detection and classification models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents the first large-scale multi-species dataset of acoustic recordings of mosquitoes tracked continuously in free flight. We present 20 hours of audio recordings that we have expertly labelled and tagged precisely in time. Significantly, 18 hours of recordings contain annotations from 36 different species. Mosquitoes are well-known carriers of diseases such as malaria, dengue and yellow fever. Collecting this dataset is motivated by the need to assist applications which utilise mosquito acoustics to conduct surveys to help predict outbreaks and inform intervention policy. The task of detecting mosquitoes from the sound of their wingbeats is challenging due to the difficulty in collecting recordings from realistic scenarios. To address this, as part of the HumBug project, we conducted global experiments to record mosquitoes ranging from those bred in culture cages to mosquitoes captured in the wild. Consequently, the audio recordings vary in signal-to-noise ratio and contain a broad range of indoor and outdoor background environments from Tanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in detail how we collected, labelled and curated the data.  The data is provided from a PostgreSQL database, which captures important metadata such as the capture method, age, feeding status and gender of the mosquitoes. Additionally, we provide code to extract features and train Bayesian convolutional neural networks for two key tasks: the identification of mosquitoes from their corresponding background environments, and the classification of detected mosquitoes into species. Our extensive dataset is both challenging to machine learning researchers focusing on acoustic identification, and critical to entomologists, geo-spatial modellers and other domain experts to understand mosquito behaviour, model their distribution, and manage the threat they pose to humans.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=vhjsBtq9OxO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Dataset: https://doi.org/10.5281/zenodo.4904800, Code: https://github.com/HumBug-Mosquito/HumBugDB</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset: https://zenodo.org/record/4904800. Code: https://github.com/HumBug-Mosquito/HumBugDB.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset: CC-BY-4.0 license. Code: MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="WV0waZz9dTF" data-number="119">
        <h4>
          <a href="/forum?id=WV0waZz9dTF">
              Constructing a Visual Dataset to Study the Effects of Spatial Apartheid in South Africa
          </a>


            <a href="/pdf?id=WV0waZz9dTF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Raesetje_Sefala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raesetje_Sefala1">Raesetje Sefala</a>, <a href="/profile?id=~Timnit_Gebru1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timnit_Gebru1">Timnit Gebru</a>, <a href="/profile?email=lmfupe%40csir.co.za" class="profile-link" data-toggle="tooltip" data-placement="top" title="lmfupe@csir.co.za">Luzango Mfupe</a>, <a href="/profile?id=~Nyalleng_Moorosi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nyalleng_Moorosi1">Nyalleng Moorosi</a>, <a href="/profile?id=~Richard_Klein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Klein1">Richard Klein</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 18 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#WV0waZz9dTF-details-794" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WV0waZz9dTF-details-794"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">satellite imagery, datasets, segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Dataset creation methodology to construct a visual dataset to study Spatial Apartheid in South Africa. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Aerial images of neighborhoods in South Africa show the clear legacy of Apartheid, a former policy of political and economic discrimination against non-European groups, with completely segregated neighborhoods of townships next to gated wealthy areas. This paper introduces the first publicly available dataset to study the evolution of spatial apartheid, using 6,768 high resolution satellite images of 9 provinces in South Africa. Our dataset was created using polygons demarcating land use, geographically labelled coordinates of buildings in South Africa, and high resolution satellite imagery covering the country from 2006-2017. We describe our iterative process to create this dataset, which includes pixel wise labels for 4 classes of neighborhoods: wealthy areas, non wealthy areas, non residential neighborhoods and vacant land. While datasets 7 times smaller than ours have cost over 1M to annotate, our dataset was created with highly constrained resources. We finally show examples of applications examining the evolution of neighborhoods in South Africa using our dataset.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=WV0waZz9dTF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sefalab/Spatial_Project" target="_blank" rel="nofollow noreferrer">https://github.com/sefalab/Spatial_Project</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="y2AbfIXgBK3" data-number="95">
        <h4>
          <a href="/forum?id=y2AbfIXgBK3">
              VFP290K: A Large-Scale Benchmark Dataset for Vision-based Fallen Person Detection
          </a>


            <a href="/pdf?id=y2AbfIXgBK3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jaeju_An1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaeju_An1">Jaeju An</a>, <a href="/profile?id=~Jeongho_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeongho_Kim2">Jeongho Kim</a>, <a href="/profile?email=gksqls5707%40g.skku.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gksqls5707@g.skku.edu">Hanbeen Lee</a>, <a href="/profile?email=kjinb1212%40g.skku.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kjinb1212@g.skku.edu">Jinbeom Kim</a>, <a href="/profile?id=~Junhyung_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junhyung_Kang1">Junhyung Kang</a>, <a href="/profile?id=~Minha_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minha_Kim2">Minha Kim</a>, <a href="/profile?id=~Saebyeol_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saebyeol_Shin1">Saebyeol Shin</a>, <a href="/profile?id=~Minha_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minha_Kim1">Minha Kim</a>, <a href="/profile?id=~Donghee_Hong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Donghee_Hong2">Donghee Hong</a>, <a href="/profile?id=~Simon_S._Woo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_S._Woo1">Simon S. Woo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#y2AbfIXgBK3-details-18" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="y2AbfIXgBK3-details-18"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fall detection, computer vision, object detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present VFP290K, a dataset of fallen person in real world based on diverse environments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Detection of fallen persons due to, for example, health problems, violence, or accidents, is a critical challenge. Accordingly, detection of these anomalous events is of paramount importance for a number of applications, including but not limited to CCTV surveillance, security, and health care. Given that many detection systems rely on a comprehensive dataset comprising fallen person images collected under diverse environments and in various situations is crucial. However, existing datasets are limited to only specific environmental conditions and lack diversity. To address the above challenges and help researchers develop more robust detection systems, we create a novel, large-scale dataset for the detection of fallen persons composed of fallen person images collected in various real-world scenarios, with the support of the South Korean government. Our Vision-based Fallen Person (VFP290K) dataset consists of 294,713 frames of fallen persons extracted from 178 videos, including 131 scenes in 49 locations. We empirically demonstrate the effectiveness of the features through extensive experiments analyzing the performance shift based on object detection models. In addition, we evaluate our VFP290K dataset with properly divided versions of our dataset by measuring the performance of fallen person detecting systems. We ranked first in the first round of the anomalous behavior recognition track of AI Grand Challenge 2020, South Korea, using our VFP290K dataset, which can be found here. Our achievement implies the usefulness of our dataset for research on fallen person detection, which can further extend to other applications, such as intelligent CCTV or monitoring systems. The data and more up-to-date information have been provided at our VFP290K site.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=y2AbfIXgBK3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/dash-vfp300k/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/dash-vfp300k/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/dash-vfp300k/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/dash-vfp300k/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset: Attribution 4.0 International (CC BY 4.0)
        Code: GNU General Public License v3.0 (GPL-3.0 License)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="OFiGmksrSz1" data-number="39">
        <h4>
          <a href="/forum?id=OFiGmksrSz1">
              SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation
          </a>


            <a href="/pdf?id=OFiGmksrSz1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Robin_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robin_Chan1">Robin Chan</a>, <a href="/profile?id=~Krzysztof_Lis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krzysztof_Lis1">Krzysztof Lis</a>, <a href="/profile?id=~Svenja_Uhlemeyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Svenja_Uhlemeyer1">Svenja Uhlemeyer</a>, <a href="/profile?id=~Hermann_Blum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hermann_Blum1">Hermann Blum</a>, <a href="/profile?id=~Sina_Honari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sina_Honari1">Sina Honari</a>, <a href="/profile?id=~Roland_Siegwart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roland_Siegwart1">Roland Siegwart</a>, <a href="/profile?id=~Pascal_Fua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pascal_Fua1">Pascal Fua</a>, <a href="/profile?id=~Mathieu_Salzmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathieu_Salzmann1">Mathieu Salzmann</a>, <a href="/profile?id=~Matthias_Rottmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Rottmann1">Matthias Rottmann</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">16 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#OFiGmksrSz1-details-29" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OFiGmksrSz1-details-29"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">anomaly detection, anomaly segmentation, semantic segmentation, autonomous driving</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A benchmark for anomaly segmentation of street scenes with two novel datasets for two separate tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects.
        However, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the "SegmentMeIfYouCan" benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown.
        We provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several models specifically designed for anomaly / obstacle segmentation, on our datasets and on public ones, using our test suite.
        The anomaly and obstacle segmentation results show that our datasets contribute to the diversity and difficulty of both data landscapes.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=OFiGmksrSz1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://segmentmeifyoucan.com/" target="_blank" rel="nofollow noreferrer">https://segmentmeifyoucan.com/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="74TZg9gsO8W" data-number="16">
        <h4>
          <a href="/forum?id=74TZg9gsO8W">
              WaveFake: A Data Set to Facilitate Audio Deepfake Detection
          </a>


            <a href="/pdf?id=74TZg9gsO8W" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Joel_Frank1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joel_Frank1">Joel Frank</a>, <a href="/profile?id=~Lea_Sch%C3%B6nherr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lea_Schönherr1">Lea Schönherr</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">12 Aug 2021 (modified: 05 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#74TZg9gsO8W-details-585" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="74TZg9gsO8W-details-585"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generative model, audio Deepfake detection, signal processing, data set</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a novel data set for facilitating research into audio Deepfake detection. Additionally, we review common signal processing techniques and provide baseline models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep generative modeling has the potential to cause significant harm to society. Recognizing this threat, a magnitude of research into detecting so-called "Deepfakes'' has emerged. This research most often focuses on the image domain, while studies exploring generated audio signals have---so-far---been neglected. In this paper we make three key contributions to narrow this gap. First, we provide researchers with an introduction to common signal processing techniques used for analyzing audio signals. Second, we present a novel data set, for which we collected nine sample sets from five different network architectures, spanning two languages. Finally, we supply practitioners with two baseline models, adopted from the signal processing community, to facilitate further research in this area.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=74TZg9gsO8W&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/5642694" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/5642694</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/5642694" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/5642694</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This data set is licensed with a CC-BY-SA 4.0 license.

        You can find the accompanying code repository at: https://github.com/RUB-SysSec/WaveFake</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="QzNHE7QHhut" data-number="193">
        <h4>
          <a href="/forum?id=QzNHE7QHhut">
              The Tufts fNIRS Mental Workload Dataset &amp; Benchmark for Brain-Computer Interfaces that Generalize
          </a>


            <a href="/pdf?id=QzNHE7QHhut" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhe_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhe_Huang2">Zhe Huang</a>, <a href="/profile?id=~Liang_Wang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Wang9">Liang Wang</a>, <a href="/profile?email=giles.blaney%40tufts.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="giles.blaney@tufts.edu">Giles Blaney</a>, <a href="/profile?email=cslaugh1%40umbc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="cslaugh1@umbc.edu">Christopher Slaughter</a>, <a href="/profile?email=devon.mckeon%40tufts.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="devon.mckeon@tufts.edu">Devon McKeon</a>, <a href="/profile?id=~Ziyu_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyu_Zhou1">Ziyu Zhou</a>, <a href="/profile?id=~Robert_Jacob1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Jacob1">Robert Jacob</a>, <a href="/profile?id=~Michael_C_Hughes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_C_Hughes1">Michael C Hughes</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#QzNHE7QHhut-details-132" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QzNHE7QHhut-details-132"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">time-series classification, fine-tuning, supervised domain adaptation, brain-computer interface, functional near-infrared spectroscopy, n-back, mental workload, cognitive workload, working memory workload</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release a large dataset of labeled fNIRS recordings and define a standardized evaluation protocol to enable benchmarking progress in mental workload classification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Functional near-infrared spectroscopy (fNIRS) promises a non-intrusive way to measure real-time brain activity and build responsive brain-computer interfaces. A primary barrier to realizing this technology's potential has been that observed fNIRS signals vary significantly across human users. Building models that generalize well to never-before-seen users has been difficult; a large amount of subject-specific data has been needed to train effective models. To help overcome this barrier, we introduce the largest open-access dataset of its kind, containing multivariate fNIRS recordings from 68 participants, each with labeled segments indicating four possible mental workload intensity levels. Labels were collected via a controlled setting in which subjects performed standard n-back tasks to induce desired working memory levels. We propose a benchmark analysis of this dataset with a standardized training and evaluation protocol, which allows future researchers to report comparable numbers and fairly assess generalization potential while avoiding any overlap or leakage between train and test data. Using this dataset and benchmark, we show how models trained using abundant fNIRS data from many other participants can effectively classify a new target subject's data, thus reducing calibration and setup time for new subjects. We further show how performance improves as the size of the available dataset grows, while also analyzing error rates across key subpopulations to audit equity concerns. We share our open-access Tufts fNIRS to Mental Workload (fNIRS2MW) dataset and open-source code as a step toward advancing brain computer interfaces.
        </span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://tufts-hci-lab.github.io/code_and_datasets/fNIRS2MW.html" target="_blank" rel="nofollow noreferrer">https://tufts-hci-lab.github.io/code_and_datasets/fNIRS2MW.html</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MAWgLrYvMs0" data-number="112">
        <h4>
          <a href="/forum?id=MAWgLrYvMs0">
              The CPD Data Set: Personnel, Use of Force, and Complaints in the Chicago Police Department
          </a>


            <a href="/pdf?id=MAWgLrYvMs0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Thibaut_Horel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thibaut_Horel1">Thibaut Horel</a>, <a href="/profile?id=~Lorenzo_Masoero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lorenzo_Masoero1">Lorenzo Masoero</a>, <a href="/profile?email=r.agrawal%40csail.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="r.agrawal@csail.mit.edu">Raj Agrawal</a>, <a href="/profile?id=~Daria_Roithmayr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daria_Roithmayr1">Daria Roithmayr</a>, <a href="/profile?id=~Trevor_Campbell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trevor_Campbell1">Trevor Campbell</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 08 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#MAWgLrYvMs0-details-885" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MAWgLrYvMs0-details-885"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Chicago, Use of Force, Complaints, Police Misconduct, Network Analysis, Spatiotemporal Analysis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a new dataset that contains information on the personnel, activities, use of force, and complaints in the Chicago Police Department (CPD).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The lack of accessibility to data on policing has severely limited researchers’ ability to conduct thorough quantitative analyses on police activity and behavior, particularly with regard to predicting and explaining police violence. In the present work, we provide a new dataset that contains information on the personnel, activities, use of force, and complaints in the Chicago Police Department (CPD). The raw data, obtained from the CPD via a series of requests under the Freedom of Information Act (FOIA), consists of 35 unlinked, inconsistent, and undocumented spreadsheets. Our paper provides a cleaned, linked, and documented version of this data that can be reproducibly generated via open source code. We provide a detailed description of the dataset contents, the procedures for cleaning the data, and summary statistics. The data have a rich variety of uses, such as prediction (e.g., predicting misconduct from officer traits, experience, and assigned units), network analysis (e.g., detecting communities within the social network of officers co-listed on complaints), spatiotemporal data analysis (e.g., investigating patterns of officer shooting events), causal inference (e.g., tracking the effects of new disciplinary practices, new training techniques, and new oversight on complaints and use of force), and much more. Access to this dataset will enable the machine learning community to meaningfully engage with the problem of police violence.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MAWgLrYvMs0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/chicago-police-violence/data/releases/download/v0.1/cpd-dataset-v0.1.zip" target="_blank" rel="nofollow noreferrer">https://github.com/chicago-police-violence/data/releases/download/v0.1/cpd-dataset-v0.1.zip</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rNs2FvJGDK" data-number="126">
        <h4>
          <a href="/forum?id=rNs2FvJGDK">
              DUE: End-to-End Document Understanding Benchmark
          </a>


            <a href="/pdf?id=rNs2FvJGDK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~%C5%81ukasz_Borchmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Łukasz_Borchmann1">Łukasz Borchmann</a>, <a href="/profile?id=~Micha%C5%82_Pietruszka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michał_Pietruszka1">Michał Pietruszka</a>, <a href="/profile?id=~Tomasz_Stanislawek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomasz_Stanislawek1">Tomasz Stanislawek</a>, <a href="/profile?id=~Dawid_Jurkiewicz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawid_Jurkiewicz1">Dawid Jurkiewicz</a>, <a href="/profile?id=~Micha%C5%82_Turski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michał_Turski1">Michał Turski</a>, <a href="/profile?id=~Karolina_Szyndler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karolina_Szyndler1">Karolina Szyndler</a>, <a href="/profile?id=~Filip_Grali%C5%84ski2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Filip_Graliński2">Filip Graliński</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#rNs2FvJGDK-details-977" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rNs2FvJGDK-details-977"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Document Understanding, Multi-modal Models, Language Models, NLP, Multimodal Data, Key Information Extraction, Question Answering, Information Extraction, Table Comprehension, KIE, NLI, Visual QA, Layout-aware Language Models</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Description of a benchmark spanning multiple end-to-end tasks related to understanding multi-modal documents with complex layouts.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community. Additionally, the lack of a commonly accepted benchmark made it difficult to quantify progress in the domain. To empower research in this field, we introduce the Document Understanding Evaluation (DUE) benchmark consisting of both available and reformulated datasets to measure the end-to-end capabilities of systems in real-world scenarios.
        The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains and layouts featuring tables, graphs, lists, and infographics. In addition, the current study reports systematic baselines and analyzes challenges in currently available datasets using recent advances in layout-aware language modeling. We open both the benchmarks and reference implementations and make them available at https://duebenchmark.com and https://github.com/due-benchmark.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rNs2FvJGDK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://duebenchmark.com/" target="_blank" rel="nofollow noreferrer">https://duebenchmark.com/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://duebenchmark.com" target="_blank" rel="nofollow noreferrer">https://duebenchmark.com</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="b3Zoeq2sCLq" data-number="195">
        <h4>
          <a href="/forum?id=b3Zoeq2sCLq">
              KeSpeech: An Open Source Speech Dataset of Mandarin and Its Eight Subdialects
          </a>


            <a href="/pdf?id=b3Zoeq2sCLq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhiyuan_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyuan_Tang1">Zhiyuan Tang</a>, <a href="/profile?email=wangdong99%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangdong99@mails.tsinghua.edu.cn">Dong Wang</a>, <a href="/profile?email=xuyanguang001%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xuyanguang001@ke.com">Yanguang Xu</a>, <a href="/profile?email=sunjianwei006%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sunjianwei006@ke.com">Jianwei Sun</a>, <a href="/profile?email=leixiaoning001%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="leixiaoning001@ke.com">Xiaoning Lei</a>, <a href="/profile?email=zhaoshuaijiang001%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaoshuaijiang001@ke.com">Shuaijiang Zhao</a>, <a href="/profile?id=~Cheng_Wen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Wen3">Cheng Wen</a>, <a href="/profile?email=tanxingjun001%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tanxingjun001@ke.com">Xingjun Tan</a>, <a href="/profile?email=xiechuandong003%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiechuandong003@ke.com">Chuandong Xie</a>, <a href="/profile?email=zhoushuran002%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhoushuran002@ke.com">Shuran Zhou</a>, <a href="/profile?email=yanrui020%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yanrui020@ke.com">Rui Yan</a>, <a href="/profile?email=lvchenjia001%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lvchenjia001@ke.com">Chenjia Lv</a>, <a href="/profile?email=hanyang030%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hanyang030@ke.com">Yang Han</a>, <a href="/profile?email=zouwei026%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zouwei026@ke.com">Wei Zou</a>, <a href="/profile?email=lixiangang002%40ke.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lixiangang002@ke.com">Xiangang Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 10 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#b3Zoeq2sCLq-details-170" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b3Zoeq2sCLq-details-170"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Mandarin dialect, speech recognition, speaker verification, subdialect identification, voice conversion</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces an open source speech dataset, KeSpeech, which involves 1,542 hours of speech signals recorded by 27,237 speakers in 34 cities in China, and the pronunciation includes standard Mandarin and its 8 subdialects. The new dataset possesses several properties. Firstly, the dataset provides multiple labels including content transcription, speaker identity and subdialect, hence supporting a variety of speech processing tasks, such as speech recognition, speaker recognition, and subdialect identification, as well as other advanced techniques like multi-task learning and conditional learning. Secondly, some of the text samples were parallel recorded with both the standard Mandarin and a particular subdialect, allowing for new applications such as subdialect style conversion. Thirdly, the number of speakers is much larger than other open-source datasets, making it suitable for tasks that require training data from vast speakers. Finally, the speech signals were recorded in two phases, which opens the opportunity for the study of the time variance property of human speech. We present the design principle of the KeSpeech dataset and four baseline systems based on the new data resource: speech recognition, speaker verification, subdialect identification and voice conversion. The dataset is free for all academic usage.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=b3Zoeq2sCLq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/KeSpeech/KeSpeech" target="_blank" rel="nofollow noreferrer">https://github.com/KeSpeech/KeSpeech</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Jul-uX7EV_I" data-number="170">
        <h4>
          <a href="/forum?id=Jul-uX7EV_I">
              SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables
          </a>


            <a href="/pdf?id=Jul-uX7EV_I" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nafise_Sadat_Moosavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nafise_Sadat_Moosavi1">Nafise Sadat Moosavi</a>, <a href="/profile?id=~Andreas_R%C3%BCckl%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Rücklé1">Andreas Rücklé</a>, <a href="/profile?id=~Dan_Roth3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Roth3">Dan Roth</a>, <a href="/profile?id=~Iryna_Gurevych1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iryna_Gurevych1">Iryna Gurevych</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 03 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#Jul-uX7EV_I-details-418" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jul-uX7EV_I-details-418"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Benchmark, Reasoning, Data-to-Text Generation, Scientific Articles</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we introduce the first reasoning-aware data-to-text generation dataset based on scientific articles and propose a pipeline for automatically extracting high-quality unsupervised training data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce SciGen, a new challenge dataset consisting of tables from scientific articles and their corresponding descriptions, for the task of reasoning-aware data-to-text generation. Describing scientific tables goes beyond the surface realization of the table content and requires reasoning over table values. The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is the first dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, such as tables from scientific articles, and thus it opens new avenues for future research in reasoning-aware text generation and evaluation. The core part of SciGen, including the test data, is annotated by one of the authors of the corresponding articles. Such expert annotations do not scale to large training data sizes. To tackle this, we propose a pipeline for automatically extracting high-quality table-description pairs from the LaTeX sources of scientific articles. We study the effectiveness of state-of-the-art data-to-text generation models on SciGen and evaluate the results using common metrics and human evaluation. Our results and analyses show that adding high-quality unsupervised training data improves the correctness and reduces the hallucination in generated descriptions, however, the ability of state-of-the-art models is still severely limited on this task.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Jul-uX7EV_I&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/UKPLab/SciGen" target="_blank" rel="nofollow noreferrer">https://github.com/UKPLab/SciGen</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/UKPLab/SciGen" target="_blank" rel="nofollow noreferrer">https://github.com/UKPLab/SciGen</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="GEcWUTN1v1v" data-number="159">
        <h4>
          <a href="/forum?id=GEcWUTN1v1v">
              Native Chinese Reader: A Dataset Towards Native-Level Chinese Machine Reading Comprehension
          </a>


            <a href="/pdf?id=GEcWUTN1v1v" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shusheng_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shusheng_Xu1">Shusheng Xu</a>, <a href="/profile?id=~Yichen_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yichen_Liu2">Yichen Liu</a>, <a href="/profile?id=~Xiaoyu_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyu_Yi1">Xiaoyu Yi</a>, <a href="/profile?id=~Siyuan_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyuan_Zhou2">Siyuan Zhou</a>, <a href="/profile?id=~Huizi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huizi_Li1">Huizi Li</a>, <a href="/profile?id=~Yi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wu1">Yi Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 16 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#GEcWUTN1v1v-details-330" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GEcWUTN1v1v-details-330"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">machine reading comprehension, Chinese dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We developed a novel Chinsese machine reading comprehension dataset that covers a wide range of Chinese writing styles and requires native-level reasoning abilities to tackle the questions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present Native Chinese Reader (NCR),  a new machine reading comprehension  MRC) dataset with particularly long articles in both modern and classical Chinese. NCR is collected from the exam questions for the Chinese course in China’s high schools, which are designed to evaluate the language proficiency of native Chinese youth.  Existing Chinese MRC datasets are either domain-specific or focusing on short contexts of a few hundred characters in modern Chinese only. By contrast, NCR contains 8390 documents with an average length of 1024 characters covering a wide range of Chinese writing styles, including modern articles, classical literature and classical poetry.  A total of  20477  questions on these documents also require strong reasoning abilities and common sense to figure out the correct answers. We implemented multiple baseline models using popular Chinese pre-trained models and additionally launched an online competition using our dataset to examine the limit of current methods.  The best model achieves 59% test accuracy while human evaluation shows an average accuracy of 79%, which indicates a significant performance gap between current MRC models and native Chinese speakers.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=GEcWUTN1v1v&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/native-chinese-reader" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/native-chinese-reader</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/native-chinese-reader" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/native-chinese-reader</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This dataset is released under the CC BY-SA 4.0 license for general research purposes.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="q-8h8-LZiUm" data-number="93">
        <h4>
          <a href="/forum?id=q-8h8-LZiUm">
              KLUE: Korean Language Understanding Evaluation
          </a>


            <a href="/pdf?id=q-8h8-LZiUm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sungjoon_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungjoon_Park1">Sungjoon Park</a>, <a href="/profile?id=~Jihyung_Moon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihyung_Moon1">Jihyung Moon</a>, <a href="/profile?id=~Sungdong_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungdong_Kim1">Sungdong Kim</a>, <a href="/profile?id=~Won_Ik_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Won_Ik_Cho1">Won Ik Cho</a>, <a href="/profile?id=~Ji_Yoon_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ji_Yoon_Han1">Ji Yoon Han</a>, <a href="/profile?id=~Jangwon_Park2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jangwon_Park2">Jangwon Park</a>, <a href="/profile?email=daydrilling%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="daydrilling@gmail.com">Chisung Song</a>, <a href="/profile?id=~Junseong_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junseong_Kim2">Junseong Kim</a>, <a href="/profile?id=~Youngsook_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngsook_Song1">Youngsook Song</a>, <a href="/profile?id=~Taehwan_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taehwan_Oh1">Taehwan Oh</a>, <a href="/profile?id=~Joohong_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joohong_Lee1">Joohong Lee</a>, <a href="/profile?email=411juhyun%40snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="411juhyun@snu.ac.kr">Juhyun Oh</a>, <a href="/profile?id=~Sungwon_Lyu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungwon_Lyu1">Sungwon Lyu</a>, <a href="/profile?id=~Younghoon_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Younghoon_Jeong1">Younghoon Jeong</a>, <a href="/profile?email=md98765%40naver.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="md98765@naver.com">Inkwon Lee</a>, <a href="/profile?email=sangwoo%40scatterlab.co.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="sangwoo@scatterlab.co.kr">Sangwoo Seo</a>, <a href="/profile?id=~Dongjun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongjun_Lee1">Dongjun Lee</a>, <a href="/profile?id=~Hyunwoo_Kim3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyunwoo_Kim3">Hyunwoo Kim</a>, <a href="/profile?id=~Myeonghwa_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Myeonghwa_Lee1">Myeonghwa Lee</a>, <a href="/profile?id=~Seongbo_Jang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seongbo_Jang1">Seongbo Jang</a>, <a href="/profile?id=~Seungwon_Do1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungwon_Do1">Seungwon Do</a>, <a href="/profile?id=~Sunkyoung_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sunkyoung_Kim1">Sunkyoung Kim</a>, <a href="/profile?email=ktlim%40hanbat.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="ktlim@hanbat.ac.kr">Kyungtae Lim</a>, <a href="/profile?id=~Jongwon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongwon_Lee1">Jongwon Lee</a>, <a href="/profile?id=~Kyumin_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyumin_Park1">Kyumin Park</a>, <a href="/profile?id=~Jamin_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jamin_Shin1">Jamin Shin</a>, <a href="/profile?id=~Seonghyun_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seonghyun_Kim2">Seonghyun Kim</a>, <a href="/profile?id=~Lucy_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucy_Park1">Lucy Park</a>, <a href="/profile?id=~Alice_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alice_Oh1">Alice Oh</a>, <a href="/profile?id=~Jung-Woo_Ha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jung-Woo_Ha1">Jung-Woo Ha</a>, <a href="/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyunghyun_Cho1">Kyunghyun Cho</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 08 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#q-8h8-LZiUm-details-329" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q-8h8-LZiUm-details-329"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Korean Natural Language Understanding Benchmarks, Pre-trained Korean Language Models</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Releasing natural language understanding benchmarks for Korean including 8 tasks and pre-trained Korean language models for Korean as baseline models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of eight Korean natural language understanding (NLU) tasks, including Topic Classification, Semantic Textual Similarity, Natural LanguageInference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We create all of the datasets from scratch in a principled way. We design the tasks to have diverse formats and each task to be built upon various source corpora that respect copyrights. Also, we propose suitable evaluation metrics and organize annotation protocols in a way to ensure quality. To prevent ethical risks in KLUE, we proactively remove examples reflecting social biases, containing toxic content or personally identifiable information (PII). Along with the benchmark datasets, we release pre-trained language models (PLM) for Korean, KLUE-BERT and KLUE-RoBERTa, and find KLUE-Roberta-large outperforms other baselines including multilingual PLMs and existing open-source Korean PLMs. The fine-tuning recipes are publicly open for anyone to reproduce our baseline result. We believe our work will facilitate future research on cross-lingual as well as Korean language models and the creation of similar resources for other languages. KLUE is available at https://klue-benchmark.com.

        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=q-8h8-LZiUm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/KLUE-benchmark/KLUE" target="_blank" rel="nofollow noreferrer">https://github.com/KLUE-benchmark/KLUE</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://klue-benchmark.com/" target="_blank" rel="nofollow noreferrer">https://klue-benchmark.com/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY-SA-4.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Gln7zxMffae" data-number="52">
        <h4>
          <a href="/forum?id=Gln7zxMffae">
              Relational Pattern Benchmarking on the Knowledge Graph Link Prediction Task
          </a>


            <a href="/pdf?id=Gln7zxMffae" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Afshin_Sadeghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Afshin_Sadeghi1">Afshin Sadeghi</a>, <a href="/profile?email=s6hiabdu%40uni-bonn.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="s6hiabdu@uni-bonn.de">Hirra Abdul Malik</a>, <a href="/profile?id=~Diego_Collarana1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diego_Collarana1">Diego Collarana</a>, <a href="/profile?id=~Jens_Lehmann3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jens_Lehmann3">Jens Lehmann</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 04 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#Gln7zxMffae-details-749" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Gln7zxMffae-details-749"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Knowledge Graphs Embedding, Link Prediction, Benchmarking, Dataset, Relational Pattern, Inductive, Transductive, Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This article describes a dataset and a link prediction benchmark for inductive and transductive relational pattern learning on the Knowledge Graphs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Knowledge graphs (KGs) encode facts about the world in a graph data structure where entities, represented as nodes, connect via relationships, acting as edges. KGs are widely used in Machine Learning, e.g., to solve Natural Language Processing based tasks. Despite all the advancements in KGs, they plummet when it comes to completeness. Link Prediction based on KG embeddings targets the sparsity and incompleteness of KGs. Available datasets for Link Prediction do not consider different graph patterns, making it difficult to measure the performance of link prediction models on different KG settings. This paper presents a diverse set of pragmatic datasets to facilitate flexible and problem-tailored Link Prediction and Knowledge Graph Embeddings research. We define graph relational patterns, from being entirely inductive in one set to being transductive in the other. For each dataset, we provide uniform evaluation metrics. We analyze the models over our datasets to compare the model’s capabilities on a specific dataset type. Our analysis of datasets over state-of-the-art models provides a better insight into the suitable parameters for each situation, optimizing the KG-embedding-based systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Gln7zxMffae&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mlwin-de/relational_pattern_benchmarking" target="_blank" rel="nofollow noreferrer">https://github.com/mlwin-de/relational_pattern_benchmarking</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/mlwin-de/relational_pattern_benchmarking" target="_blank" rel="nofollow noreferrer">https://github.com/mlwin-de/relational_pattern_benchmarking</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY license is included in  https://github.com/mlwin-de/relational_pattern_benchmarking/blob/master/LICENSE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zfMtM7HZGLT" data-number="49">
        <h4>
          <a href="/forum?id=zfMtM7HZGLT">
              Benchmarks for Corruption Invariant Person Re-identification
          </a>


            <a href="/pdf?id=zfMtM7HZGLT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Minghui_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minghui_Chen1">Minghui Chen</a>, <a href="/profile?id=~Zhiqiang_Wang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiqiang_Wang5">Zhiqiang Wang</a>, <a href="/profile?id=~Feng_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feng_Zheng1">Feng Zheng</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">17 Aug 2021 (modified: 03 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#zfMtM7HZGLT-details-435" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zfMtM7HZGLT-details-435"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ReID, Robustness, Corruption, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We conduct large scale evaluation of corruption robustnees in person ReID and have several interesting findings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">When deploying person re-identification (ReID) model in safety-critical applications, it is pivotal to understanding the robustness of the model against a diverse array of image corruptions. However, current evaluations of person ReID only consider the performance on clean datasets and ignore images in various corrupted scenarios. In this work, we comprehensively establish five ReID benchmarks for learning corruption invariant representation. In the field of ReID, we are the first to conduct an exhaustive study on corruption invariant learning in single- and cross-modality datasets, including Market-1501, CUHK03, MSMT17, RegDB, SYSU-MM01. After reproducing and examining the robustness performance of 21 recent ReID methods, we have some observations:
         1) transformer-based models are more robust towards corrupted images, compared with CNN-based models,
         2) increasing the probability of random erasing (a commonly used augmentation method) hurts model corruption robustness,
         3) cross-dataset generalization improves with corruption robustness increases.
        By analyzing the above observations, we propose a strong baseline on both single- and cross-modality ReID datasets which achieves improved robustness against diverse corruptions.
        Our codes are available on https://github.com/MinghuiChen43/CIL-ReID.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zfMtM7HZGLT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/MinghuiChen43/CIL-ReID" target="_blank" rel="nofollow noreferrer">https://github.com/MinghuiChen43/CIL-ReID</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="7Bywt2mQsCe" data-number="232">
        <h4>
          <a href="/forum?id=7Bywt2mQsCe">
              Measuring Mathematical Problem Solving With the MATH Dataset
          </a>


            <a href="/pdf?id=7Bywt2mQsCe" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>, <a href="/profile?id=~Collin_Burns1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Collin_Burns1">Collin Burns</a>, <a href="/profile?id=~Saurav_Kadavath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saurav_Kadavath1">Saurav Kadavath</a>, <a href="/profile?email=akularora%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="akularora@berkeley.edu">Akul Arora</a>, <a href="/profile?id=~Steven_Basart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Basart1">Steven Basart</a>, <a href="/profile?id=~Eric_Tang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Tang2">Eric Tang</a>, <a href="/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>, <a href="/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 08 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#7Bywt2mQsCe-details-360" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7Bywt2mQsCe-details-360"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">math, transformers, reasoning, logic</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">To find the limits of Transformers, we collected 12,500 math problems. While a three-time IMO gold medalist got 90%, GPT-3 models got ~5%, with accuracy increasing slowly. If trends continue, ML models are far from achieving mathematical reasoning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=7Bywt2mQsCe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hendrycks/math" target="_blank" rel="nofollow noreferrer">https://github.com/hendrycks/math</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="5HR3vCylqD" data-number="312">
        <h4>
          <a href="/forum?id=5HR3vCylqD">
              SustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning
          </a>


            <a href="/pdf?id=5HR3vCylqD" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Christopher_Yeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Yeh1">Christopher Yeh</a>, <a href="/profile?id=~Chenlin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenlin_Meng1">Chenlin Meng</a>, <a href="/profile?id=~Sherrie_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sherrie_Wang1">Sherrie Wang</a>, <a href="/profile?id=~Anne_Driscoll1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anne_Driscoll1">Anne Driscoll</a>, <a href="/profile?id=~Erik_Rozi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Rozi1">Erik Rozi</a>, <a href="/profile?id=~Patrick_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Liu1">Patrick Liu</a>, <a href="/profile?id=~Jihyeon_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihyeon_Lee2">Jihyeon Lee</a>, <a href="/profile?id=~Marshall_Burke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marshall_Burke1">Marshall Burke</a>, <a href="/profile?id=~David_B._Lobell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_B._Lobell1">David B. Lobell</a>, <a href="/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">23 Aug 2021 (modified: 10 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">19 Replies</span>


        </div>

          <a href="#5HR3vCylqD-details-741" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5HR3vCylqD-details-741"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sustainable development goals, sdgs, social good, environmental science, sustainability, machine learning, benchmark, datasets</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we introduce SustainBench, a collection of 15 benchmark tasks across 7 SDGs, including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Progress toward the United Nations Sustainable Development Goals (SDGs) has been hindered by a lack of data on key environmental and socioeconomic indicators, which historically have come from ground surveys with sparse temporal and spatial coverage. Recent advances in machine learning have made it possible to utilize abundant, frequently-updated, and globally available data, such as from satellites or social media, to provide insights into progress toward SDGs. Despite promising early results, approaches to using such data for SDG measurement thus far have largely evaluated on different datasets or used inconsistent evaluation metrics, making it hard to understand whether performance is improving and where additional research would be most fruitful. Furthermore, processing satellite and ground survey data requires domain knowledge that many in the machine learning community lack. In this paper, we introduce SustainBench, a collection of 15 benchmark tasks across 7 SDGs, including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land. Datasets for 11 of the 15 tasks are released publicly for the first time. Our goals for SustainBench are to (1) lower the barriers to entry for the machine learning community to contribute to measuring and achieving the SDGs; (2) provide standard benchmarks for evaluating machine learning models on tasks across a variety of SDGs; and (3) encourage the development of novel machine learning methods where improved model performance facilitates progress towards the SDGs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=5HR3vCylqD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sustainlab-group/sustainbench/" target="_blank" rel="nofollow noreferrer">https://github.com/sustainlab-group/sustainbench/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="woX9uagUQiE" data-number="281">
        <h4>
          <a href="/forum?id=woX9uagUQiE">
              MIND dataset for diet planning and dietary healthcare with machine learning: Dataset creation using combinatorial optimization and controllable generation with domain experts
          </a>


            <a href="/pdf?id=woX9uagUQiE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Changhun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changhun_Lee1">Changhun Lee</a>, <a href="/profile?id=~Soohyeok_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soohyeok_Kim1">Soohyeok Kim</a>, <a href="/profile?id=~Sehwa_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sehwa_Jeong1">Sehwa Jeong</a>, <a href="/profile?id=~Chiehyeon_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chiehyeon_Lim1">Chiehyeon Lim</a>, <a href="/profile?email=jydk6557%40naver.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jydk6557@naver.com">Jayun Kim</a>, <a href="/profile?email=kimhana0419%40naver.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kimhana0419@naver.com">Yeji Kim</a>, <a href="/profile?email=my.jung%40kosin.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="my.jung@kosin.ac.kr">Minyoung Jung</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">21 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">47 Replies</span>


        </div>

          <a href="#woX9uagUQiE-details-635" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="woX9uagUQiE-details-635"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">diet planning, healthcare, machine learning, dataset, MIND, dietkit</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">MIND for diet planning and dietary healthcare with machine learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Diet planning, a basic and regular human activity, is important to all individuals. Children, adults, the healthy, and the infirm all profit from diet planning. Many recent attempts have been made to develop machine learning (ML) applications related to diet planning. However, given the complexity and difficulty of implementing this task, no high-quality diet-level dataset exists at present. Professionals, particularly dietitians and physicians, would benefit greatly from such a dataset and ML application. In this work, we create and publish the Korean Menus–Ingredients–Nutrients–Diets (MIND) dataset for a ML application regarding diet planning and dietary health research. The nature of diet planning entails both explicit (nutrition) and implicit (composition) requirements. Thus, the MIND dataset was created by integrating input from experts who considered implicit data requirements for diet solution with the capabilities of an operations research (OR) model that specifies and applies explicit data requirements for diet solution and a controllable generative machine that automates the high-quality diet generation process. MIND consists of data from 1,500 South Korean daily diets, 3,238 menus, and 3,036 ingredients. MIND considers the daily recommended dietary intake of 14 major nutrients. MIND can be easily downloaded and analyzed using the Python package dietkit accessible via the package installer for Python. MIND is expected to contribute to the use of ML in solving medical, economic, and social problems associated with diet planning. Furthermore, our approach of integrating data from experts with OR and ML models is expected to promote the use of ML in other fields that require the generation of high-quality synthetic professional task data, especially since the use of ML to automate and support professional tasks has become a highly valuable service.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=woX9uagUQiE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pki663/dietkit/tree/master/samples" target="_blank" rel="nofollow noreferrer">https://github.com/pki663/dietkit/tree/master/samples</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/pki663/dietkit" target="_blank" rel="nofollow noreferrer">https://github.com/pki663/dietkit</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="DjzPaX8AT0z" data-number="218">
        <h4>
          <a href="/forum?id=DjzPaX8AT0z">
              A Channel Coding Benchmark for Meta-Learning
          </a>


            <a href="/pdf?id=DjzPaX8AT0z" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rui_Li11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Li11">Rui Li</a>, <a href="/profile?id=~Ondrej_Bohdal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ondrej_Bohdal1">Ondrej Bohdal</a>, <a href="/profile?id=~Rajesh_K_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rajesh_K_Mishra1">Rajesh K Mishra</a>, <a href="/profile?id=~Hyeji_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyeji_Kim1">Hyeji Kim</a>, <a href="/profile?id=~Da_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Li3">Da Li</a>, <a href="/profile?id=~Nicholas_Donald_Lane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Donald_Lane1">Nicholas Donald Lane</a>, <a href="/profile?id=~Timothy_Hospedales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Hospedales1">Timothy Hospedales</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#DjzPaX8AT0z-details-371" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DjzPaX8AT0z-details-371"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">channel coding, meta-learning, benchmark, communication theory, wireless communications, information theory, software defined radio, testbeds</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose channel coding as a novel benchmark to study several aspects of meta-learning, including the impact of task distribution breadth and shift on meta-learner performance, which can be controlled in the coding problem. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Meta-learning provides a popular and effective family of methods for data-efficient learning of new tasks. However, several important issues in meta-learning have proven hard to study thus far. For example, performance degrades in real-world settings where meta-learners must learn from a wide and potentially multi-modal distribution of training tasks; and when distribution shift exists between meta-train and meta-test task distributions. These issues are typically hard to study since the shape of task distributions, and shift between them are not straightforward to measure or control in standard benchmarks. We propose the channel coding problem as a benchmark for meta-learning. Channel coding is an important practical application where task distributions naturally arise, and fast adaptation to new tasks is practically valuable. We use this benchmark to study several aspects of meta-learning, including the impact of task distribution breadth and shift on meta-learner performance, which can be controlled in the coding problem. Going forward, this benchmark provides a tool for the community to study the capabilities and limitations of meta-learning, and to drive research on practically robust and effective meta-learners.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=DjzPaX8AT0z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ruihuili/MetaCC.git" target="_blank" rel="nofollow noreferrer">https://github.com/ruihuili/MetaCC.git</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "> https://github.com/ruihuili/MetaCC/tree/master/dataset</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="0uQIr4XA77f" data-number="165">
        <h4>
          <a href="/forum?id=0uQIr4XA77f">
              STEP: Segmenting and Tracking Every Pixel
          </a>


            <a href="/pdf?id=0uQIr4XA77f" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mark_Weber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Weber1">Mark Weber</a>, <a href="/profile?id=~Jun_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Xie1">Jun Xie</a>, <a href="/profile?id=~Maxwell_D_Collins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxwell_D_Collins1">Maxwell D Collins</a>, <a href="/profile?id=~Yukun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yukun_Zhu1">Yukun Zhu</a>, <a href="/profile?id=~Paul_Voigtlaender1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Voigtlaender1">Paul Voigtlaender</a>, <a href="/profile?id=~Hartwig_Adam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hartwig_Adam1">Hartwig Adam</a>, <a href="/profile?id=~Bradley_Green3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bradley_Green3">Bradley Green</a>, <a href="/profile?id=~Andreas_Geiger3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Geiger3">Andreas Geiger</a>, <a href="/profile?id=~Bastian_Leibe3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bastian_Leibe3">Bastian Leibe</a>, <a href="/profile?id=~Daniel_Cremers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Cremers1">Daniel Cremers</a>, <a href="/profile?id=~Aljosa_Osep2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aljosa_Osep2">Aljosa Osep</a>, <a href="/profile?id=~Laura_Leal-Taix%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laura_Leal-Taixé1">Laura Leal-Taixé</a>, <a href="/profile?id=~Liang-Chieh_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang-Chieh_Chen1">Liang-Chieh Chen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 11 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">14 Replies</span>


        </div>

          <a href="#0uQIr4XA77f-details-629" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0uQIr4XA77f-details-629"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The task of assigning semantic classes and track identities to every pixel in a video is called video panoptic segmentation. Our work is the first that targets this task in a real-world setting requiring dense interpretation in both spatial and temporal domains. As the ground-truth for this task is difficult and expensive to obtain, existing datasets are either constructed synthetically or only sparsely annotated within short video clips. To overcome this, we introduce a new benchmark encompassing two datasets, KITTI-STEP, and MOTChallenge-STEP. The datasets contain long video sequences, providing challenging examples and a test-bed for studying long-term pixel-precise segmentation and tracking under real-world conditions. We further propose a novel evaluation metric Segmentation and Tracking Quality (STQ) that fairly balances semantic and tracking aspects of this task and is more appropriate for evaluating sequences of arbitrary length. Finally, we provide several baselines to evaluate the status of existing methods on this new challenging dataset. We have made our datasets, metric, benchmark servers, and baselines publicly available, and hope this will inspire future research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=0uQIr4XA77f&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://cvlibs.net/datasets/kitti/eval_step.php" target="_blank" rel="nofollow noreferrer">http://cvlibs.net/datasets/kitti/eval_step.php</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">http://cvlibs.net/datasets/kitti/eval_step.php
        https://motchallenge.net/results/STEP-ICCV21/</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value "><a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank" rel="nofollow noreferrer">https://creativecommons.org/licenses/by-nc-sa/3.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rH8yliN6C83" data-number="143">
        <h4>
          <a href="/forum?id=rH8yliN6C83">
              AP-10K: A Benchmark for Animal Pose Estimation in the Wild
          </a>


            <a href="/pdf?id=rH8yliN6C83" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hang_Yu8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Yu8">Hang Yu</a>, <a href="/profile?id=~Yufei_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yufei_Xu1">Yufei Xu</a>, <a href="/profile?id=~Jing_Zhang17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Zhang17">Jing Zhang</a>, <a href="/profile?id=~Wei_Zhao8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Zhao8">Wei Zhao</a>, <a href="/profile?id=~Ziyu_Guan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyu_Guan1">Ziyu Guan</a>, <a href="/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dacheng_Tao1">Dacheng Tao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 29 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">15 Replies</span>


        </div>

          <a href="#rH8yliN6C83-details-484" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rH8yliN6C83-details-484"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Accurate animal pose estimation is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. Previous works only focus on specific animals while ignoring the diversity of animal species, limiting the generalization ability. In this paper, we propose AP-10K, the first large-scale benchmark for general animal pose estimation, to facilitate the research in animal pose estimation. AP-10K consists of 10,015 images collected and filtered from 23 animal families and 54 species following the taxonomic rank and high-quality keypoint annotations labeled and checked manually. Based on AP-10K, we benchmark representative pose estimation models on the following three tracks: (1) supervised learning for animal pose estimation, (2) cross-domain transfer learning from human pose estimation to animal pose estimation, and (3) intra- and inter-family domain generalization for unseen animals. The experimental results provide sound empirical evidence on the superiority of learning from diverse animals species in terms of both accuracy and generalization ability. It opens new directions for facilitating future research in animal pose estimation. AP-10k is publicly available at https://github.com/AlexTheBad/AP10K.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rH8yliN6C83&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/AlexTheBad/AP10K" target="_blank" rel="nofollow noreferrer">https://github.com/AlexTheBad/AP10K</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LjjqegBNtPi" data-number="116">
        <h4>
          <a href="/forum?id=LjjqegBNtPi">
              Seasons in Drift: A Long Term Thermal Imaging Dataset for Studying Concept Drift
          </a>


            <a href="/pdf?id=LjjqegBNtPi" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ivan_Nikolov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Nikolov1">Ivan Nikolov</a>, <a href="/profile?id=~Mark_Philip_Philipsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Philip_Philipsen1">Mark Philip Philipsen</a>, <a href="/profile?id=~Jinsong_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinsong_Liu1">Jinsong Liu</a>, <a href="/profile?id=~Jacob_Velling_Dueholm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Velling_Dueholm1">Jacob Velling Dueholm</a>, <a href="/profile?id=~Anders_Skaarup_Johansen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anders_Skaarup_Johansen1">Anders Skaarup Johansen</a>, <a href="/profile?id=~Kamal_Nasrollahi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kamal_Nasrollahi1">Kamal Nasrollahi</a>, <a href="/profile?id=~Thomas_B._Moeslund1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_B._Moeslund1">Thomas B. Moeslund</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 29 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#LjjqegBNtPi-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LjjqegBNtPi-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">long-term dataset, large dataset, thermal imaging, concept drift, anomaly detection, object detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A long-term thermal dataset used for studying the changes of performance of surveillance models based on concept drift from environmental effects like weather, day/night cycle, seasonality, scene activity, etc.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The time dimension of datasets and the long-term performance of machine learning models have received little attention. With extended deployments in the wild, models are bound to encounter novel scenarios and concept drift that cannot be accounted for during development and training. In order for long-term patterns and cycles to appear in datasets, the datasets must cover long periods of time. Since this is rarely the case, it is difficult to explore how computer vision algorithms cope with changes in data distribution occurring across long-term cycles such as seasons. Video surveillance is an application area clearly affected by concept drift. For this reason, we publish the Long-term Thermal Drift (LTD) dataset. LTD consists of thermal surveillance imaging from a single location across 8 months. Along with thermal images we provide relevant metadata such as weather, the day/night cycle, and scene activity. In this paper, we use the metadata for in-depth analysis of the causal and correlational relationships between environmental variables and the performance of selected computer vision algorithms used for anomaly and object detection. Long-term performance is shown to be most correlated with temperature, humidity, the day/night cycle, and scene activity level. This suggests that the coverage of these variables should be prioritised when building datasets for similar applications. As a baseline, we propose to mitigate the impact of concept drift by first detecting points in time where drift occurs. At this point, we collect additional data that is used to retraining the models. This improves later performance by an average of 25% across all tested algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LjjqegBNtPi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.kaggle.com/ivannikolov/longterm-thermal-drift-dataset" target="_blank" rel="nofollow noreferrer">https://www.kaggle.com/ivannikolov/longterm-thermal-drift-dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.kaggle.com/ivannikolov/longterm-thermal-drift-dataset" target="_blank" rel="nofollow noreferrer">https://www.kaggle.com/ivannikolov/longterm-thermal-drift-dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution 4.0 International (CC BY 4.0)
        Please cite the paper connected to this dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="JtjzUXPEaCu" data-number="91">
        <h4>
          <a href="/forum?id=JtjzUXPEaCu">
              CropHarvest: A global dataset for crop-type classification
          </a>


            <a href="/pdf?id=JtjzUXPEaCu" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gabriel_Tseng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Tseng1">Gabriel Tseng</a>, <a href="/profile?id=~Ivan_Zvonkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Zvonkov1">Ivan Zvonkov</a>, <a href="/profile?id=~Catherine_Lilian_Nakalembe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_Lilian_Nakalembe1">Catherine Lilian Nakalembe</a>, <a href="/profile?id=~Hannah_Kerner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hannah_Kerner1">Hannah Kerner</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#JtjzUXPEaCu-details-42" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JtjzUXPEaCu-details-42"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">agriculture, remote sensing, land cover mapping, machine learning ready, satellite data, crop type classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">CropHarvest: An ML-ready global dataset for crop-type classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Remote sensing datasets pose a number of interesting challenges to machine learning researchers and practitioners, from domain shift (spatially, semantically and temporally) to highly imbalanced labels. In addition, the outputs of models trained on remote sensing datasets can contribute to positive societal impacts, for example in food security and climate change. However, there are many barriers that limit the accessibility of satellite data to the machine learning community, including a lack of large labeled datasets as well as an understanding of the range of satellite products available, how these products should be processed, and how to manage multi-dimensional geospatial data. To lower these barriers and facilitate the use of satellite datasets by the machine learning community, we present CropHarvest---a satellite dataset of more than 90,000 geographically-diverse samples with agricultural labels. The data and accompanying python package are available at https://github.com/nasaharvest/cropharvest.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=JtjzUXPEaCu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/nasaharvest/cropharvest" target="_blank" rel="nofollow noreferrer">https://github.com/nasaharvest/cropharvest</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="J4Nl2qRMDrR" data-number="71">
        <h4>
          <a href="/forum?id=J4Nl2qRMDrR">
              ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation
          </a>


            <a href="/pdf?id=J4Nl2qRMDrR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Laurynas_Karazija1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurynas_Karazija1">Laurynas Karazija</a>, <a href="/profile?id=~Iro_Laina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iro_Laina1">Iro Laina</a>, <a href="/profile?id=~Christian_Rupprecht1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Rupprecht1">Christian Rupprecht</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 12 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#J4Nl2qRMDrR-details-799" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="J4Nl2qRMDrR-details-799"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Textures, Benchmark, Multi-Object Learning, Unsupervised Segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">ClevrTex is a synthetic dataset and benchmark with complex visual appearance for multi-object learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a recent surge in methods that aim to decompose and segment scenes into multiple objects in an unsupervised manner, i.e., unsupervised multi-object segmentation. Performing such a task is a long-standing goal of computer vision, offering to unlock object-level reasoning without requiring dense annotations to train segmentation models. Despite significant progress, current models are developed and trained on visually simple scenes depicting mono-colored objects on plain backgrounds. The natural world, however, is visually complex with confounding aspects such as diverse textures and complicated lighting effects. In this study, we present a new benchmark called ClevrTex, designed as the next challenge to compare, evaluate and analyze algorithms. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques. ClevrTex has 50k examples depicting 3-10 objects arranged on a background, created using a catalog of 60 materials, and a further test set featuring 10k images created using 25 different materials. We benchmark a large set of recent unsupervised multi-object segmentation models on ClevrTex and find all state-of-the-art approaches fail to learn good representations in the textured setting, despite impressive performance on simpler data. We also create variants of the ClevrTex dataset, controlling for different aspects of scene complexity, and probe current approaches for individual shortcomings. Dataset and code are available at https://www.robots.ox.ac.uk/~vgg/research/clevrtex.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=J4Nl2qRMDrR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.robots.ox.ac.uk/~vgg/research/clevrtex" target="_blank" rel="nofollow noreferrer">https://www.robots.ox.ac.uk/~vgg/research/clevrtex</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.robots.ox.ac.uk/~vgg/data/clevrtex/" target="_blank" rel="nofollow noreferrer">https://www.robots.ox.ac.uk/~vgg/data/clevrtex/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released under CC-BY 4.0 license.
        The code is released under BSD 3-Clause license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zNQBIBKJRkd" data-number="215">
        <h4>
          <a href="/forum?id=zNQBIBKJRkd">
              Reduced, Reused and Recycled: The Life of a  Dataset in Machine Learning Research
          </a>


            <a href="/pdf?id=zNQBIBKJRkd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Bernard_Koch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernard_Koch1">Bernard Koch</a>, <a href="/profile?id=~Emily_Denton2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Denton2">Emily Denton</a>, <a href="/profile?id=~Alex_Hanna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Hanna1">Alex Hanna</a>, <a href="/profile?id=~Jacob_Gates_Foster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Gates_Foster1">Jacob Gates Foster</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Track Datasets and Benchmarks Round2 Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#zNQBIBKJRkd-details-326" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zNQBIBKJRkd-details-326"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Benchmarks, AI Ethics, Algorithmic Fairness, Science of Science</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Field-level analysis of dynamics of dataset (re)use revealing increasing concentration on fewer and fewer datasets introduced by a few elite institutions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals.  Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We find increasing concentration on fewer and fewer datasets within task communities, significant adoption of datasets from other tasks, and concentration across the field on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientific evaluation, AI ethics, and equity/access within the field.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zNQBIBKJRkd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/kochbj/Reduced_Reused_Recycled" target="_blank" rel="nofollow noreferrer">https://github.com/kochbj/Reduced_Reused_Recycled</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mPducS1MsEK" data-number="201">
        <h4>
          <a href="/forum?id=mPducS1MsEK">
              Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning
          </a>


            <a href="/pdf?id=mPducS1MsEK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Thomas_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Liao1">Thomas Liao</a>, <a href="/profile?id=~Rohan_Taori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rohan_Taori1">Rohan Taori</a>, <a href="/profile?id=~Inioluwa_Deborah_Raji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Inioluwa_Deborah_Raji1">Inioluwa Deborah Raji</a>, <a href="/profile?id=~Ludwig_Schmidt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ludwig_Schmidt1">Ludwig Schmidt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#mPducS1MsEK-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mPducS1MsEK-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">evaluation, progress, benchmarks, meta-survey, meta-review, validity, transfer</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a meta-review of evaluation failures across subfields of machine learning, finding surprisingly consistent failure modes.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Many subfields of machine learning share a common stumbling block: evaluation.  Advances in machine learning often evaporate under closer scrutiny or turn out to be less widely applicable than originally hoped.  We conduct a meta-review of 107 survey papers from natural language processing, recommender systems, computer vision, reinforcement learning, computational biology, graph learning, and more, organizing the wide range of surprisingly consistent critique into a concrete taxonomy of observed failure modes. Inspired by measurement and evaluation theory, we divide failure modes into two categories: internal and external validity. Internal validity issues pertain to evaluation on a learning problem in isolation, such as improper comparisons to baselines or overfitting from test set re-use. External validity relies on relationships between different learning problems, for instance, whether progress on a learning problem translates to progress on seemingly related tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mPducS1MsEK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/tholiao/are_we_learning_yet" target="_blank" rel="nofollow noreferrer">https://github.com/tholiao/are_we_learning_yet</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Q0zOIaec8HF" data-number="15">
        <h4>
          <a href="/forum?id=Q0zOIaec8HF">
              Benchmarking Multimodal AutoML for Tabular Data with Text Fields
          </a>


            <a href="/pdf?id=Q0zOIaec8HF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xingjian_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingjian_Shi1">Xingjian Shi</a>, <a href="/profile?id=~Jonas_Mueller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonas_Mueller1">Jonas Mueller</a>, <a href="/profile?id=~Nick_Erickson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nick_Erickson1">Nick Erickson</a>, <a href="/profile?id=~Mu_Li4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mu_Li4">Mu Li</a>, <a href="/profile?id=~Alex_Smola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Smola1">Alex Smola</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">12 Aug 2021 (modified: 30 Dec 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#Q0zOIaec8HF-details-694" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q0zOIaec8HF-details-694"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multimodal AutoML, Text Data, Tabular Data, Natural Language Processing, Supervised Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a new benchmark for classification/regression with data tables that jointly contain numeric, categorical, and text features, as well as a systematic evaluation of various text/tabular modeling strategies over this benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Q0zOIaec8HF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sxjscience/automl_multimodal_benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/sxjscience/automl_multimodal_benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/sxjscience/automl_multimodal_benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/sxjscience/automl_multimodal_benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jpwGODt2Av" data-number="8">
        <h4>
          <a href="/forum?id=jpwGODt2Av">
              Whole Brain Vessel Graphs: A Dataset and Benchmark for Graph Learning and Neuroscience
          </a>


            <a href="/pdf?id=jpwGODt2Av" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Johannes_C._Paetzold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_C._Paetzold1">Johannes C. Paetzold</a>, <a href="/profile?id=~Julian_McGinnis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julian_McGinnis1">Julian McGinnis</a>, <a href="/profile?id=~Suprosanna_Shit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suprosanna_Shit1">Suprosanna Shit</a>, <a href="/profile?id=~Ivan_Ezhov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Ezhov1">Ivan Ezhov</a>, <a href="/profile?id=~Paul_B%C3%BCschl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Büschl1">Paul Büschl</a>, <a href="/profile?email=chinmayprabhakar55%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chinmayprabhakar55@gmail.com">Chinmay Prabhakar</a>, <a href="/profile?id=~Anjany_Sekuboyina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anjany_Sekuboyina1">Anjany Sekuboyina</a>, <a href="/profile?id=~Mihail_Todorov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihail_Todorov1">Mihail Todorov</a>, <a href="/profile?id=~Georgios_Kaissis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgios_Kaissis1">Georgios Kaissis</a>, <a href="/profile?email=erturk%40helmholtz-muenchen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="erturk@helmholtz-muenchen.de">Ali Ertürk</a>, <a href="/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>, <a href="/profile?id=~bjoern_menze1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~bjoern_menze1">bjoern menze</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">09 Aug 2021 (modified: 15 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#jpwGODt2Av-details-111" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jpwGODt2Av-details-111"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Learning, Brain Vessels, Graph Extraction, Node Classification, Link Prediction, GNN, Neuroscience</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We are presenting a large dataset of whole-brain vessel graphs to advance graph learning research in the field of neuroscience.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Biological neural networks define the brain function and intelligence of humans and other mammals, and form ultra-large, spatial, structured graphs. Their neuronal organization is closely interconnected with the spatial organization of the brain's microvasculature, which supplies oxygen to the neurons and builds a complementary spatial graph. This vasculature (or the vessel structure) plays an important role in neuroscience; for example, the organization of (and changes to) vessel structure can represent early signs of various pathologies, e.g. Alzheimer's disease or stroke. Recently, advances in tissue clearing have enabled whole brain imaging and segmentation of the entirety of the mouse brain's vasculature.

        Building on these advances in imaging, we are presenting an extendable dataset of whole-brain vessel graphs based on specific imaging protocols. Specifically, we extract vascular graphs using a refined graph extraction scheme leveraging the volume rendering engine Voreen and provide them in an accessible and adaptable form through the OGB and PyTorch Geometric dataloaders. Moreover, we benchmark numerous state-of-the-art graph learning algorithms on the biologically relevant tasks of vessel prediction and vessel classification using the introduced vessel graph dataset.

        Our work paves a path towards advancing graph learning research into the field of neuroscience. Complementarily, the presented dataset raises challenging graph learning research questions for the machine learning community, in terms of incorporating biological priors into learning algorithms, or in scaling these algorithms to handle sparse,spatial graphs with millions of nodes and edges.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jpwGODt2Av&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/jocpae/VesselGraph" target="_blank" rel="nofollow noreferrer">https://github.com/jocpae/VesselGraph</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/jocpae/VesselGraph" target="_blank" rel="nofollow noreferrer">https://github.com/jocpae/VesselGraph</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="j6NxpQbREA1" data-number="196">
        <h4>
          <a href="/forum?id=j6NxpQbREA1">
              AI and the Everything in the Whole Wide World Benchmark
          </a>


            <a href="/pdf?id=j6NxpQbREA1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Inioluwa_Deborah_Raji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Inioluwa_Deborah_Raji1">Inioluwa Deborah Raji</a>, <a href="/profile?id=~Emily_Denton2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Denton2">Emily Denton</a>, <a href="/profile?id=~Emily_M._Bender1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_M._Bender1">Emily M. Bender</a>, <a href="/profile?id=~Alex_Hanna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Hanna1">Alex Hanna</a>, <a href="/profile?email=paullada%40uw.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="paullada@uw.edu">Amandalynne Paullada</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#j6NxpQbREA1-details-911" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="j6NxpQbREA1-details-911"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">evaluation, dataset, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The benchmarking paradigm in machine learning is incompatible with claims to performance on underspecified general tasks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There is a tendency across different subfields in AI to see value in a small collection of influential benchmarks, which we term 'general' benchmarks. These benchmarks operate as stand-ins or abstractions for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore how such benchmarks are designed, constructed and used in order to reveal key limitations of their framing as the functionally 'general' broad measures of progress they are set up to be.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=j6NxpQbREA1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="oJ0oHQtAld" data-number="246">
        <h4>
          <a href="/forum?id=oJ0oHQtAld">
              A Toolbox for Construction and Analysis of Speech Datasets
          </a>


            <a href="/pdf?id=oJ0oHQtAld" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Evelina_Bakhturina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evelina_Bakhturina1">Evelina Bakhturina</a>, <a href="/profile?id=~Vitaly_Lavrukhin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vitaly_Lavrukhin1">Vitaly Lavrukhin</a>, <a href="/profile?id=~Boris_Ginsburg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Ginsburg1">Boris Ginsburg</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#oJ0oHQtAld-details-901" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oJ0oHQtAld-details-901"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">speech datasets</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An introduction to a Toolbox for Construction and Analysis of Speech Datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automatic Speech Recognition and Text-to-Speech systems are primarily trained in a supervised fashion and require high-quality, accurately labeled speech datasets.
        In this work, we examine common problems with speech data and introduce a toolbox for the construction and interactive error analysis of speech datasets. The construction tool is based on K{\"u}rzinger et al. work, and, to the best of our knowledge, the dataset exploration tool is the world's first open-source tool of this kind. We demonstrate how to apply these tools to create a Russian speech dataset and analyze existing speech datasets (Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as a part of the NeMo framework.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=oJ0oHQtAld&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">n/a</span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="e82_BlJL43M" data-number="137">
        <h4>
          <a href="/forum?id=e82_BlJL43M">
              RB2: Robotic Manipulation Benchmarking with a Twist
          </a>


            <a href="/pdf?id=e82_BlJL43M" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sudeep_Dasari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sudeep_Dasari2">Sudeep Dasari</a>, <a href="/profile?id=~Jianren_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianren_Wang2">Jianren Wang</a>, <a href="/profile?id=~Joyce_Hong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joyce_Hong1">Joyce Hong</a>, <a href="/profile?id=~Shikhar_Bahl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shikhar_Bahl1">Shikhar Bahl</a>, <a href="/profile?id=~Yixin_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yixin_Lin1">Yixin Lin</a>, <a href="/profile?id=~Austin_S_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Austin_S_Wang1">Austin S Wang</a>, <a href="/profile?id=~Abitha_Thankaraj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abitha_Thankaraj1">Abitha Thankaraj</a>, <a href="/profile?id=~Karanbir_Singh_Chahal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karanbir_Singh_Chahal1">Karanbir Singh Chahal</a>, <a href="/profile?id=~Berk_Calli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Berk_Calli1">Berk Calli</a>, <a href="/profile?id=~Saurabh_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saurabh_Gupta1">Saurabh Gupta</a>, <a href="/profile?id=~David_Held1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Held1">David Held</a>, <a href="/profile?id=~Lerrel_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lerrel_Pinto1">Lerrel Pinto</a>, <a href="/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepak_Pathak1">Deepak Pathak</a>, <a href="/profile?id=~Vikash_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikash_Kumar2">Vikash Kumar</a>, <a href="/profile?id=~Abhinav_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Gupta1">Abhinav Gupta</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">17 Replies</span>


        </div>

          <a href="#e82_BlJL43M-details-806" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="e82_BlJL43M-details-806"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">learning, benchmarks, robotics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a hardware based robotics benchmark for learning based methods and validate it across various lab setups.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Benchmarks offer a scientific way to compare algorithms using objective performance metrics.
        Good benchmarks have two features: (a) they should be widely useful for many research groups; (b) and they should produce reproducible findings. In robotic manipulation research, there is a trade-off between reproducibility and broad accessibility. If the benchmark is kept restrictive (fixed hardware, objects), the numbers are reproducible but the setup becomes less general. On the other hand, a benchmark could be a loose set of protocols (e.g. object set) but the underlying variation in setups make the results non-reproducible. In this paper, we re-imagine benchmarking for robotic manipulation as state-of-the-art algorithmic implementations, alongside the usual set of tasks and experimental protocols. The added baseline implementations will provide a way to easily recreate SOTA numbers in a new local robotic setup, thus providing credible relative rankings between existing approaches and new work. However, these "local rankings" could vary between different setups. To resolve this issue, we build a mechanism for pooling experimental data between labs, and thus we establish a single global ranking for existing (and proposed) SOTA algorithms. Our benchmark, called Ranking-Based Robotics Benchmark (RB2), is evaluated on tasks that are inspired from clinically validated Southampton Hand Assessment Procedures. Our benchmark was run across two different labs and reveals several surprising findings. For example, extremely simple baselines like open-loop behavior cloning, outperform more complicated models (e.g. closed loop, RNN, Offline-RL, etc.) that are preferred by the field. We hope our fellow researchers will use RB2 to improve their research's quality and rigor.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://rb2.info/" target="_blank" rel="nofollow noreferrer">https://rb2.info/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="XyDozX3_L4l" data-number="121">
        <h4>
          <a href="/forum?id=XyDozX3_L4l">
              NATURE: Natural Auxiliary Text Utterances for Realistic Spoken Language Evaluation
          </a>


            <a href="/pdf?id=XyDozX3_L4l" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~David_Alfonso-Hermelo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Alfonso-Hermelo1">David Alfonso-Hermelo</a>, <a href="/profile?id=~Ahmad_Rashid1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmad_Rashid1">Ahmad Rashid</a>, <a href="/profile?id=~Abbas_Ghaddar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abbas_Ghaddar1">Abbas Ghaddar</a>, <a href="/profile?id=~Philippe_Langlais2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Langlais2">Philippe Langlais</a>, <a href="/profile?id=~Mehdi_Rezagholizadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mehdi_Rezagholizadeh1">Mehdi Rezagholizadeh</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 14 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#XyDozX3_L4l-details-571" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XyDozX3_L4l-details-571"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">slot filling, intent detection, voice assistant, dialog system, natural language understanding, virtual assistant, intent classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a set of simple speech-oriented operators and show that simple pattern alterations deteriorate significantly the performance of state-of the-art models for Voice Assistants.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Slot-filling and intent detection are the backbone of conversational agents such as voice assistants, and are active areas of research. Even though state-of-the-art techniques on publicly available benchmarks show impressive performance, their ability to generalize to realistic scenarios is yet to be demonstrated. In this work, we present NATURE, a set of simple spoken-language-oriented transformations, applied to the evaluation set of datasets, to introduce human spoken language variations while preserving the semantics of an utterance. We apply NATURE to common slot-filling and intent detection benchmarks and demonstrate that simple perturbations from the standard evaluation set by NATURE can deteriorate model performance significantly. Through our experiments we demonstrate that when NATURE operators are applied to evaluation set of popular benchmarks the model accuracy can drop by up to 40%.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=XyDozX3_L4l&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/rali-udem/sf_id_benchmarks" target="_blank" rel="nofollow noreferrer">https://github.com/rali-udem/sf_id_benchmarks</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/rali-udem/sf_id_benchmarks" target="_blank" rel="nofollow noreferrer">https://github.com/rali-udem/sf_id_benchmarks</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FgYTwJbjbf" data-number="78">
        <h4>
          <a href="/forum?id=FgYTwJbjbf">
              FFA-IR: Towards an Explainable and Reliable Medical Report Generation Benchmark&nbsp;
          </a>


            <a href="/pdf?id=FgYTwJbjbf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mingjie_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingjie_Li2">Mingjie Li</a>, <a href="/profile?id=~Wenjia_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjia_Cai1">Wenjia Cai</a>, <a href="/profile?id=~Rui_Liu12" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Liu12">Rui Liu</a>, <a href="/profile?id=~Yuetian_Weng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuetian_Weng1">Yuetian Weng</a>, <a href="/profile?id=~Xiaoyun_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyun_Zhao1">Xiaoyun Zhao</a>, <a href="/profile?id=~Cong_Wang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cong_Wang9">Cong Wang</a>, <a href="/profile?id=~Xin_Chen20" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Chen20">Xin Chen</a>, <a href="/profile?id=~Zhong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhong_Liu2">Zhong Liu</a>, <a href="/profile?id=~Caineng_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caineng_Pan1">Caineng Pan</a>, <a href="/profile?id=~Mengke_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengke_Li2">Mengke Li</a>, <a href="/profile?id=~yingfeng_zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~yingfeng_zheng1">yingfeng zheng</a>, <a href="/profile?id=~Yizhi_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhi_Liu2">Yizhi Liu</a>, <a href="/profile?id=~Flora_D._Salim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Flora_D._Salim1">Flora D. Salim</a>, <a href="/profile?id=~Karin_Verspoor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karin_Verspoor1">Karin Verspoor</a>, <a href="/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>, <a href="/profile?id=~Xiaojun_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojun_Chang1">Xiaojun Chang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">18 Aug 2021 (modified: 30 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#FgYTwJbjbf-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FgYTwJbjbf-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">medical report generation, vision and language, fundus fluorescein angiography, explaibable and reliable evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Towards explainable and reliable MRG benchmark based on fundus fluorescein angiography images and reports, which provides explainable annotations and reliable evaluation tools to facilitate the developmet of medical report generation methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The automatic generation of long and coherent medical reports given medical images (e.g. Chest X-ray and Fundus Fluorescein Angiography (FFA)) has great potential to support clinical practice. Researchers have explored advanced methods from computer vision and natural language processing to incorporate medical domain knowledge for the generation of readable medical reports. However, existing medical report generation (MRG) benchmarks lack both explainable annotations and reliable evaluation tools, hindering the current research advances from two aspects: firstly, existing methods can only predict reports without accurate explanation, undermining the trustworthiness of the diagnostic methods; secondly, the comparison among the predicted reports from different MRG methods is unreliable using the evaluation metrics of natural-language generation (NLG). To address these issues, in this paper, we propose an explainable and reliable MRG benchmark based on FFA Images and Reports (FFA-IR). Specifically, FFA-IR is large, with 10,790 reports along with 1,048,584 FFA images from clinical practice; it includes explainable annotations, based on a schema of 46 categories of lesions; and it is bilingual, providing both English and Chinese reports for each case. Besides using the widely used NLG metrics, we propose a set of nine human evaluation criteria to evaluate the generated reports. We envision FFA-IR as a testbed for explainable and reliable medical report generation. We also hope that it can broadly accelerate medical imaging research and facilitate interaction between the fields of medical imaging, computer vision, and natural language processing.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Code: https://github.com/mlii0117/FFA-IR</span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">Our dataset is hosted on the Physionet with the following link: https://doi.org/10.13026/ccbh-z832</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="enYjtbjYJrf" data-number="127">
        <h4>
          <a href="/forum?id=enYjtbjYJrf">
              Chaos as an interpretable benchmark for forecasting and data-driven modelling
          </a>


            <a href="/pdf?id=enYjtbjYJrf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~William_Gilpin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Gilpin1">William Gilpin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 13 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">16 Replies</span>


        </div>

          <a href="#enYjtbjYJrf-details-554" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="enYjtbjYJrf-details-554"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">time series, data-driven modelling, chaos, dynamical systems, forecasting</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a curated collection of chaotic dynamical systems for benchmarking and interpreting forecasting and data-driven modelling, which can be re-integrated to generate new datasets for diverse applications.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems, each paired with corresponding precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across our dataset. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present.  We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=enYjtbjYJrf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/williamgilpin/dysts" target="_blank" rel="nofollow noreferrer">https://github.com/williamgilpin/dysts</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/williamgilpin/dysts" target="_blank" rel="nofollow noreferrer">https://github.com/williamgilpin/dysts</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value "> Apache 2.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="R7vr14ffhF9" data-number="31">
        <h4>
          <a href="/forum?id=R7vr14ffhF9">
              Synthetic Benchmarks for Scientific Research in Explainable Machine Learning
          </a>


            <a href="/pdf?id=R7vr14ffhF9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yang_Liu40" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu40">Yang Liu</a>, <a href="/profile?id=~Sujay_Khandagale1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sujay_Khandagale1">Sujay Khandagale</a>, <a href="/profile?id=~Colin_White1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colin_White1">Colin White</a>, <a href="/profile?id=~Willie_Neiswanger2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Willie_Neiswanger2">Willie Neiswanger</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">15 Aug 2021 (modified: 04 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#R7vr14ffhF9-details-250" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="R7vr14ffhF9-details-250"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">explainability, synthetic data, feature attribution, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release a set of synthetic benchmarks for explainable AI and use it to benchmark existing approaches across a variety of settings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on real-world datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=R7vr14ffhF9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/abacusai/xai-bench" target="_blank" rel="nofollow noreferrer">https://github.com/abacusai/xai-bench</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8Y50dBbmGU" data-number="277">
        <h4>
          <a href="/forum?id=8Y50dBbmGU">
              CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example
          </a>


            <a href="/pdf?id=8Y50dBbmGU" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sheshera_Mysore1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheshera_Mysore1">Sheshera Mysore</a>, <a href="/profile?id=~Tim_O%26%23x27%3BGorman2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_O'Gorman2">Tim O'Gorman</a>, <a href="/profile?id=~Andrew_McCallum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_McCallum1">Andrew McCallum</a>, <a href="/profile?id=~Hamed_Zamani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hamed_Zamani1">Hamed Zamani</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 06 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">12 Replies</span>


        </div>

          <a href="#8Y50dBbmGU-details-566" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8Y50dBbmGU-details-566"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">faceted search, exploratory search, query by example, scientific information retrieval, test collection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Dataset to evaluate a fine-grained scientific document similarity task.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Query by Example is a well-known information retrieval task in which a document is chosen by the user as the search query and the goal is to retrieve relevant documents from a large collection. However, a document often covers multiple aspects of a topic. To address this scenario we introduce the task of faceted Query by Example in which users can also specify a finer grained aspect in addition to the input query document. We focus on the application of this task in scientific literature search. We envision models which are able to retrieve scientific papers analogous to a query scientific paper along specifically chosen rhetorical structure elements as one solution to this problem. In this work, the rhetorical structure elements, which we refer to as facets,  indicate objectives, methods, or results of a scientific paper. We introduce and describe an expert annotated test collection to evaluate models trained to perform this task. Our test collection consists of a diverse set of 50 query documents in English, drawn from computational linguistics and machine learning venues. We carefully follow the annotation guideline used by TREC for depth-k pooling (k = 100 or 250) and the resulting data collection consists of graded relevance scores with high annotation agreement. State of the art models evaluated on our dataset show a significant gap to be closed in further work. Our dataset may be accessed here: https://github.com/iesl/CSFCube</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=8Y50dBbmGU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/iesl/CSFCube" target="_blank" rel="nofollow noreferrer">https://github.com/iesl/CSFCube</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="udVUN__gFO" data-number="210">
        <h4>
          <a href="/forum?id=udVUN__gFO">
              DEBAGREEMENT: A comment-reply dataset for (dis)agreement detection in online debates
          </a>


            <a href="/pdf?id=udVUN__gFO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~John_Pougu%C3%A9-Biyong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Pougué-Biyong1">John Pougué-Biyong</a>, <a href="/profile?email=valentina.semenova%40maths.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="valentina.semenova@maths.ox.ac.uk">Valentina Semenova</a>, <a href="/profile?id=~Alexandre_Matton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_Matton1">Alexandre Matton</a>, <a href="/profile?email=rachel%40scale.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rachel@scale.com">Rachel Han</a>, <a href="/profile?id=~Aerin_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aerin_Kim1">Aerin Kim</a>, <a href="/profile?id=~Renaud_Lambiotte1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renaud_Lambiotte1">Renaud Lambiotte</a>, <a href="/profile?email=doyne.farmer%40inet.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="doyne.farmer@inet.ox.ac.uk">Doyne Farmer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 09 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#udVUN__gFO-details-482" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="udVUN__gFO-details-482"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">stance detection, (dis)agreement detection, pre-trained language models, graph representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents a comment-reply dataset collected from Reddit which unveils opportunities to combine pre-trained language models and graph representation learning methods for (dis)agreement detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce DEBAGREEMENT, a dataset of 42,894 comment-reply pairs from the popular discussion website Reddit, annotated with agree, neutral or disagree labels. We collect data from five forums on Reddit: r/BlackLivesMatter, r/Brexit, r/climate, r/democrats, r/Republican. For each forum, we select comment pairs such that they form altogether a user interaction graph. DEBAGREEMENT presents a challenge for Natural Language Processing (NLP) systems, as it contains slang, sarcasm and topic-specific jokes, often present in online exchanges. We evaluate the performance of state-of-the-art language models on a (dis)agreement detection task, and investigate the use of contextual information available (graph, authorship, and temporal information). Since recent research has shown that context, such as social context or knowledge graph information, enables language models to better perform on downstream NLP tasks, DEBAGREEMENT provides novel opportunities for combining graph-based and text-based machine learning techniques to detect (dis)agreements online.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=udVUN__gFO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://scale.com/open-datasets/oxford" target="_blank" rel="nofollow noreferrer">https://scale.com/open-datasets/oxford</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://scale.com/open-datasets/oxford" target="_blank" rel="nofollow noreferrer">https://scale.com/open-datasets/oxford</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0 International Public License (“CC BY 4.0”)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="pX4x8f6Km5T" data-number="206">
        <h4>
          <a href="/forum?id=pX4x8f6Km5T">
              Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs
          </a>


            <a href="/pdf?id=pX4x8f6Km5T" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zihao_Wang11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zihao_Wang11">Zihao Wang</a>, <a href="/profile?id=~Hang_Yin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Yin3">Hang Yin</a>, <a href="/profile?id=~Yangqiu_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangqiu_Song1">Yangqiu Song</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 04 Jan 2022)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">11 Replies</span>


        </div>

          <a href="#pX4x8f6Km5T-details-581" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pX4x8f6Km5T-details-581"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graph, complex query, logical query</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Complex Query Answering (CQA) is an important reasoning task on knowledge graphs. Current CQA learning models have been shown to be able to generalize from atomic operators to more complex formulas, which can be regarded as the combinatorial generalizability. In this paper, we present EFO-1-QA, a new dataset to benchmark the combinatorial generalizability of CQA models by including 301 different queries types, which is 20 times larger than existing datasets. Besides, our benchmark, for the first time, provide a benchmark to evaluate and analyze the impact of different operators and normal forms by using (a) 7 choices of the operator systems and (b) 9 forms of complex queries. Specifically, we provide the detailed study of the combinatorial generalizability of two commonly used operators, i.e., projection and intersection, and justify the impact of the forms of queries given the canonical choice of operators. Our code and data can provide an effective pipeline to benchmark CQA models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=pX4x8f6Km5T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/HKUST-KnowComp/EFO-1-QA-benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/HKUST-KnowComp/EFO-1-QA-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Contribution Process Agreement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://hkustconnect-my.sharepoint.com/:f:/g/personal/zwanggc_connect_ust_hk/EpaFL1PUoOFBuCc7hclIM30B8c21e-Tnv1gL11jw_z_SQQ?e=m8RJb5" target="_blank" rel="nofollow noreferrer">https://hkustconnect-my.sharepoint.com/:f:/g/personal/zwanggc_connect_ust_hk/EpaFL1PUoOFBuCc7hclIM30B8c21e-Tnv1gL11jw_z_SQQ?e=m8RJb5</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="VhIIQBm00VI" data-number="200">
        <h4>
          <a href="/forum?id=VhIIQBm00VI">
              Few-Shot Learning Evaluation in Natural Language Understanding
          </a>


            <a href="/pdf?id=VhIIQBm00VI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Subhabrata_Mukherjee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subhabrata_Mukherjee2">Subhabrata Mukherjee</a>, <a href="/profile?id=~Xiaodong_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodong_Liu1">Xiaodong Liu</a>, <a href="/profile?id=~Guoqing_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guoqing_Zheng1">Guoqing Zheng</a>, <a href="/profile?id=~Saghar_Hosseini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saghar_Hosseini1">Saghar Hosseini</a>, <a href="/profile?id=~Hao_Cheng4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Cheng4">Hao Cheng</a>, <a href="/profile?id=~Greg_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Greg_Yang1">Greg Yang</a>, <a href="/profile?id=~Christopher_Meek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Meek1">Christopher Meek</a>, <a href="/profile?id=~Ahmed_Hassan_Awadallah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Hassan_Awadallah1">Ahmed Hassan Awadallah</a>, <a href="/profile?id=~Jianfeng_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Gao1">Jianfeng Gao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 05 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#VhIIQBm00VI-details-725" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VhIIQBm00VI-details-725"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">few-shot learning, NLU, evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A framework for evaluating few-shot learning in natural language understanding </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most recent progress in natural language understanding (NLU) has been driven, in part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU models have now matched or exceeded "human-level" performance on many tasks in these benchmarks. Most of these benchmarks, however, give models access to relatively large amounts of labeled data for training. As such, the models are provided far more data than required by humans to achieve strong performance. That has motivated a line of work that focuses on improving few-shot learning performance of NLU models. However, there is a lack of standardized evaluation benchmarks for few-shot NLU resulting in different experimental settings in different papers.
        To help accelerate this line of work, we introduce CLUES, a benchmark for evaluating the few-shot learning capabilities of NLU models. We demonstrate that while recent models reach human performance when they have access to large amounts of labeled data, there is a huge gap in performance in the few-shot setting for most tasks. We also demonstrate differences between alternative model families and adaptation techniques in the few shot setting. Finally, we discuss several principles and choices in designing the experimental settings for evaluating the true few-shot learning performance and suggest a unified standardized approach to few-shot learning evaluation. We aim to encourage research on NLU models that can generalize to new tasks with a small number of examples. Code and data for CLUES are available at https://github.com/microsoft/CLUES.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=VhIIQBm00VI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/microsoft/CLUES" target="_blank" rel="nofollow noreferrer">https://github.com/microsoft/CLUES</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9-LSfSU74n-" data-number="19">
        <h4>
          <a href="/forum?id=9-LSfSU74n-">
              A Dataset for Answering Time-Sensitive Questions
          </a>


            <a href="/pdf?id=9-LSfSU74n-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Wenhu_Chen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhu_Chen3">Wenhu Chen</a>, <a href="/profile?id=~Xinyi_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyi_Wang2">Xinyi Wang</a>, <a href="/profile?id=~William_Yang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Yang_Wang2">William Yang Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">12 Aug 2021 (modified: 04 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">10 Replies</span>


        </div>

          <a href="#9-LSfSU74n--details-521" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9-LSfSU74n--details-521"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Question Answering, Temporal Reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We found that the existing QA models are lacking the capability to perform robust temporal reasoning, thus we construct a dataset to better benchmark progress in this direction </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model's temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1) mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best-performing model FiD can only achieve 46\% accuracy, still far behind the human performance of 87\%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9-LSfSU74n-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/wenhuchen/Time-Sensitive-QA" target="_blank" rel="nofollow noreferrer">https://github.com/wenhuchen/Time-Sensitive-QA</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="qkcLxoC52kL" data-number="276">
        <h4>
          <a href="/forum?id=qkcLxoC52kL">
              OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs
          </a>


            <a href="/pdf?id=qkcLxoC52kL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Weihua_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weihua_Hu1">Weihua Hu</a>, <a href="/profile?id=~Matthias_Fey2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Fey2">Matthias Fey</a>, <a href="/profile?id=~Hongyu_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongyu_Ren1">Hongyu Ren</a>, <a href="/profile?id=~Maho_Nakata1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maho_Nakata1">Maho Nakata</a>, <a href="/profile?id=~Yuxiao_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxiao_Dong1">Yuxiao Dong</a>, <a href="/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jure_Leskovec1">Jure Leskovec</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">20 Aug 2021 (modified: 03 Nov 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#qkcLxoC52kL-details-705" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qkcLxoC52kL-details-705"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Machine Learning, Graph Neural Networks, Open Graph Benchmark, Large-scale graphs</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present OGB-LSC, a set of three large-scale graph datasets, to advance state-of-the-art in large-scale graph machine learning. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks---link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=qkcLxoC52kL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://ogb.stanford.edu/docs/lsc/" target="_blank" rel="nofollow noreferrer">https://ogb.stanford.edu/docs/lsc/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="SSKZPJCt7B" data-number="108">
        <h4>
          <a href="/forum?id=SSKZPJCt7B">
              RobustBench: a standardized adversarial robustness benchmark
          </a>


            <a href="/pdf?id=SSKZPJCt7B" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Francesco_Croce1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Croce1">Francesco Croce</a>, <a href="/profile?id=~Maksym_Andriushchenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maksym_Andriushchenko1">Maksym Andriushchenko</a>, <a href="/profile?id=~Vikash_Sehwag1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikash_Sehwag1">Vikash Sehwag</a>, <a href="/profile?id=~Edoardo_Debenedetti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edoardo_Debenedetti1">Edoardo Debenedetti</a>, <a href="/profile?id=~Nicolas_Flammarion1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Flammarion1">Nicolas Flammarion</a>, <a href="/profile?id=~Mung_Chiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mung_Chiang2">Mung Chiang</a>, <a href="/profile?id=~Prateek_Mittal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prateek_Mittal1">Prateek Mittal</a>, <a href="/profile?id=~Matthias_Hein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Hein2">Matthias Hein</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">19 Aug 2021 (modified: 31 Oct 2021)</span>
              <span class="item">NeurIPS 2021 Datasets and Benchmarks Track (Round 2)</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">13 Replies</span>


        </div>

          <a href="#SSKZPJCt7B-details-546" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SSKZPJCt7B-details-546"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Adversarial robustness, Adversarial examples, Benchmarking robustness, Deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a standardized benchmark for adversarial robustness along with a unified access to the model zoo (80+ models) and a detailed analysis of robust networks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="32" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container>- and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="33" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=SSKZPJCt7B&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://robustbench.github.io/" target="_blank" rel="nofollow noreferrer">https://robustbench.github.io/</a></span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>