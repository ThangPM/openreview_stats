<ul class="list-unstyled submissions-list">
    <li class="note " data-id="q5IBAUCsQQ" data-number="400">
        <h4>
          <a href="/forum?id=q5IBAUCsQQ">
              Homekit2020: A Time Series Classification Benchmark for Influenza Prediction Using Wearable Devices
          </a>


            <a href="/pdf?id=q5IBAUCsQQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mike_A_Merrill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_A_Merrill1">Mike A Merrill</a>, <a href="/profile?id=~Arinbj%C3%B6rn_Kolbeinsson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arinbjörn_Kolbeinsson1">Arinbjörn Kolbeinsson</a>, <a href="/profile?id=~Piyusha_Gade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piyusha_Gade1">Piyusha Gade</a>, <a href="/profile?id=~Ernesto_Ramirez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ernesto_Ramirez1">Ernesto Ramirez</a>, <a href="/profile?id=~Ludwig_Schmidt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ludwig_Schmidt1">Ludwig Schmidt</a>, <a href="/profile?id=~Luca_Foschini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luca_Foschini1">Luca Foschini</a>, <a href="/profile?id=~Tim_Althoff2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Althoff2">Tim Althoff</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#q5IBAUCsQQ-details-89" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q5IBAUCsQQ-details-89"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">wearables, time series classification, sensing, mobile health</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Homekit2020 is a dataset and benchmark of 592,000 user-days of FitBit data for evaluating time series classification models on influenza detection tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite increased interest in wearables as a tool for the detection of respiratory viral infections through time series classification, there are not as of yet any public benchmarks or evaluation schemes for such mobile sensing data. The few datasets that are shared typically do not contain data from more than dozens of individuals, do not contain high resolution raw data, and are not published with Pytorch dataloaders for easy integration into machinelearning pipelines. These limitations preclude statistically meaningful comparisons between classifiers, decrease the availability of training data, and hinder rapid experimentation with new models. Furthermore, there is no consensus within the domain as to how tasks should be structured, leading to evaluations that do not represent real world performance and do not facilitate apples-to-apples comparisons between publications. Here, we present Homekit2020: the first large-scale, public, pip-installable dataset of wearable data, symptom reports, and ground-truth laboratory PCR test results, along with an evaluation framework that mimics realistic model deployments and characterizes statistical uncertainty with respect to model selection. Furthermore, we evaluate several neural and non-neural baselines on our benchmark in order to establish baseline performance on this benchmark by evaluating SOTA models from influenza detection and time series classification.

        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=q5IBAUCsQQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/behavioral-data/Homekit2020" target="_blank" rel="nofollow noreferrer">https://github.com/behavioral-data/Homekit2020</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">Our dataset uses an open credentialized access procedure similar to PhysioNet. Access to the data requires a verified Synapse account and a data use agreement.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.synapse.org/#!Synapse:syn22803188" target="_blank" rel="nofollow noreferrer">https://www.synapse.org/#!Synapse:syn22803188</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is available under the MIT License, and use of the data is subject to the conditions outlined in the data use agreement. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uufDJhmter" data-number="399">
        <h4>
          <a href="/forum?id=uufDJhmter">
              BRI3L: A brightness illusion image dataset for identification and localization of regions of illusory perception
          </a>


            <a href="/pdf?id=uufDJhmter" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Aniket_Roy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aniket_Roy1">Aniket Roy</a>, <a href="/profile?id=~Anirban_Roy3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirban_Roy3">Anirban Roy</a>, <a href="/profile?id=~Soma_Mitra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soma_Mitra1">Soma Mitra</a>, <a href="/profile?id=~Kuntal_Ghosh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuntal_Ghosh2">Kuntal Ghosh</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#uufDJhmter-details-55" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uufDJhmter-details-55"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">illusions, visual perception</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">dataset to study illusions</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, therefore, the conclusions are not generic. The effectiveness of deep learning models to understand and interpret visual illusions hindered due to lack of large-scale benchmark dataset. To this end, we first generate a large-scale dataset of 22,366 images of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusion/non-illusion, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated in this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset.
        To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization.
        In this study, we consider  brightness illusions where the perceived brightness of a region is different from the actual luminosity in the image. Specifically, we consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White's illusion, 4) Grid illusion, and 5) Induced Grating illusion.
        Benchmarking on the dataset achieves 99.56\% accuracy in illusion identification and 84.37\% pixel accuracy in illusion localization. This model also generalizes over unseen brightness illusions like Dungeon, Mach bands as well as brightness assimilation to contrast transitions, and even natural images.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uufDJhmter&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/file/d/1X3Gqrp8Vq8PrF_e0w6K3jxgLuqYIHDoF/view?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/file/d/1X3Gqrp8Vq8PrF_e0w6K3jxgLuqYIHDoF/view?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/file/d/1X3Gqrp8Vq8PrF_e0w6K3jxgLuqYIHDoF/view?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/file/d/1X3Gqrp8Vq8PrF_e0w6K3jxgLuqYIHDoF/view?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ueUXUjlXAMf" data-number="398">
        <h4>
          <a href="/forum?id=ueUXUjlXAMf">
              A Framework for Large Scale Synthetic Graph Dataset Generation
          </a>


            <a href="/pdf?id=ueUXUjlXAMf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sajad_Darabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sajad_Darabi1">Sajad Darabi</a>, <a href="/profile?id=~Piotr_Bigaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piotr_Bigaj1">Piotr Bigaj</a>, <a href="/profile?id=~Dawid_Majchrowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawid_Majchrowski1">Dawid Majchrowski</a>, <a href="/profile?id=~Pawel_Morkisz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pawel_Morkisz1">Pawel Morkisz</a>, <a href="/profile?id=~Alex_Fit-Florea1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Fit-Florea1">Alex Fit-Florea</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ueUXUjlXAMf-details-390" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ueUXUjlXAMf-details-390"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Generation, Deep Graph Learning, Framework and Tool, Data Generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a scalable synthetic graph generation tool.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many graph analysis tasks such as node and edge classification, link prediction, and clustering with numerous practical applications such as fraud detection, drug discovery, or recommender systems. Allbeit there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications with trillions of edges and billions of nodes. Further, new algorithms and models are benchmarked across similar datasets with similar properties. In this work, we tackle this shortcoming by proposing a scalable synthetic graph generation tool that can mimic the original data distribution of real-world graphs and scale them to arbitrary sizes. This tool can be used then to learn a set of parametric models from proprietary datasets that can subsequently be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. Finally, the performance of the graph learning algorithms depends not only on the size but also on the dataset's structure. We show how our framework generalizes across a set of datasets, mimicking both structural and feature distributions as well as its scalability across varying dataset sizes.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ueUXUjlXAMf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zBBmV-i84Go" data-number="397">
        <h4>
          <a href="/forum?id=zBBmV-i84Go">
              Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets
          </a>


            <a href="/pdf?id=zBBmV-i84Go" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gokul_NC1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gokul_NC1">Gokul NC</a>, <a href="/profile?id=~Manideep_Ladi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manideep_Ladi1">Manideep Ladi</a>, <a href="/profile?id=~Sumit_Negi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sumit_Negi1">Sumit Negi</a>, <a href="/profile?id=~Prem_Selvaraj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prem_Selvaraj1">Prem Selvaraj</a>, <a href="/profile?id=~Pratyush_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pratyush_Kumar1">Pratyush Kumar</a>, <a href="/profile?id=~Mitesh_M_Khapra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mitesh_M_Khapra1">Mitesh M Khapra</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#zBBmV-i84Go-details-417" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zBBmV-i84Go-details-417"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">sign language</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release the largest available pretraining dataset for sign language across multiple languages and show how multilingual fine-tuning using a unified vocabulary is helpful to achieve SOTA results</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition. We make three contributions. First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all PII. Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual foundation model which can be fine-tuned for various sign recognition tasks across languages. Third, we create MultiSign-SLR, a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS, a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results. All datasets, models, and code will be open-sourced in the OpenHands library.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zBBmV-i84Go&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The uploading of datasets is still in-progress.
        Hence we provide links in advance as to where they will be available within a couple of weeks after all the submissions.

        Unlabeled pretraining datasets for pose-based self-supervised learning:
        https://openhands.ai4bharat.org/en/latest/instructions/self_supervised.html

        Label-aligned ISLR and finger-spelling pose-based datasets:
        https://openhands.ai4bharat.org/en/latest/instructions/datasets.html
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="T2NklUScCDH" data-number="395">
        <h4>
          <a href="/forum?id=T2NklUScCDH">
              Making Intelligence: Values, IQ, and ML Benchmarks
          </a>


            <a href="/pdf?id=T2NklUScCDH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Borhane_Blili-Hamelin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Borhane_Blili-Hamelin1">Borhane Blili-Hamelin</a>, <a href="/profile?id=~Leif_Hancox-Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leif_Hancox-Li1">Leif Hancox-Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#T2NklUScCDH-details-559" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T2NklUScCDH-details-559"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Benchmarks, measurements, values, ethics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We draw on the literature demonstrating how ethical values inevitably influence measures of human intelligence to argue that the same applies for benchmarks in ML.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Current work on datasets and benchmarks often implicitly assumes that one can define good benchmarks from a neutral scientific standpoint. In this position paper, we argue that more attention needs to be paid to the unavoidable roles of ethical values when it comes to designing and selecting benchmarks. We bring research on the role of values in science to bear on the question of how AI systems are evaluated. Specifically, we examine the relevance of feminist critiques of the value- neutrality of IQ and human intelligence research for the case of ML benchmarks. Human intelligence and ML benchmarks share similarities in setting standards for describing, evaluating and comparing performance on tasks relevant to intelligence. We outline a few ways that ML can change its evaluation practices to explicitly consider ethical values.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="lRUCfzs5Hzg" data-number="394">
        <h4>
          <a href="/forum?id=lRUCfzs5Hzg">
              How Transferable are Video Representations Based on Synthetic Data?
          </a>


            <a href="/pdf?id=lRUCfzs5Hzg" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yo-whan_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yo-whan_Kim1">Yo-whan Kim</a>, <a href="/profile?id=~SouYoung_Jin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SouYoung_Jin2">SouYoung Jin</a>, <a href="/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rameswar_Panda1">Rameswar Panda</a>, <a href="/profile?id=~Hilde_Kuehne5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hilde_Kuehne5">Hilde Kuehne</a>, <a href="/profile?id=~Leonid_Karlinsky3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonid_Karlinsky3">Leonid Karlinsky</a>, <a href="/profile?id=~Samarth_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samarth_Mishra1">Samarth Mishra</a>, <a href="/profile?id=~Venkatesh_Saligrama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Venkatesh_Saligrama1">Venkatesh Saligrama</a>, <a href="/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kate_Saenko1">Kate Saenko</a>, <a href="/profile?id=~Aude_Oliva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aude_Oliva1">Aude Oliva</a>, <a href="/profile?id=~Rogerio_Feris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rogerio_Feris1">Rogerio Feris</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#lRUCfzs5Hzg-details-13" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lRUCfzs5Hzg-details-13"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Action recognition has improved dramatically with massive-scale video datasets. Yet, these datasets are accompanied with issues related to curation cost, privacy, ethics, bias, and copyright. Compared to that, only minor efforts have been devoted toward exploring the potential of synthetic video data. In this work, as a stepping stone towards addressing these shortcomings, we study the transferability of video representations learned solely from synthetically-generated video clips, instead of real data. We propose SynAPT, a novel benchmark for action recognition based on a combination of existing synthetic datasets, in which a model is pre-trained on synthetic videos rendered by various graphics simulators, and then transferred to a set of downstream action recognition datasets, containing different categories than the synthetic data. We provide an extensive baseline analysis on SynAPT revealing that the simulation-to-real gap is minor for datasets with low object and scene bias, where models pre-trained with synthetic data even outperform their real data counterparts. We posit that the gap between real and synthetic action representations can be attributed to contextual bias and static objects related to the action, instead of the temporal dynamics of the action itself. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=lRUCfzs5Hzg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MU2495w47rz" data-number="393">
        <h4>
          <a href="/forum?id=MU2495w47rz">
              OpenXAI: Towards a Transparent Evaluation of Model Explanations
          </a>


            <a href="/pdf?id=MU2495w47rz" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chirag_Agarwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chirag_Agarwal1">Chirag Agarwal</a>, <a href="/profile?id=~Eshika_Saxena1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eshika_Saxena1">Eshika Saxena</a>, <a href="/profile?id=~Satyapriya_Krishna2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Satyapriya_Krishna2">Satyapriya Krishna</a>, <a href="/profile?id=~Martin_Pawelczyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Pawelczyk1">Martin Pawelczyk</a>, <a href="/profile?id=~Nari_Johnson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nari_Johnson1">Nari Johnson</a>, <a href="/profile?id=~Isha_Puri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Isha_Puri1">Isha Puri</a>, <a href="/profile?id=~Marinka_Zitnik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marinka_Zitnik1">Marinka Zitnik</a>, <a href="/profile?id=~Himabindu_Lakkaraju1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Himabindu_Lakkaraju1">Himabindu Lakkaraju</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#MU2495w47rz-details-478" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MU2495w47rz-details-478"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">explainable artificial intelligence, benchmark, evaluation, leaderboard</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce OpenXAI, a flexible and comprehensive open source ecosystem for evaluating, comparing, and benchmarking SOTA as well as any newly proposed explanation methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While several types of post hoc explanation methods (e.g., feature attribution methods) have been proposed in recent literature, there is little to no work on systematically benchmarking these methods in an efficient and transparent manner. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to benchmark explanations. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MU2495w47rz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://open-xai.github.io/" target="_blank" rel="nofollow noreferrer">https://open-xai.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="heBKnuV42O" data-number="392">
        <h4>
          <a href="/forum?id=heBKnuV42O">
              DDXPlus: A New Dataset For Automatic Medical Diagnosis
          </a>


            <a href="/pdf?id=heBKnuV42O" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Arsene_Fansi_Tchango1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arsene_Fansi_Tchango1">Arsene Fansi Tchango</a>, <a href="/profile?id=~Rishab_Goel3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishab_Goel3">Rishab Goel</a>, <a href="/profile?id=~Zhi_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhi_Wen1">Zhi Wen</a>, <a href="/profile?id=~Julien_Martel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julien_Martel2">Julien Martel</a>, <a href="/profile?id=~Joumana_Ghosn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joumana_Ghosn1">Joumana Ghosn</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#heBKnuV42O-details-307" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="heBKnuV42O-details-307"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Automatic Diagnosis, Automatic Symptom Detection, Differential Diagnosis, Synthetic Patients</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents, for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=heBKnuV42O&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374" target="_blank" rel="nofollow noreferrer">https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374" target="_blank" rel="nofollow noreferrer">https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY licence</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="olvz0gAdGOX" data-number="391">
        <h4>
          <a href="/forum?id=olvz0gAdGOX">
              ActionNet: A Multimodal Dataset for Human Activities Using Wearable Sensors in a Kitchen Environment
          </a>


            <a href="/pdf?id=olvz0gAdGOX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Joseph_DelPreto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_DelPreto1">Joseph DelPreto</a>, <a href="/profile?id=~Chao_Liu9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Liu9">Chao Liu</a>, <a href="/profile?id=~Yiyue_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiyue_Luo1">Yiyue Luo</a>, <a href="/profile?id=~Michael_Foshey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Foshey1">Michael Foshey</a>, <a href="/profile?id=~Yunzhu_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunzhu_Li1">Yunzhu Li</a>, <a href="/profile?id=~Antonio_Torralba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonio_Torralba1">Antonio Torralba</a>, <a href="/profile?id=~Wojciech_Matusik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Matusik2">Wojciech Matusik</a>, <a href="/profile?id=~Daniela_Rus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniela_Rus1">Daniela Rus</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#olvz0gAdGOX-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="olvz0gAdGOX-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Wearable sensors, multimodal dataset, activities of daily living, kitchen activities, robot assistants, learning pipelines, motion tracking, eye tracking, tactile sensing, muscle activity, cameras, microphones</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A multimodal dataset and recording framework use wearable sensors and synchronized ground-truth data to record humans performing kitchen tasks, with the goal of enabling insights into manipulation, task planning, and more capable robot assistants.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces ActionNet, a multimodal dataset and recording framework with an emphasis on wearable sensing in a kitchen environment.  It provides rich, synchronized data streams along with ground truth data to facilitate learning pipelines that could extract insights about how humans interact with the physical world during activities of daily living, and help lead to more capable and collaborative robot assistants.  The wearable sensing suite captures motion, force, and attention information; it includes eye tracking with a first-person camera, forearm muscle activity sensors, a body-tracking system using 17 inertial sensors, finger-tracking gloves, and custom tactile sensors on the hands that use a matrix of conductive threads.  This is coupled with activity labels and with externally-captured data from multiple RGB cameras, a depth camera, and microphones.  The specific tasks recorded in ActionNet are designed to highlight lower-level physical skills and higher-level scene reasoning or action planning.  They include simple object manipulations (e.g., stacking plates), dexterous actions (e.g., peeling or cutting vegetables), and complex action sequences (e.g., setting a table or loading a dishwasher).  The resulting dataset and underlying experiment framework are available at https://action-net.csail.mit.edu. ActionNet aims to support future applications including evaluating the relative effectiveness of modality subsets, activity classification, and fine-grained action segmentation. It could also help inform the next generation of smart textiles that may one day unobtrusively send rich data streams to in-home collaborative or autonomous robot assistants.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=olvz0gAdGOX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://action-net.csail.mit.edu" target="_blank" rel="nofollow noreferrer">https://action-net.csail.mit.edu</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://action-net.csail.mit.edu" target="_blank" rel="nofollow noreferrer">https://action-net.csail.mit.edu</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons; in particular we plan on using a CC BY-NC-SA 4.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="lFYqQPqIHtd" data-number="390">
        <h4>
          <a href="/forum?id=lFYqQPqIHtd">
              End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models
          </a>


            <a href="/pdf?id=lFYqQPqIHtd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Barry_Menglong_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Barry_Menglong_Yao1">Barry Menglong Yao</a>, <a href="/profile?id=~Aditya_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Shah1">Aditya Shah</a>, <a href="/profile?id=~Lichao_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lichao_Sun1">Lichao Sun</a>, <a href="/profile?id=~Jin-Hee_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jin-Hee_Cho1">Jin-Hee Cho</a>, <a href="/profile?id=~Lifu_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lifu_Huang1">Lifu Huang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#lFYqQPqIHtd-details-885" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lFYqQPqIHtd-details-885"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fact checking, fact verification, multimodal fact-checking, multimodal fake news detection, explainable fact-checking, explanation generation, evidence retrieval, claim verification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The paper introduces a novel dataset for end-to-end multimodal fact-checking, which consists of retrieving relevant evidence, predicting a truthfulness label, and generating a rationalization statement to explain the reasoning and ruling process.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We propose the end-to-end multimodal fact-checking and explanation generation, where the input is a claim and a large collection of web sources, including articles, images, videos, and tweets, and the goal is to assess the truthfulness of the claim by retrieving relevant evidences and predicting a truthfulness label (i.e., support, refute and not enough information), and to generate a rationalization statement to explain the reasoning and ruling process. To support this research, we construct MOCHEG, a large-scale dataset consisting of 21,184  claims where each claim is annotated with a truthfulness label and ruling statement, with 43,148 text evidences and 15,375 image evidences. To establish baseline performances on MOCHEG, we experiment with several state-of-the-art neural architectures on the three pipelined subtasks: multimodal evidence retrieval, claim verification and explanation generation, and demonstrate that the performance of the state-of-the-art end-to-end multimodal fact-checking does not provide satisfactory outcomes.  To the best of our knowledge, we are the first to build the benchmark dataset and solutions for end-to-end multimodal fact-checking and explanation generation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=lFYqQPqIHtd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://nlplab1.cs.vt.edu/~menglong/project/multimodal/fact_checking/MOCHEG/dataset/" target="_blank" rel="nofollow noreferrer">http://nlplab1.cs.vt.edu/~menglong/project/multimodal/fact_checking/MOCHEG/dataset/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://nlplab1.cs.vt.edu/~menglong/project/multimodal/fact_checking/MOCHEG/dataset/" target="_blank" rel="nofollow noreferrer">http://nlplab1.cs.vt.edu/~menglong/project/multimodal/fact_checking/MOCHEG/dataset/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset is licensed under the CC BY 4.0. The associated codes to MOCHEG for data crawler and baseline are licensed under Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="pciyt0TM31V" data-number="389">
        <h4>
          <a href="/forum?id=pciyt0TM31V">
              Look, Radiate, and Learn: Self-supervised Localisation via Radio-Visual Correspondence
          </a>


            <a href="/pdf?id=pciyt0TM31V" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mohammed_Alloulah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammed_Alloulah1">Mohammed Alloulah</a>, <a href="/profile?id=~Maximilian_Arnold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Arnold1">Maximilian Arnold</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#pciyt0TM31V-details-644" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pciyt0TM31V-details-644"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, radio, radar, radio-visual correspondence, contrastive learning, perception, cross-modal learning, localisation, 6G, sensing, correspondence learning, joint communication and sensing</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Learning to self-label radar images using paired visual images</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Next generation cellular networks will implement radio sensing functions alongside customary communications, thereby enabling unprecedented worldwide sensing coverage outdoors. Deep learning has revolutionised computer vision but has had limited application to radio perception tasks, in part due to lack of systematic datasets and benchmarks dedicated to the study of the performance and promise of radio sensing. To address this gap, we present MaxRay: a synthetic radio-visual dataset and benchmark that facilitate precise target localisation in radio. We further propose to learn to localise targets in radio without supervision by extracting self-coordinates from radio-visual correspondence. We use such self-supervised coordinates to train a radio localiser network. We characterise our performance against a number of state-of-the-art baselines. Our results indicate that accurate radio target localisation can be automatically learned from paired radio-visual data without labels, which is highly relevant to empirical data. This opens the door for vast data scalability and may prove key to realising the promise of robust radio sensing atop a unified perception-communication cellular infrastructure. Dataset will be hosted on IEEE DataPort.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=pciyt0TM31V&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We will release the full dataset to the public domain next year. Until then during review process, we have provided instructions in our code repo below on how to download a subset of the dataset.

        We have created the following private repo, GitHub, and Gmail accounts for sharing code (and account authentication if applicable) to facilitate the review process until we open source everything next year.

        GitHub repo: https://github.com/MaximilianArnold/look-radiate-learn

        Gmail account: look.radiate.learn@gmail.com PW: T6By5rHbm576kh2
        GitHub account: look.radiate.learn@gmail.com PW: T6By5rHbm576kh2

        </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">Ditto above.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MzaPEKHv-0J" data-number="388">
        <h4>
          <a href="/forum?id=MzaPEKHv-0J">
              PeRFception: Perception using Radiance Fields
          </a>


            <a href="/pdf?id=MzaPEKHv-0J" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yoonwoo_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoonwoo_Jeong1">Yoonwoo Jeong</a>, <a href="/profile?id=~Seungjoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungjoo_Shin1">Seungjoo Shin</a>, <a href="/profile?id=~Junha_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junha_Lee2">Junha Lee</a>, <a href="/profile?id=~Chris_Choy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Choy1">Chris Choy</a>, <a href="/profile?id=~Anima_Anandkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anima_Anandkumar1">Anima Anandkumar</a>, <a href="/profile?id=~Minsu_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minsu_Cho1">Minsu Cho</a>, <a href="/profile?id=~Jaesik_Park3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaesik_Park3">Jaesik Park</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#MzaPEKHv-0J-details-980" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MzaPEKHv-0J-details-980"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, neural radiance field, image classification, 3D shape classification, 3D semantic segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a new dataset, PeRFception dataset, that is a new unified radiance field dataset for the 2D image classification, 3D shape classification, and 3D semantic segmentation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale implicit representation datasets  for perception tasks, called the  \textbf{\ourdataset{}}, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the  classification and segmentation models that directly take as input this implicit format and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data will be publicly available. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MzaPEKHv-0J&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/file/d/16g0CM2NFTjFgB90QFVjCPzVAgP7o77AD/view?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/file/d/16g0CM2NFTjFgB90QFVjCPzVAgP7o77AD/view?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/file/d/16g0CM2NFTjFgB90QFVjCPzVAgP7o77AD/view?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/file/d/16g0CM2NFTjFgB90QFVjCPzVAgP7o77AD/view?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We will publicly release the data and benchmarked models. For details, we recommend to read the attached instruction file. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The PeRFception datasets are released under the Attribution-NonCommercial 4.0 International (CC23
        BY-NC 4.0) license2, which allows anybody to use them for non-commercial research and educational24
        purposes25</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="DcfsR89KUa" data-number="387">
        <h4>
          <a href="/forum?id=DcfsR89KUa">
              Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world
          </a>


            <a href="/pdf?id=DcfsR89KUa" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eugene_Vinitsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Vinitsky1">Eugene Vinitsky</a>, <a href="/profile?id=~Nathan_Lichtl%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nathan_Lichtlé1">Nathan Lichtlé</a>, <a href="/profile?id=~Xiaomeng_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaomeng_Yang1">Xiaomeng Yang</a>, <a href="/profile?id=~Brandon_Amos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brandon_Amos1">Brandon Amos</a>, <a href="/profile?id=~Jakob_Nicolaus_Foerster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_Nicolaus_Foerster1">Jakob Nicolaus Foerster</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 29 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#DcfsR89KUa-details-780" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DcfsR89KUa-details-780"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-Agent Reinforcement Learning, Benchmark, Simulator, Self-Driving, Human-Modeling</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a fast, data-driven simulator for studying multi-agent partially observed coordination in human driving.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce \textit{Nocturne}, a new 2D driving simulator for investigating multi-agent coordination under partial observability. The focus of Nocturne is to enable research into inference and theory of mind in real-world multi-agent settings without the computational overhead of computer vision and feature extraction from images. Agents in this simulator only observe an obstructed view of the scene, mimicking human visual sensing constraints. Unlike existing benchmarks that are bottlenecked by rendering human-like observations directly using a camera input, Nocturne uses efficient intersection methods to compute a vectorized set of visible features in a C++ back-end, allowing the simulator to run at $2000+$ steps-per-second. Using open-source trajectory and map data, we construct a simulator to load and replay arbitrary trajectories and scenes from real-world driving data. Using this environment, we benchmark reinforcement-learning and imitation-learning agents and demonstrate that the agents are quite far from human-level coordination ability and deviate significantly from the expert trajectories.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=DcfsR89KUa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/facebookresearch/nocturne" target="_blank" rel="nofollow noreferrer">https://github.com/facebookresearch/nocturne</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="H4Po2dDzdFq" data-number="386">
        <h4>
          <a href="/forum?id=H4Po2dDzdFq">
              FACT: Learning Governing Abstractions Behind Integer Sequences
          </a>


            <a href="/pdf?id=H4Po2dDzdFq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peter_Belcak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Belcak1">Peter Belcak</a>, <a href="/profile?id=~Ard_Kastrati1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ard_Kastrati1">Ard Kastrati</a>, <a href="/profile?email=flaviosc%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="flaviosc@ethz.ch">Flavio Schenker</a>, <a href="/profile?id=~Roger_Wattenhofer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roger_Wattenhofer1">Roger Wattenhofer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#H4Po2dDzdFq-details-94" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H4Po2dDzdFq-details-94"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, benchmark, integer, sequences, abstraction, learning, evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A toolkit with a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Integer sequences are of central importance to the modeling of concepts admitting complete finitary descriptions. We introduce a novel view on the learning of such concepts and lay down a set of benchmarking tasks aimed at conceptual understanding by machine learning models. These tasks indirectly assess model ability to abstract, and challenge them to reason both interpolatively and extrapolatively from the knowledge gained by observing representative examples. To further aid research in knowledge representation and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit. The toolkit surrounds a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations, enabling the making of the future advancements with ease.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=H4Po2dDzdFq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/FACT-Development-Team/FACT" target="_blank" rel="nofollow noreferrer">https://github.com/FACT-Development-Team/FACT</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/FACT-Development-Team/FACT

        The README.md file of the repository explains where the dataset can be downloaded.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This work is licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FA9jVbCIgBh" data-number="385">
        <h4>
          <a href="/forum?id=FA9jVbCIgBh">
              A Survey and Datasheet Repository of Publicly Available US Criminal Justice Datasets
          </a>


            <a href="/pdf?id=FA9jVbCIgBh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Miri_Zilka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Miri_Zilka1">Miri Zilka</a>, <a href="/profile?email=b.butcher%40sussex.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="b.butcher@sussex.ac.uk">Bradley Butcher</a>, <a href="/profile?id=~Adrian_Weller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Weller1">Adrian Weller</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#FA9jVbCIgBh-details-266" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FA9jVbCIgBh-details-266"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Criminal justice is an increasingly important application domain for machine learning and algorithmic fairness, as predictive tools are becoming widely used in police, courts, and prison systems worldwide. A few relevant benchmarks have received significant attention, e.g., the COMPAS dataset, often without proper consideration of the domain context. To raise awareness of publicly available criminal justice datasets and encourage their responsible use, we conduct a survey, consider contexts, highlight potential uses, and identify gaps and limitations. We provide datasheets for 15 datasets and upload them to a public repository. We compare the datasets across several dimensions, including size, coverage of the population, and potential use, highlighting concerns. We hope that this work can provide a useful starting point for researchers looking for appropriate datasets related to criminal justice, and that the repository will continue to grow as a community effort. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FA9jVbCIgBh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="7etfTT9ppbO" data-number="384">
        <h4>
          <a href="/forum?id=7etfTT9ppbO">
              An Empirical Deep Dive into Deep Learning's Driving Dynamics
          </a>


            <a href="/pdf?id=7etfTT9ppbO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Charles_Edison_Tripp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Charles_Edison_Tripp1">Charles Edison Tripp</a>, <a href="/profile?id=~Jordan_Perr-Sauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jordan_Perr-Sauer1">Jordan Perr-Sauer</a>, <a href="/profile?id=~Lucas_Hayne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucas_Hayne1">Lucas Hayne</a>, <a href="/profile?id=~Monte_Lunacek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Monte_Lunacek1">Monte Lunacek</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#7etfTT9ppbO-details-928" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7etfTT9ppbO-details-928"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generalization, optimization, dataset, neural architecture search, fully-connected networks, regularization, empirical study, batch size, learning rate, label noise, depth, topology, shape</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a large empirical dataset of training runs for fully-connected networks with a range of configurations on a variety of learning tasks (datasets).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present an empirical dataset surveying the deep learning phenomenon on fully-connected networks, encompassing the training and test performance of numerous network topologies, sweeping across multiple learning tasks, depths, numbers of free parameters, learning rates, batch sizes, and regularization penalties. The dataset probes 178 thousand hyperparameter settings with an average of 20 repetitions each, totalling 3.5 million training runs and 20 performance metrics for each of the 11.7 billion training epochs observed. Accumulating this 671 GB dataset utilized 5,448 CPU core-years, 17.8 GPU-years, and 111.2 node-years. Additionally, we provide a preliminary analysis revealing patterns which persist across learning tasks and topologies. We aim to inspire work empirically studying modern machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=7etfTT9ppbO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.25984/1872441" target="_blank" rel="nofollow noreferrer">https://doi.org/10.25984/1872441</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://data.openei.org/submissions/5708
        https://doi.org/10.25984/1872441

        The OEDI entry for the dataset lists several dataset resources: https://doi.org/10.25984/1872441

        One is the readme describing the dataset and the file structure in detail: https://github.com/openEDI/documentation/blob/main/BUTTER.md .

        Another listed resource is the "S3 Dataset Viewer" (https://data.openei.org/s3_viewer?bucket=oedi-data-lake&amp;prefix=butter%2F) which allows direct downloads of the dataset as tar files. Un-tarred copies of the dataset are also stored here and can be queried directly (online, without downloading it). A working online example notebook which queries the data online can be found and tested here: https://mybinder.org/v2/gh/NREL/BUTTER-Better-Understanding-of-Training-Topologies-through-Empirical-Results/main?labpath=examples%2FBUTTER-Access-Example-Python.ipynb

        https://oedi-data-lake.s3.amazonaws.com/butter/all_runs.tar is a direct download link of a tar file containing the complete run data dataset as described in the readme.

        https://oedi-data-lake.s3.amazonaws.com/butter/complete_summary.tar is a direct download link of a tar file containing the complete summary data dataset as described in the readme.

        As described in the readme, both run and summary datasets are stored as hive-partitioned compressed parquet files. Once untarred (or online via S3), the datasets can be queried directly with a partitioned parquet viewer such as pyarrow. See the example notebook for sample code demonstrating how to do so.

        More examples of querying and plotting the dataset can be found here: https://github.com/NREL/BUTTER-Better-Understanding-of-Training-Topologies-through-Empirical-Results and are described in the paper, supplementary material, and dataset readme.

        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is licensed with the CC Attribution-ShareAlike 4.0 License. The source code is licensed with the MIT License.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="UrAYT2QwOX8" data-number="382">
        <h4>
          <a href="/forum?id=UrAYT2QwOX8">
              bank-account-fraud: Tabular Dataset(s) for Fraud Detection under Multiple Bias Conditions
          </a>


            <a href="/pdf?id=UrAYT2QwOX8" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~S%C3%A9rgio_Jesus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sérgio_Jesus1">Sérgio Jesus</a>, <a href="/profile?id=~Jos%C3%A9_Pombal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~José_Pombal1">José Pombal</a>, <a href="/profile?email=duarte.alves%40feedzai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="duarte.alves@feedzai.com">Duarte Alves</a>, <a href="/profile?id=~Andr%C3%A9_Cruz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Cruz1">André Cruz</a>, <a href="/profile?id=~Pedro_Saleiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Saleiro1">Pedro Saleiro</a>, <a href="/profile?id=~Rita_P._Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rita_P._Ribeiro1">Rita P. Ribeiro</a>, <a href="/profile?email=joao.jgama%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="joao.jgama@gmail.com">João Gama</a>, <a href="/profile?id=~Pedro_Bizarro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Bizarro1">Pedro Bizarro</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#UrAYT2QwOX8-details-927" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UrAYT2QwOX8-details-927"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">tabular data, bias, fairness, fraud detection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Evaluating new ML techniques on realistic datasets plays a crucial role in the development of ML research and broader adoption by practitioners. Furthermore, with the growing ethical concerns around the potential of bias in algorithmic decision-making, fairness evaluation is becoming a standard practice in ML. However, while there has been a significant increase of publicly available unstructured datasets for computer vision and NLP tasks, there is still a lack of large-scale domain-specific tabular datasets, which hinders potential research applied to this particular domain. Additionally, many high-stakes decision-making applications, where an accurate evaluation of algorithmic fairness is of paramount importance, rely on this type of data. Ultimately, this affects the quality of the deployed models in critical applications, and consequently, automated decisions applied to people. To tackle this issue, we present bank-account-fraud, the first publicly available, large-scale, and privacy-preserving suite of tabular datasets for fraud detection. The suite was generated by applying state-of-the-art tabular dataset generation techniques on an anonymized, real-world bank account opening application dataset. This setting carries a set of challenges that are commonplace in real world applications, including distribution shifts in time and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods in dynamic environments, each dataset variant of the presented suite depicts a specific predetermined and controlled type of data bias, including time-related patterns. With this dataset, we hope to potentiate ML research, through a more realistic, complete and robust test bed for novel and existing ML methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=UrAYT2QwOX8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/feedzai/bank-account-fraud" target="_blank" rel="nofollow noreferrer">https://github.com/feedzai/bank-account-fraud</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/feedzai/bank-account-fraud" target="_blank" rel="nofollow noreferrer">https://github.com/feedzai/bank-account-fraud</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons CC BY-NC-ND 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ikw7gqAGz7A" data-number="381">
        <h4>
          <a href="/forum?id=ikw7gqAGz7A">
              Creating Variants of Freebase for Robust Development of Intelligent Tasks on Knowledge Graphs
          </a>


            <a href="/pdf?id=ikw7gqAGz7A" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Farahnaz_Akrami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Farahnaz_Akrami1">Farahnaz Akrami</a>, <a href="/profile?id=~Mohammed_Samiul_Saeef1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammed_Samiul_Saeef1">Mohammed Samiul Saeef</a>, <a href="/profile?id=~Nasim_Shirvani_Mahdavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nasim_Shirvani_Mahdavi1">Nasim Shirvani Mahdavi</a>, <a href="/profile?id=~Xiao_Shi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Shi2">Xiao Shi</a>, <a href="/profile?id=~Chengkai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengkai_Li1">Chengkai Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ikw7gqAGz7A-details-603" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ikw7gqAGz7A-details-603"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">knowledge graphs, Freebase, link prediction, graph query, natural language generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper analyzes data modeling idiosyncrasies of Freebase, studies their impacts on developing models and algorithms for intelligent tasks on knowledge graphs, and provides several variants of Freebase by inclusion/exclusion of such idiosyncrasies</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Knowledge graphs (KGs) are an essential asset to a wide variety of tasks and applications in the fields of artificial intelligence and machine learning. They encode rich semantic, factual information, and different datasets could be potentially linked together for purposes greater than what they support separately. Despite the growing importance of KGs, many KGs are kept proprietary and  many of the publicly available datasets are relatively small. Freebase is amongst the largest public cross-domain KGs that store common facts. It possesses several data modeling idiosyncrasies rarely found in comparable datasets such as Wikidata, YAGO, and so on. It has a strong type system; its properties are purposefully represented in reverse pairs; and it uses mediator objects to facilitate representation of multiary relationships. These design choices serve important practical purposes in realistically modeling the real-world. But they also pose nontrivial challenges that could hinder the advancement of KG-oriented technologies. More specifically, when algorithms and models for intelligent tasks are developed and evaluated agnostically of these data modeling idiosyncrasies, one could either miss the opportunity to leverage such features or fall into pitfalls without knowing. This paper lays out a comprehensive analysis of the challenges associated with the aforementioned idiosyncrasies of Freebase, measures their impact on tasks such as link prediction, and provides several variants of the Freebase dataset by inclusion/exclusion of various data modeling idiosyncrasies. Furthermore, the datasets underwent thorough cleaning in order to improve their utility. The datasets and data preprocessing scripts are made publicly available. They can be a valuable resource to researchers and practitioners in developing technologies by and for knowledge graphs. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ikw7gqAGz7A&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/idirlab/freebases" target="_blank" rel="nofollow noreferrer">https://github.com/idirlab/freebases</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/idirlab/freebases" target="_blank" rel="nofollow noreferrer">https://github.com/idirlab/freebases</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LnWCryDOXPz" data-number="380">
        <h4>
          <a href="/forum?id=LnWCryDOXPz">
              GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks
          </a>


            <a href="/pdf?id=LnWCryDOXPz" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kenza_Amara1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenza_Amara1">Kenza Amara</a>, <a href="/profile?id=~Zhitao_Ying1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhitao_Ying1">Zhitao Ying</a>, <a href="/profile?id=~Zitao_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zitao_Zhang1">Zitao Zhang</a>, <a href="/profile?email=zhihan%40ebay.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhihan@ebay.com">Zhihao Han</a>, <a href="/profile?email=yshan%40ebay.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yshan@ebay.com">Yinan Shan</a>, <a href="/profile?email=ubrandes%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="ubrandes@ethz.ch">Ulrik Brandes</a>, <a href="/profile?email=sebastian.schemm%40env.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="sebastian.schemm@env.ethz.ch">Sebastian Schemm</a>, <a href="/profile?id=~Ce_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ce_Zhang1">Ce Zhang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#LnWCryDOXPz-details-330" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LnWCryDOXPz-details-330"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph neural networks, explainability, systematic evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the first systematic evaluation framework for graph neural network explainability.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As one of the most popular machine learning models today, graph neural networks (GNNs) have attracted intense interest recently, and so does their explainability. Users are increasingly interested in a better understanding of GNN models and their outcomes. Unfortunately, today's evaluation frameworks for GNN explainability often rely on synthetic datasets, leading to conclusions of limited scope due to a lack of complexity in the problem instances. As GNN models are deployed to more mission-critical applications, we are in dire need for a common evaluation protocol of explainability methods of GNNs. In this paper, we propose, to our best knowledge, the first systematic evaluation framework for GNN explainability, considering explainability on three different "user needs:" explanation focus, mask nature, and mask transformation. We propose a unique metric that combines the fidelity measures and classify explanations based on their quality of being sufficient or necessary. We scope ourselves to node classification tasks and compare the most representative techniques in the field of input-level explainability for GNNs. For the widely used synthetic benchmarks, surprisingly shallow techniques such as personalized PageRank have the best performance for a minimum computation time. But when the graph structure is more complex and nodes have meaningful features, gradient-based methods, in particular Saliency, are the best according to our evaluation criteria. However, none dominates the others on all evaluation dimensions and there is always a trade-off. We further apply our evaluation protocol in a case study on eBay graphs to reflect the production environment.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LnWCryDOXPz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) [2022] [Kenza Amara]

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="nQZHEunntbJ" data-number="379">
        <h4>
          <a href="/forum?id=nQZHEunntbJ">
              AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels
          </a>


            <a href="/pdf?id=nQZHEunntbJ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nicholas_Roberts2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Roberts2">Nicholas Roberts</a>, <a href="/profile?id=~Xintong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xintong_Li2">Xintong Li</a>, <a href="/profile?id=~Tzu-Heng_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tzu-Heng_Huang1">Tzu-Heng Huang</a>, <a href="/profile?id=~Dyah_Adila1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dyah_Adila1">Dyah Adila</a>, <a href="/profile?email=spencer.schoenberg%40wisc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="spencer.schoenberg@wisc.edu">Spencer Schoenberg</a>, <a href="/profile?id=~Cheng-Yu_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng-Yu_Liu1">Cheng-Yu Liu</a>, <a href="/profile?email=lpick2%40wisc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lpick2@wisc.edu">Lauren Pick</a>, <a href="/profile?email=hma232%40wisc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hma232@wisc.edu">Haotian Ma</a>, <a href="/profile?id=~Aws_Albarghouthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aws_Albarghouthi1">Aws Albarghouthi</a>, <a href="/profile?id=~Frederic_Sala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederic_Sala1">Frederic Sala</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#nQZHEunntbJ-details-552" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nQZHEunntbJ-details-552"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">weak supervision, automated weak supervision, foundation models, automl, diverse tasks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce AutoWS-Bench-101: a benchmarking framework for automated weak supervision techniques on diverse tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Weak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings---a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveal the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning. We observe that it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=nQZHEunntbJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Kaylee0501/AutoWS-Bench-101" target="_blank" rel="nofollow noreferrer">https://github.com/Kaylee0501/AutoWS-Bench-101</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Kaylee0501/AutoWS-Bench-101" target="_blank" rel="nofollow noreferrer">https://github.com/Kaylee0501/AutoWS-Bench-101</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code license: Apache 2.0
        Dataset licenses:
        • MNIST: CC BY-SA 3.0
        • CIFAR-10: CC BY 4.0 (on https://www.tensorflow.org/datasets/catalog/cifar10)
        • Spherical MNIST: MIT
        • Permuted MNIST: Apache 2.0
        • Navier-Stokes: MIT
        • ECG: ODC-BY 1.0
        • EMBER: MIT</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="XbC2O26s-Y2" data-number="378">
        <h4>
          <a href="/forum?id=XbC2O26s-Y2">
              Deep Learning-based Code Complexity Prediction
          </a>


            <a href="/pdf?id=XbC2O26s-Y2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mingi_Jeon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingi_Jeon1">Mingi Jeon</a>, <a href="/profile?email=sybaik2006%40yonsei.a.ckr" class="profile-link" data-toggle="tooltip" data-placement="top" title="sybaik2006@yonsei.a.ckr">Seung-yeop Baik</a>, <a href="/profile?id=~Joonghyuk_Hahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joonghyuk_Hahn1">Joonghyuk Hahn</a>, <a href="/profile?id=~Yo-Sub_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yo-Sub_Han1">Yo-Sub Han</a>, <a href="/profile?id=~Sang-Ki_Ko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sang-Ki_Ko1">Sang-Ki Ko</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#XbC2O26s-Y2-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XbC2O26s-Y2-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">computational complexity, code classification, programming language, code understanding</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We suggest a deep-learning based approach for estimating computational (time) complexity of given programs and provide the largest code complexity dataset as the benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deciding the computational complexity of algorithms is a really challenging problem even for human algorithm experts. Theoretically, the problem of deciding the computational complexity of a given program is undecidable due to the famous Halting problem. In this paper, we tackle the problem by designing a neural network that comprehends the algorithmic nature of codes and estimates the worst-case complexity.
        First, we construct a code dataset called the CodeComplex that consists of 4,200 Java codes submitted to programming competitions by human programmers and their complexity labels annotated by a group of algorithm experts. As far as we are aware, the CodeComplex dataset is by far the largest code dataset for the complexity prediction problem. Then, we present several baseline algorithms using the previous code understanding neural models such as CodeBERT, GraphCodeBERT, PLBART, and CodeT5. As the previous code understanding models do not work well on longer codes due to the code length limit, we propose the hierarchical Transformer architecture which takes method-level code snippets instead of whole codes and combines the method-level embeddings to the class-level embedding and ultimately to the code-level embedding. Moreover, we introduce pre-training objectives for the proposed model to induce the model to learn both the intrinsic property of the method-level codes and the relationship between the components.
        Lastly, we demonstrate that the proposed hierarchical architecture and pre-training objectives achieve state-of-the-art performance in terms of complexity prediction accuracy compared to the previous code understanding models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=XbC2O26s-Y2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/yonsei-toc/CodeComple" target="_blank" rel="nofollow noreferrer">https://github.com/yonsei-toc/CodeComple</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/yonsei-toc/CodeComple" target="_blank" rel="nofollow noreferrer">https://github.com/yonsei-toc/CodeComple</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache-2.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="HM4d3kKv99I" data-number="377">
        <h4>
          <a href="/forum?id=HM4d3kKv99I">
              SATBench: Benchmarking the speed-accuracy tradeoff in object recognition by humans and dynamic neural networks
          </a>


            <a href="/pdf?id=HM4d3kKv99I" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ajay_Subramanian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ajay_Subramanian1">Ajay Subramanian</a>, <a href="/profile?id=~Sara_Brittany_Price1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sara_Brittany_Price1">Sara Brittany Price</a>, <a href="/profile?id=~Omkar_Kumbhar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Omkar_Kumbhar1">Omkar Kumbhar</a>, <a href="/profile?id=~Elena_Sizikova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elena_Sizikova1">Elena Sizikova</a>, <a href="/profile?id=~Najib_J._Majaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Najib_J._Majaj1">Najib J. Majaj</a>, <a href="/profile?email=denis.pelli%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="denis.pelli@nyu.edu">Denis G Pelli</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#HM4d3kKv99I-details-842" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HM4d3kKv99I-details-842"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">object recognition, speed-accuracy tradeoff, human psychophysics, dynamic neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We collect a human dataset of timed ImageNet object recognition and propose to model the human speed-accuracy tradeoff using dynamic neural networks, a class of networks capable of inference-time adaptive computation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The core of everyday tasks like reading and driving is active object recognition. Attempts to model such tasks are currently stymied by the inability to incorporate time. People show a flexible tradeoff between speed and accuracy and this tradeoff is a crucial human skill. Deep neural networks have emerged as promising candidates for predicting peak human object recognition performance and neural activity. However, modeling the temporal dimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to serve as useful computational models for how humans recognize objects. To this end, we here present the first large-scale (148 observers, 4 neural networks, 8 tasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet images. In each human trial, a beep, indicating the desired reaction time, sounds at a fixed delay after the image is presented, and observer's response counts only if it occurs near the time of the beep. In a series of blocks, we test many beep latencies, i.e., reaction times. We observe that human accuracy increases with reaction time and proceed to compare its characteristics with the behavior of several dynamic neural networks that are capable of inference-time adaptive computation. Using FLOPs as an analog for reaction time, we compare networks with humans on curve-fit error, category-wise correlation, and curve steepness, and conclude that cascaded dynamic neural networks are a promising model of human reaction time in object recognition tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=HM4d3kKv99I&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ajaysub110/satbench" target="_blank" rel="nofollow noreferrer">https://github.com/ajaysub110/satbench</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset URL: https://osf.io/2cpmb/

        The URL above contains compressed human and network data files which are available publicly and downloadable for free. Code required to process and analyze the dataset can be found in the attached public GitHub repository: https://github.com/ajaysub110/satbench</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons (CC) License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ZpHIbrET8iy" data-number="376">
        <h4>
          <a href="/forum?id=ZpHIbrET8iy">
              PRIDE: A benchmark for structure-guided protein design evaluation
          </a>


            <a href="/pdf?id=ZpHIbrET8iy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhihang_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhihang_Hu1">Zhihang Hu</a>, <a href="/profile?id=~Hanqun_CAO1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanqun_CAO1">Hanqun CAO</a>, <a href="/profile?id=~Dongchen_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongchen_He1">Dongchen He</a>, <a href="/profile?id=~Wenjian_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjian_Jiang1">Wenjian Jiang</a>, <a href="/profile?id=~Tao_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Shen2">Tao Shen</a>, <a href="/profile?id=~Sheng_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheng_Wang4">Sheng Wang</a>, <a href="/profile?id=~Siqi_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siqi_Sun2">Siqi Sun</a>, <a href="/profile?id=~Irwin_King1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irwin_King1">Irwin King</a>, <a href="/profile?id=~Yu_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Li1">Yu Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ZpHIbrET8iy-details-127" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZpHIbrET8iy-details-127"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Protein Design, Deep Learning, Benchmark Dataset, Protein Structure</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Protein design seeks to identify sequences with desired structures performing novel functions and generate proteins not found in nature to improve proteins for human purposes. In recent years, practical success has come from the advancement of computational resources and large amounts of data that describe existing sequences, structures, and functions.  Here we first reviewed most existing protein design methods and organized them into different categories. Moreover, it is difficult to validate and compare performance between methods since protein design has not yet a uniform specification, such as datasets and evaluation metrics. Therefore, in this work, we further propose a new benchmark and related evaluation metric assisting researchers to work unparalleled toward a unified direction. The dataset is manipulated and constructed on the Protein Data Bank which consists of 32893 distinct sequences in total. Meanwhile, comparisons of existing protein design's performance on our provided dataset and designed metric are made.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ZpHIbrET8iy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Oa2-cdfBxun" data-number="374">
        <h4>
          <a href="/forum?id=Oa2-cdfBxun">
              mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors
          </a>


            <a href="/pdf?id=Oa2-cdfBxun" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sizhe_An1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sizhe_An1">Sizhe An</a>, <a href="/profile?id=~Yin_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yin_Li3">Yin Li</a>, <a href="/profile?id=~Umit_Ogras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Umit_Ogras1">Umit Ogras</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Oa2-cdfBxun-details-13" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Oa2-cdfBxun-details-13"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Human Pose Estimation, Human Activity Recognition, IMU, Multi-Modal, mmWave, Healthcare, Rehabilitation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">mRI is a large-scale multi-modal human pose estimation dataset focusing on rehab movements, supporting human pose estimation and human activity recognition tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability to estimate 3D human body pose and movement, also known as human pose estimation (HPE), enables many applications for home-based health monitoring, such as remote rehabilitation training. Several possible solutions have emerged using sensors ranging from RGB cameras, depth sensors, millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite previous efforts on datasets and benchmarks for HPE, few dataset exploits multiple modalities and focuses on home-based health monitoring. To bridge the gap, we present mRI, a multi-modal 3D human pose estimation dataset with mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 5 million frames from 20 subjects performing rehabilitation exercises and supports the benchmarks of HPE and action detection. We perform extensive experiments using our dataset and delineate the strength of each modality. We hope that the release of mRI can catalyze the research in pose estimation, multi-modal learning, and action understanding, and more importantly facilitate the applications of home-based health monitoring. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Oa2-cdfBxun&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://sizhean.github.io/mri  Note that we only show demo and part of data now in the project page as the data and code will involve the privacy protocol as stated in Ethics statement. Post-processing data (RGB) will delay the dataset release date but we will upload the data once we finish it, before the main conference.</span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value "><a href="https://sizhean.github.io/mri" target="_blank" rel="nofollow noreferrer">https://sizhean.github.io/mri</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/SizheAn/mRI" target="_blank" rel="nofollow noreferrer">https://github.com/SizheAn/mRI</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="HuWeozHW0FT" data-number="373">
        <h4>
          <a href="/forum?id=HuWeozHW0FT">
              Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text
          </a>


            <a href="/pdf?id=HuWeozHW0FT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cheng-An_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng-An_Hsieh1">Cheng-An Hsieh</a>, <a href="/profile?id=~Cheng-Ping_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng-Ping_Hsieh1">Cheng-Ping Hsieh</a>, <a href="/profile?id=~Pu-Jen_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pu-Jen_Cheng1">Pu-Jen Cheng</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#HuWeozHW0FT-details-21" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HuWeozHW0FT-details-21"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multimodal retrieval, multimodal representation, information retrieval</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">a novel multimodal retrieval dataset containing multimodal documents and multimodal-related queries.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multimodal learning is a recent challenge that extends unimodal learning by generalizing its domain to diverse modalities, such as texts, images, or speech. This extension requires models to process and relate information from multiple modalities. In Information Retrieval, traditional retrieval tasks focus on the similarity between unimodal documents and queries, while image-text retrieval hypothesizes that most texts contain the scene context from images. This separation has ignored that real-world queries may involve text content, image captions, or both. To address this, we introduce Multimodal Retrieval on Representation of ImaGe witH Text (Mr. Right), a novel and robust dataset for multimodal retrieval. We utilize the Wikipedia dataset with rich text-image examples and generate three types of queries: text-related, image-related, and mixed. To validate the effectiveness of our dataset, we provide a multimodal training paradigm and evaluate previous text retrieval and image retrieval frameworks.  The results show that proposed multimodal retrieval can improve retrieval performance, but creating a well-unified document representation with texts and images is still a challenge. We hope Mr. Right allows us to broaden current retrieval systems better and contributes to accelerating the advancement of multimodal learning in the Information Retrieval.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=HuWeozHW0FT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hsiehjackson/Mr.Right" target="_blank" rel="nofollow noreferrer">https://github.com/hsiehjackson/Mr.Right</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/hsiehjackson/Mr.Right" target="_blank" rel="nofollow noreferrer">https://github.com/hsiehjackson/Mr.Right</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="3dRVo6NwJAP" data-number="371">
        <h4>
          <a href="/forum?id=3dRVo6NwJAP">
              How hard are computer vision datasets? Calibrating dataset difficulty to viewing time
          </a>


            <a href="/pdf?id=3dRVo6NwJAP" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~David_Mayo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Mayo1">David Mayo</a>, <a href="/profile?id=~Jesse_Cummings1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Cummings1">Jesse Cummings</a>, <a href="/profile?id=~Xinyu_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyu_Lin1">Xinyu Lin</a>, <a href="/profile?id=~Dan_Gutfreund1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Gutfreund1">Dan Gutfreund</a>, <a href="/profile?id=~Boris_Katz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Katz1">Boris Katz</a>, <a href="/profile?id=~Andrei_Barbu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrei_Barbu3">Andrei Barbu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#3dRVo6NwJAP-details-286" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3dRVo6NwJAP-details-286"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Humans outperform object recognizers despite the fact that models perform well on current datasets. Numerous efforts attempt to make more challenging datasets by scaling them up from the web, exploring distribution shift, or adding controls for biases. The difficulty of each image in each dataset is not independently evaluated, nor is the concept of dataset difficulty as a whole well-posed. We develop a new dataset difficulty metric based on how long humans must view an image in order to classify a target object. Images whose objects can be recognized in 17ms are considered to be easier than those which require seconds of viewing time. Using 133,588 judgments on two major datasets, ImageNet and ObjectNet, we determine the distribution of image difficulties in those datasets, which we find varies wildly, but significantly undersamples hard images. Rather than hoping that distribution shift or other approaches will lead to hard datasets, we should measure the difficulty of datasets and seek to explicitly fill out the class of difficult examples. Analyzing model performance guided by image difficulty reveals that models tend to have lower performance and a larger generalization gap on harder images. Encourag- ingly for the biological validity of current architectures, much of the variance in human difficulty can be accounted for given an object recognizer by computing a combination of prediction depth, c-score, and adversarial robustness. We release a dataset of such judgments as a complementary metric to raw performance and a network’s ability to explain neural recordings. Such experiments with humans allow us to create a metric for progress in object recognition datasets, which we find are skewed toward easy examples, to test the biological validity of models in a novel way, and to develop tools for shaping datasets as they are being gathered to focus them on filling out the missing class of hard examples from today’s datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=3dRVo6NwJAP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://objectnet.dev/flash" target="_blank" rel="nofollow noreferrer">http://objectnet.dev/flash</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://objectnet.dev/flash" target="_blank" rel="nofollow noreferrer">http://objectnet.dev/flash</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mPL8xA2C-zq" data-number="369">
        <h4>
          <a href="/forum?id=mPL8xA2C-zq">
              Offline Reinforcement Learning with Crowd-sourced Rewards
          </a>


            <a href="/pdf?id=mPL8xA2C-zq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kimin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kimin_Lee1">Kimin Lee</a>, <a href="/profile?id=~Shixiang_Shane_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shixiang_Shane_Gu1">Shixiang Shane Gu</a>, <a href="/profile?id=~Craig_Boutilier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Craig_Boutilier2">Craig Boutilier</a>, <a href="/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="/profile?id=~Mohammad_Ghavamzadeh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Ghavamzadeh2">Mohammad Ghavamzadeh</a>, <a href="/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ofir_Nachum1">Ofir Nachum</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#mPL8xA2C-zq-details-564" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mPL8xA2C-zq-details-564"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline RL</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Offline reinforcement learning (RL) provides a framework to train agents from a fixed dataset without interaction with the environment. Offline RL benchmark datasets generally comprise sequences of state-action-reward tuples, which is disconnected from practical settings where providing a ground-truth reward label for every state-action pair in the dataset may be infeasible. Indeed, many practical applications use a crowd-sourced pool of diverse annotators to compare or rate logged interactions. Using such crowd-sourced rewards poses two challenges for offline training: (i) different annotators may rate state-action pairs using different (personal, subjective) reward functions; (ii) it is often more natural for annotators to compare/rate \emph{trajectories} rather than individual state-action pairs, which can exacerbate credit assignment problems. In this work, we introduce new offline RL benchmarks that embody these aspects of crowd-sourced data. To systemically investigate the impact of each aspect on offline RL methods, we create new datasets by relabeling existing offline datasets with new reward functions. We benchmark state-of-the-art offline RL algorithms using these datasets, showing that crowd-sourced-like data can induce poor performance. We also evaluate simple reward shaping techniques (e.g., learning a reward function from preferences and standardizing labeled rewards). While these simple patches can improve  performance in some cases, in others they are ineffective, highlighting key areas for future research to improve. We hope that our benchmark can accelerate the development of new offline RL methods for real-world applications.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mPL8xA2C-zq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/submission_offline_RL-8D54" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/submission_offline_RL-8D54</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset: unless otherwise noted, all datasets are licensed under the Creative Commons Attribution 4.0 License (CC BY), and code is licensed under the Apache 2.0 License.

        Training code: all training codes are licensed under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="d48qsHzdhiu" data-number="368">
        <h4>
          <a href="/forum?id=d48qsHzdhiu">
              DeepAccident: A Large-Scale Accident Dataset for Multi-Vehicle Autonomous Driving
          </a>


            <a href="/pdf?id=d48qsHzdhiu" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~TIANQI_WANG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~TIANQI_WANG1">TIANQI WANG</a>, <a href="/profile?id=~Wenxuan_Ji3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenxuan_Ji3">Wenxuan Ji</a>, <a href="/profile?id=~Shoufa_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoufa_Chen1">Shoufa Chen</a>, <a href="/profile?id=~Chongjian_GE1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjian_GE1">Chongjian GE</a>, <a href="/profile?id=~Enze_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enze_Xie1">Enze Xie</a>, <a href="/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#d48qsHzdhiu-details-539" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="d48qsHzdhiu-details-539"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Autonomous Driving, 3D Object Detection, Motion Prediction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we propose a large-scale dataset, termed DeepAccident, which contains various accident scenarios reconstructed deliberately in a realistic simulator, targeting to improve the safety of autonomous driving. Existing datasets typically collected data in simple and safe driving situations because of the catastrophic consequences of collecting data in those accident-prone situations, making it difficult to validate the performance of the autonomous driving system in edge-case scenarios where most of the accidents are reported from. Unlike existing datasets which only provide a viewpoint from the ego-vehicle, each scenario in the DeepAccident dataset involves four data collection vehicles: two vehicles for causing accidents and another two vehicles following behind them, respectively, to offer various viewpoints and enable multi-vehicle cooperative autonomous driving. In detail, the DeepAccident dataset provides a comprehensive sensor set for each data collection vehicle to collect multi-modal data, including LiDAR, surrounding cameras, motion, and HD maps. The dataset contains 131k annotated point clouds (3$\times$ of nuScenes) and 791k annotated camera images, which can support various tasks, including 3D object detection, Bird's Eye View (BEV) segmentation, and motion prediction. We have extensively tested the proposed DeepAccident dataset for LiDAR-based 3D object detection and motion prediction task as well as validating the performance boost provided by multi-vehicle cooperation. Project page: \url{https://hku-deepaccident.github.io/}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=d48qsHzdhiu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://hku-deepaccident.github.io/" target="_blank" rel="nofollow noreferrer">https://hku-deepaccident.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://hku-deepaccident.github.io/" target="_blank" rel="nofollow noreferrer">https://hku-deepaccident.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NCSA 4.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Oqr3AOjTm9m" data-number="367">
        <h4>
          <a href="/forum?id=Oqr3AOjTm9m">
              Critically Assessing the State of the Art in Neural Network Verification
          </a>


            <a href="/pdf?id=Oqr3AOjTm9m" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Matthias_K%C3%B6nig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_König1">Matthias König</a>, <a href="/profile?id=~Annelot_Willemijn_Bosman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Annelot_Willemijn_Bosman1">Annelot Willemijn Bosman</a>, <a href="/profile?id=~Holger_Hoos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Holger_Hoos1">Holger Hoos</a>, <a href="/profile?id=~Jan_N._van_Rijn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_N._van_Rijn1">Jan N. van Rijn</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Oqr3AOjTm9m-details-554" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Oqr3AOjTm9m-details-554"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural Network Verification, Benchmarking, Algorithm Portfolios, Shapley value</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent works have proposed many methods to formally verify neural networks against minimal input perturbations. This type of verification is referred to as local robustness verification. The field of local robustness verification is highly diverse, as verifiers rely on a multitude of techniques, such as Mixed Integer Programming or Satisfiability Modulo Theories. At the same time, problem instances differ based on the network that is verified, the network input or the verification property. This gives rise to the question, which verification algorithm is most suitable to solve a given verification problem. To answer this question, we perform an extensive analysis of current evaluation practices for local robustness verifiers as well as an empirical performance assessment of several verification methods across a broad set of neural networks and properties. Most notably, we find that most algorithms only support ReLU-based networks, while other activation functions remain under-supported. Furthermore, we show that there is no single best algorithm that dominates in performance across all problem instances and illustrate the potential of using algorithm portfolios for more efficient local robustness verification.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Oqr3AOjTm9m&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/marti-mcfly/nn-verification-assessment" target="_blank" rel="nofollow noreferrer">https://github.com/marti-mcfly/nn-verification-assessment</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="pnV7n7zpLUm" data-number="366">
        <h4>
          <a href="/forum?id=pnV7n7zpLUm">
              Silicodata: A chest X-ray Dataset for Silicosis Detection
          </a>


            <a href="/pdf?id=pnV7n7zpLUm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yasmeena_Akhter2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yasmeena_Akhter2">Yasmeena Akhter</a>, <a href="/profile?id=~Rishabh_Ranjan4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Ranjan4">Rishabh Ranjan</a>, <a href="/profile?id=~Mayank_Vatsa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mayank_Vatsa1">Mayank Vatsa</a>, <a href="/profile?id=~Richa_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richa_Singh1">Richa Singh</a>, <a href="/profile?id=~Santanu_Chaudhury2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santanu_Chaudhury2">Santanu Chaudhury</a>, <a href="/profile?id=~Anjali_Agrawal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anjali_Agrawal1">Anjali Agrawal</a>, <a href="/profile?email=pallavi.rao%40imagecorelab.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pallavi.rao@imagecorelab.com">Pallavi Rao</a>, <a href="/profile?email=dr.aggarwal.shruti%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dr.aggarwal.shruti@gmail.com">Shruti Aggarwal</a>, <a href="/profile?email=arjun.kalyanpur%40telradsol.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="arjun.kalyanpur@telradsol.com">Arjun Kalyanpur</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#pnV7n7zpLUm-details-785" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pnV7n7zpLUm-details-785"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Silicosis, Pneumoconiosis, Data Constrained Problem</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Proposed a dataset for Silicosis and related disease detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Designing machine learning algorithms for assisting medical experts offers significant societal benefits and is one of the high priority areas of growth for several countries. Similar to the process of algorithm design for other applications, healthcare applications also require problem specific databases. However, unlike other machine learning problems, the data collection in healthcare is more challenging and annotation requires the expertise of healthcare professionals. This research aims at creating a dataset for a healthcare challenge known as Silicosis. It is a progressive interstitial lung disease which occurs due to occupational exposure and remains a significant occupational hazard worldwide. Due to the nature of the disease, the findings of silicosis often is confused with tuberculosis and silicotuberculosis. Therefore, in the proposed database, along with samples belonging to silicosis and healthy images, these two related classes are also included. This is the first public dataset of its kind and contains detailed annotations for lung segmentation, disease region segmentation, and disease prediction from multiple radiologists. We also present the baseline results for segmentation and classification. The results show that state-of-the-art classifiers are unable to yield good prediction results on different classes and it requires significant focused efforts to address this arduous research challenge.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=pnV7n7zpLUm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://iab-rubric.org/resources/Silicodata.html" target="_blank" rel="nofollow noreferrer">http://iab-rubric.org/resources/Silicodata.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wPEXGTzZJt" data-number="365">
        <h4>
          <a href="/forum?id=wPEXGTzZJt">
              ViSioNS: Visual Search in Natural Scenes Benchmark
          </a>


            <a href="/pdf?id=wPEXGTzZJt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ferm%C3%ADn_Travi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fermín_Travi1">Fermín Travi</a>, <a href="/profile?id=~Gonzalo_Ruarte1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gonzalo_Ruarte1">Gonzalo Ruarte</a>, <a href="/profile?id=~Gaston_Bujia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaston_Bujia1">Gaston Bujia</a>, <a href="/profile?id=~Juan_E_Kamienkowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juan_E_Kamienkowski1">Juan E Kamienkowski</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#wPEXGTzZJt-details-407" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wPEXGTzZJt-details-407"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">visual search, eye movements, computational models of human behavior, ideal bayesian observer</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper builds a benchmark for comparing state-of-the-art human visual search models on different datasets comprising eye movements in natural scenes, discussing their limitations and how their integration could lead to performance improvements.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Visual search is an essential part of almost any everyday human interaction with the visual environment. Nowadays, several algorithms are able to predict gaze positions during simple observation, but few models attempt to simulate human behavior during visual search in natural scenes. Furthermore, these models vary widely in their design and exhibit differences in the datasets and metrics with which they were evaluated. Thus, there is a need for a reference point, on which each model can be tested and from where potential improvements can be derived. In this study, we select publicly available state-of-the-art visual search models and datasets in natural scenes, and provide a common framework for their evaluation. To this end, we apply a unified format and criteria, bridging the gaps between them, and we estimate the models’ efficiency and similarity with humans using a specific set of metrics. This integration has allowed us to enhance the Ideal Bayesian Searcher by combining it with a neural network-based visual search model, which enables it to generalize to other datasets. The present work sheds light on the limitations of current models and how integrating different approaches with a unified criteria can lead to better algorithms. Moreover, it moves forward on bringing forth a solution for the urgent need for benchmarking data and metrics to support the development of more general human visual search computational models. All of the code used here, including metrics, plots, and visual search models, alongside the preprocessed datasets, are available at $\url{https://github.com/FerminT/VisualSearchBenchmark}$.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wPEXGTzZJt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/FerminT/VisualSearchBenchmark" target="_blank" rel="nofollow noreferrer">https://github.com/FerminT/VisualSearchBenchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2021 Fermin Travi

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="x-Z14aN2fdN" data-number="361">
        <h4>
          <a href="/forum?id=x-Z14aN2fdN">
              NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds
          </a>


            <a href="/pdf?id=x-Z14aN2fdN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Patrick_Feeney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Feeney1">Patrick Feeney</a>, <a href="/profile?email=sarah.schneider%40tufts.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sarah.schneider@tufts.edu">Sarah Schneider</a>, <a href="/profile?id=~Panagiotis_Lymperopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Panagiotis_Lymperopoulos1">Panagiotis Lymperopoulos</a>, <a href="/profile?id=~Liping_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liping_Liu1">Liping Liu</a>, <a href="/profile?id=~Matthias_Scheutz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Scheutz1">Matthias Scheutz</a>, <a href="/profile?id=~Michael_C_Hughes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_C_Hughes1">Michael C Hughes</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#x-Z14aN2fdN-details-793" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x-Z14aN2fdN-details-793"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">novelty detection, category discovery, open world</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Multi-modal dataset of images and structured JSON text from an "open world" video game, suitable for novelty detection and adaptation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order for artificial agents to perform useful tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification. This practice restricts novelties to well-framed images of distinct object types. We suggest that new benchmarks are needed to represent the challenges of navigating an open world. Our new NovelCraft dataset contains multi-modal episodic data of the images and symbolic world-states seen by an agent completing a pogo-stick assembly task within a video game world. In some episodes, we insert novel objects that can impact gameplay. Novelty can vary in size, position, and occlusion within complex scenes. We benchmark state-of-the-art novelty detection and generalized category discovery models with a focus on comprehensive evaluation. Results suggest an opportunity for future research: models aware of task-specific costs of different types of mistakes could more effectively detect and adapt to novelty in open worlds.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=x-Z14aN2fdN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://NovelCraft.cs.tufts.edu" target="_blank" rel="nofollow noreferrer">https://NovelCraft.cs.tufts.edu</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://NovelCraft.cs.tufts.edu" target="_blank" rel="nofollow noreferrer">https://NovelCraft.cs.tufts.edu</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Data is released under a CC BY 4.0 license. Code is released under an MIT license.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Q83-QeTB9lS" data-number="360">
        <h4>
          <a href="/forum?id=Q83-QeTB9lS">
              A Large Scale Realistic Benchmark for Individual Treatment Effect Prediction and Uplift Modeling
          </a>


            <a href="/pdf?id=Q83-QeTB9lS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eustache_Diemert2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eustache_Diemert2">Eustache Diemert</a>, <a href="/profile?id=~Artem_Betlei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Artem_Betlei1">Artem Betlei</a>, <a href="/profile?id=~Christophe_Renaudin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christophe_Renaudin1">Christophe Renaudin</a>, <a href="/profile?id=~Massih-Reza_Amini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Massih-Reza_Amini1">Massih-Reza Amini</a>, <a href="/profile?email=theophane.gregoir%40mines-paristech.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="theophane.gregoir@mines-paristech.fr">Théophane Gregoir</a>, <a href="/profile?id=~Thibaud_Rahier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thibaud_Rahier1">Thibaud Rahier</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Q83-QeTB9lS-details-566" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q83-QeTB9lS-details-566"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Individual Treatment Effect Prediction, Uplift Modeling</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Large Scale Realistic Benchmark for Individual Treatment Effect Prediction and Uplift Modeling</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Individual Treatment Effect (ITE) prediction is an important area of research in machine learning which aims at explaining and estimating the causal impact of an action at the granular level. It represents a problem of growing interest in multiple sectors of application such as healthcare, online advertising or socioeconomics. To foster research on this topic we release a publicly available collection of 13.9 million samples collected from several randomized control trials, scaling up previously available datasets by a healthy 210x factor. We provide details on the data collection and perform sanity checks to validate the use of this data for causal inference tasks. First, we formalize the task of uplift modeling (UM) that can be performed with this data, along with the relevant evaluation metrics. Then, we propose synthetic yet near-realistic response surfaces and heterogeneous treatment assignment, providing a general set-up for ITE prediction. Finally, we report experiments to validate key characteristics of the dataset leveraging its size to evaluate and compare $-$ with high statistical significance $-$ a selection of baseline UM and ITE prediction methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Q83-QeTB9lS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://ailab.criteo.com/criteo-uplift-prediction-dataset/" target="_blank" rel="nofollow noreferrer">https://ailab.criteo.com/criteo-uplift-prediction-dataset/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset: https://ailab.criteo.com/criteo-uplift-prediction-dataset/
        Code: https://github.com/criteo-research/large-scale-ITE-UM-benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value "><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="nofollow noreferrer">https://creativecommons.org/licenses/by-nc-sa/4.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="2QgotfvO0mO" data-number="359">
        <h4>
          <a href="/forum?id=2QgotfvO0mO">
              Benchmarking Approximate k-Nearest Neighbour Search for Big High Dimensional Dynamic Data
          </a>


            <a href="/pdf?id=2QgotfvO0mO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ben_Harwood2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Harwood2">Ben Harwood</a>, <a href="/profile?id=~Amir_Dezfouli2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amir_Dezfouli2">Amir Dezfouli</a>, <a href="/profile?id=~Iadine_Chades1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iadine_Chades1">Iadine Chades</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#2QgotfvO0mO-details-114" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2QgotfvO0mO-details-114"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Nearest Neighbour Search, Similarity Search, Indexing, Knowledge retrieval, Knowledge discovery, High Dimensional Data, Big Data, Large Scale, Hashing, Graph Traversal, Product Quantisation, Online Learning, Representation Learning, Metric Learning, Robotic Vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel framework for benchmarking Approximate k-Nearest Neighbour (ANN) methods on big high dimensional dynamic data that identifies suitable ANN methods for ML and other applications and will accelerate future ANN research.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Approximate k-Nearest Neighbour (ANN) methods are commonly used for mining information from big high-dimensional datasets. For each application the high-level dataset properties and run-time requirements determine which method will provide the most suitable tradeoffs. However, due to a significant lack of comprehensive benchmarking, judicious method selection is not currently possible for ANN applications that involve frequent online changes to datasets. Here we address this issue by building upon existing benchmarks for static search problems to provide a new benchmarking framework for big high dimensional dynamic data. We apply our framework to dynamic scenarios modelled after common real world applications. In all cases we are able to identify a suitable recall-runtime tradeoff to improve upon a worst-case exhaustive search. Our framework provides a flexible solution to accelerate future ANN research and enable researchers in other online data-rich domains to find suitable methods for handling their ANN searches.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=2QgotfvO0mO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="H5lb8LvYPcW" data-number="357">
        <h4>
          <a href="/forum?id=H5lb8LvYPcW">
              KAIST-MTMC: A New Large-Scale Multi-Target Multiple Camera Tracking Benchmark
          </a>


            <a href="/pdf?id=H5lb8LvYPcW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sanghyun_Woo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanghyun_Woo1">Sanghyun Woo</a>, <a href="/profile?id=~Kwanyong_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kwanyong_Park1">Kwanyong Park</a>, <a href="/profile?id=~Inkyu_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Inkyu_Shin1">Inkyu Shin</a>, <a href="/profile?id=~Myungchul_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Myungchul_Kim2">Myungchul Kim</a>, <a href="/profile?email=jhyoon%40mirusystems.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jhyoon@mirusystems.com">Jaeho Yoon</a>, <a href="/profile?email=jhj%40testworks.co.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="jhj@testworks.co.kr">Hanjin Cho</a>, <a href="/profile?email=smchoi%40testworks.co.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="smchoi@testworks.co.kr">Su Mi Choi</a>, <a href="/profile?email=kimhyeongbok%40testworks.co.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="kimhyeongbok@testworks.co.kr">Hyeongbok Kim</a>, <a href="/profile?id=~In_So_Kweon2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~In_So_Kweon2">In So Kweon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#H5lb8LvYPcW-details-620" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H5lb8LvYPcW-details-620"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-camera Multi-object Tracking, Large-scale benchmark, Multi-object Tracking, Person Re-ID, Person Detection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-target multi-camera tracking aims to identify persons and their track identities over time from video streams taken by multiple cameras. It is one of the crucial research topics due to its potential impactful real-world applications, such as visual surveillance, crowd behavior analysis, and anomaly detection. However, as the data collection and labeling for this task are difficult and expensive, the existing datasets are constructed either synthetically or artificially within a controlled camera network setting. To overcome this, we build a new real-world, large-scale dataset, namely KAIST-MTMC. The advantages of introducing our new dataset to the community are threefold. First, it provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities. In particular, it contains long video sequences captured by 16 cameras in two different environments across various time, weather, and season conditions. Second, our work is the first that targets this task with an additional input modality. Specifically, our camera system uses spatially aligned and temporally synchronized RGB and thermal cameras, allowing the tracker to optionally utilize additional thermal information for more accurate multi-camera tracking. Last but not least, our new benchmark is a super-set of existing datasets such as person detection, re-identification, and (single camera-based) multiple object tracking, and thus can benefit this independent fields as well. We provide several baselines and new learning setups on this new challenging dataset and set the reference scores for future studies. The datasets, baselines, and benchmark servers will be made publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=H5lb8LvYPcW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/kaist-mtmc/home" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/kaist-mtmc/home</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">1) Landing Page: https://sites.google.com/view/kaist-mtmc/home
        2) Dataset Download Link: https://www.dropbox.com/sh/z949p05727wubuf/AAAYXlsMxyygsfj3yYVFcpz1a?dl=0
        Currently, only the validation set is available for download.
        Upon the publication, we will release the remaining train and test sets.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">KAIST-MTMC was established as part of the 「Intellectual Information Industry Infrastructure Construction」 project of the Ministry of Science and ICT (MSIT) and the National Information Society Agency (NIA) of South Korea.

        All rights to KAIST-MTMC, AI application models and data authoring tools, various manuals, etc., which are tangible and intangible results of this project, belong to KAIST and the National Information Society Agency (NIA). It can be used for for-profit and non-profit research and development purposes in various fields.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YgvkaodUT7l" data-number="354">
        <h4>
          <a href="/forum?id=YgvkaodUT7l">
              EmoWild: Building an In-the-Wild Non-Cognizant Image Emotion Dataset
          </a>


            <a href="/pdf?id=YgvkaodUT7l" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sethuraman_T_V1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sethuraman_T_V1">Sethuraman T V</a>, <a href="/profile?id=~Virendra_Patil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Virendra_Patil1">Virendra Patil</a>, <a href="/profile?id=~Manasa_Kolla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manasa_Kolla1">Manasa Kolla</a>, <a href="/profile?id=~Tridib_Mukherjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tridib_Mukherjee1">Tridib Mukherjee</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#YgvkaodUT7l-details-216" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YgvkaodUT7l-details-216"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Emotion Recognition, Valence-Arousal, Image Captioning, BERT, human cognition</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a shift in emotion annotation paradigm and provide a new benchmark for the community to explore and enhance the human emotion perception</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Estimating the perceived emotion to visual stimuli has gained significant traction in recent years. However, the data curated through subjective studies for this purpose are typically---(a) limited in scale and diversity (of both images and the subjects); and (b) conducted in a controlled environment making the subjects cognizant and self-aware---thus, consequently diluting the objective of capturing relatively spontaneous emotions evoked across a multitude of personalities. As a result, previous emotion estimation models built on such non-cognizant datasets fail to capture the entire image information and thus the emotional context----which is necessary for generalization and handling of cross-content and cross-subject variety. We address these key gaps in the emotion recognition domain by building: (i) a unique dataset corpus of 40k in-the-wild images (EmoWild) with continuous valence and arousal values from relatively subliminal comments of the users on social media, while analyzing and equitably accounting for the user persona; and (ii) Cognitive Contextual Summarization model that takes the captions generated from the images and feeds them to a BERT network----for contextual understanding, that typically evokes human emotion---thereby enabling continuous emotion estimation. The EmoWild dataset augments the model's ability to generalize better on a variety of images and datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YgvkaodUT7l&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">We plan to make this dataset public post acceptance and maintain the repository bearing all responsibilities</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The details are provided in the supplementary</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We plan to release our dataset post acceptance. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We will release our dataset under Creative Commons License. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="5mi-CkvEqj" data-number="353">
        <h4>
          <a href="/forum?id=5mi-CkvEqj">
              TweetNERD - End to End Entity Linking Benchmark for Tweets
          </a>


            <a href="/pdf?id=5mi-CkvEqj" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shubhanshu_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shubhanshu_Mishra1">Shubhanshu Mishra</a>, <a href="/profile?id=~Aman_Saini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aman_Saini1">Aman Saini</a>, <a href="/profile?id=~Raheleh_Makki2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raheleh_Makki2">Raheleh Makki</a>, <a href="/profile?id=~Sneha_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sneha_Mehta1">Sneha Mehta</a>, <a href="/profile?email=ahaghighi%40twitter.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ahaghighi@twitter.com">Aria Haghighi</a>, <a href="/profile?id=~Ali_Mollahosseini2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Mollahosseini2">Ali Mollahosseini</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#5mi-CkvEqj-details-308" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5mi-CkvEqj-details-308"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Twitter, Social Media, Named Entity Recognition, Named Entity Disambiguation, Entity Linking, Wikification, Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">TweetNERD is a dataset of 340K+ Tweets for benchmarking Named Entity Recognition and Disambiguation systems on English Tweets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Named Entity Recognition and Disambiguation (NERD) systems are foundational for information retrieval, question answering, event detection, and other natural language processing (NLP) applications. We introduce TweetNERD, a dataset of 340K+ tweets across 2010-2021, for benchmarking NERD systems on Tweets. This is the largest benchmark and most temporally diverse dataset for NERD on Tweets and can be used to facilitate research in this area. We describe evaluation setup with TweetNERD for three NERD tasks: Named Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End Entity Linking (End2End); and provide performance of existing publicly available methods on specific TweetNERD splits.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=5mi-CkvEqj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6617192" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6617192</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6617192" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6617192</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0 (CC-BY-4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="r2BIXzAd1GE" data-number="351">
        <h4>
          <a href="/forum?id=r2BIXzAd1GE">
              The Sandbox Environment for Generalizable Agent Research (SEGAR)
          </a>


            <a href="/pdf?id=r2BIXzAd1GE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~R_Devon_Hjelm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~R_Devon_Hjelm1">R Devon Hjelm</a>, <a href="/profile?id=~Bogdan_Mazoure1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bogdan_Mazoure1">Bogdan Mazoure</a>, <a href="/profile?id=~Florian_Golemo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Golemo1">Florian Golemo</a>, <a href="/profile?id=~Felipe_Vieira_Frujeri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felipe_Vieira_Frujeri1">Felipe Vieira Frujeri</a>, <a href="/profile?id=~Pedro_Braga1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Braga1">Pedro Braga</a>, <a href="/profile?id=~Mihai_Jalobeanu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihai_Jalobeanu1">Mihai Jalobeanu</a>, <a href="/profile?id=~Samira_Ebrahimi_Kahou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samira_Ebrahimi_Kahou1">Samira Ebrahimi Kahou</a>, <a href="/profile?id=~Andrey_Kolobov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Kolobov1">Andrey Kolobov</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#r2BIXzAd1GE-details-791" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r2BIXzAd1GE-details-791"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, representation learning, generalization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Flexible sandbox environment for advancing research on generalizable reinforcement learning agents.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A broad challenge of research on generalization in sequential decision-making is designing benchmarks that clearly measure progress. While there has been notable headway, most popular generalization benchmarks, such as Meta-World and Procgen, consist of a fixed set of tasks. They don't give an experimenter easy access to the underlying factors of the environment to allow designing custom source and target task distributions for evaluating particular aspects of generalization. Other benchmarks, such as CausalWorld, are more extensible but are also computationally expensive to run. We built the Sandbox Environment for Generalizable Agent Research (SEGAR) with all of these considerations in mind. In a nutshell, SEGAR is a toolkit for defining generalization objectives over computationally lightweight sequential decision-making tasks and measuring learning algorithms' performance w.r.t. these objectives. In addition to providing several task distributions out of the box, SEGAR enables fine-grained RL generalization experiments by giving researchers tools for creating families of tasks relevant for testing specific hypotheses. We present an overview of SEGAR, explain how it contributes to generalization research, and conduct experiments illustrating a few types of research questions SEGAR can help answer. SEGAR can be found
        https://github.com/microsoft/segar and is open-sourced under an MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=r2BIXzAd1GE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/microsoft/segar" target="_blank" rel="nofollow noreferrer">https://github.com/microsoft/segar</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) Microsoft Corporation and Mila - Quebec AI Institute

        Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="dh-WDeS9RZi" data-number="350">
        <h4>
          <a href="/forum?id=dh-WDeS9RZi">
              StarCraftSensor: An Accessible and Interpretable Sensor Network Benchmark Dataset
          </a>


            <a href="/pdf?id=dh-WDeS9RZi" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sean_Kulinski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sean_Kulinski1">Sean Kulinski</a>, <a href="/profile?id=~Nicholas_R_Waytowich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_R_Waytowich1">Nicholas R Waytowich</a>, <a href="/profile?id=~James_Zachary_Hare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Zachary_Hare1">James Zachary Hare</a>, <a href="/profile?id=~David_I._Inouye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_I._Inouye1">David I. Inouye</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#dh-WDeS9RZi-details-362" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dh-WDeS9RZi-details-362"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While applying machine learning (ML) to sensor network (SN) tasks (e.g., event prediction, target identification, or missing value imputation) is a burgeoning area of research, the systematic progress of the field is limited by the availability of SN datasets. Existing SN datasets have been released for specific SN applications, but often they are small, noisy, uninterpretable or unlabeled, or they require much preprocessing to be compatible with ML tools. Therefore, the ML community lacks an easy-to-use benchmark dataset for prototyping new SN tasks and algorithms. To fill this gap, we construct a benchmark SN dataset based on StarCraft II replays that is as easy to use as MNIST and CIFAR10 while still enabling SN-specific tasks. Specifically, we carefully summarize a window of 255 game states to create 1.8 million summary images from 30,000 replays including all relevant metadata such as game outcome and player races. Each window can be represented as an image and thus can be processed with standard computer vision tools. We develop three versions of decreasing complexity: Hyperspectral images that include one channel for every unit type, RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We also map many SN tasks to concrete ML tasks that can be easily prototyped using our SN datasets and provide several demonstrations of these SN tasks. All datasets, code for extraction, and code for dataset loading will be publicly released to enable extensions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=dh-WDeS9RZi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/starcraft-dataset-extraction-private-078D/" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/starcraft-dataset-extraction-private-078D/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://figshare.com/s/b56ef1c8cc8c87e9115f" target="_blank" rel="nofollow noreferrer">https://figshare.com/s/b56ef1c8cc8c87e9115f</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We will publicly release the final datasets on Zenodo with a permissive CC BY 4.0 license (https://creativecommons.org/licenses/by/4.0). We will release the StarCraft II dataset extraction code and the relevant data loaders and modules for using the data as a Python package with an MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xQL84PlElx" data-number="349">
        <h4>
          <a href="/forum?id=xQL84PlElx">
              A New Aligned Simple German Corpus
          </a>


            <a href="/pdf?id=xQL84PlElx" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Vanessa_Toborek2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vanessa_Toborek2">Vanessa Toborek</a>, <a href="/profile?email=busch%40uni-bonn.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="busch@uni-bonn.de">Moritz Busch</a>, <a href="/profile?id=~Malte_Bo%C3%9Fert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Malte_Boßert1">Malte Boßert</a>, <a href="/profile?id=~Christian_Bauckhage1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Bauckhage1">Christian Bauckhage</a>, <a href="/profile?id=~Pascal_Welke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pascal_Welke1">Pascal Welke</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#xQL84PlElx-details-930" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xQL84PlElx-details-930"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">text simplification, sentence alignment, dataset, nlp</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a new article- and sentence-aligned dataset for German and Simple German.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">"Leichte Sprache", the German pendant to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German -- German. It contains multiple document-aligned sources which we have aligned using automatic sentence-alignment methods. We evaluate our alignments based on a manually labelled subset of aligned documents. The quality of our sentence alignments surpasses previous work, which is outdated and reports low F1-scores. We publish the dataset under CC BY-SA and the accompanying code under MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=xQL84PlElx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/buschmo/Simple-German-Corpus" target="_blank" rel="nofollow noreferrer">https://github.com/buschmo/Simple-German-Corpus</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uf7cPWpaXda" data-number="348">
        <h4>
          <a href="/forum?id=uf7cPWpaXda">
              A Surrogate Model Benchmark for Dynamic Climate Impact Models
          </a>


            <a href="/pdf?id=uf7cPWpaXda" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Julian_Kuehnert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julian_Kuehnert1">Julian Kuehnert</a>, <a href="/profile?id=~Deborah_Fairbanks_McGlynn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deborah_Fairbanks_McGlynn1">Deborah Fairbanks McGlynn</a>, <a href="/profile?id=~Sekou_Remy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sekou_Remy1">Sekou Remy</a>, <a href="/profile?id=~Anne_Jones2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anne_Jones2">Anne Jones</a>, <a href="/profile?id=~Aisha_Walcott-Bryant1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aisha_Walcott-Bryant1">Aisha Walcott-Bryant</a>, <a href="/profile?id=~Michiaki_Tatsubori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michiaki_Tatsubori1">Michiaki Tatsubori</a>, <a href="/profile?id=~Takao_Moriyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Takao_Moriyama1">Takao Moriyama</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#uf7cPWpaXda-details-531" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uf7cPWpaXda-details-531"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">seasonal forecast ensembles, climate impact models, ML surrogate modeling, uncertainty quantification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As acute climate change impacts weather and climate variability, there is an increasing need for robust climate impact model predictions that account for this variability and project it onto a range of potential climate hazard scenarios. The range of possible climate variables which form the input of climate impact models is typically represented by ensemble forecasts that capture the inherent uncertainty of weather models. To project the uncertainty associated with the distribution of input climate forecasts onto impact model output scenarios, each forecast ensemble member must be propagated through the physical model. In the case of complex impact models, this process is computationally expensive. It is therefore desirable to train an ML surrogate model to predict ensembles of climate hazard scenarios under reduced computational costs. To enable benchmarking for dynamic climate impact models, we release a dataset of seasonal weather forecasts, comprising 50 ensemble members of temperature and precipitation forecasts of 6-month horizon, spanning a period of 5 years for a single location in Nairobi, Kenya. This dataset is accompanied by a climate driven disease model, the Liverpool Malaria Model (LMM), which predicts the malaria transmission coefficient $R_0$.  Two types of uncertainty-aware surrogate models are provided as reference implementations, namely a Random Forest Quantile Regression (RFQR) model and a Bayesian Long Short-Term Memory (BLSTM) neural network. The BLSTM is found to predict time series of individual ensemble members with higher accuracy and precision compared to the RFQR. By using the predicted confidence in each ensemble member, we can account for the uncertainty of previously unobserved weather conditions when assessing a combined hazard metric. This is shown by proposing a dynamic Conditional Value at Risk (CVaR) metric function.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uf7cPWpaXda&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/IBM/UQ-climate-impact-benchmarking" target="_blank" rel="nofollow noreferrer">https://github.com/IBM/UQ-climate-impact-benchmarking</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset is released as open source under following DOI:
        https://doi.org/10.5281/zenodo.6647320
        The associated GitHub working repository can be accessed through following link:
        https://github.com/IBM/UQ-climate-impact-benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">DATASET LICENSE:
            Dataset created by The Weather Company, an IBM business. This service is
            based on data and products of the European Center for Medium-range Weather
            Forecasts (ECMWF-Archive and ECMWF-RT). Generated using Copernicus Climate
            Change Service information [2019 and ongoing]. ECMWF Archive data published
            under a Creative Commons Attribution 4.0 International (CC BY 4.0):
            https://creativecommons.org/licenses/by/4.0/
            Disclaimer: Neither the European Commission nor ECMWF is responsible for any
            use that may be made of the information it contains.

        CODE LICENSE:
                                         Apache License
                                   Version 2.0, January 2004
                                http://www.apache.org/licenses/</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mIqsHtCeTQo" data-number="347">
        <h4>
          <a href="/forum?id=mIqsHtCeTQo">
              Shifts 2.0: Extending The Dataset of Real Distributional Shifts
          </a>


            <a href="/pdf?id=mIqsHtCeTQo" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andrey_Malinin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Malinin1">Andrey Malinin</a>, <a href="/profile?id=~andreas_athanasopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~andreas_athanasopoulos1">andreas athanasopoulos</a>, <a href="/profile?id=~Muhamed_Barakovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhamed_Barakovic1">Muhamed Barakovic</a>, <a href="/profile?id=~Meritxell_Bach_Cuadra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meritxell_Bach_Cuadra1">Meritxell Bach Cuadra</a>, <a href="/profile?id=~Mark_Gales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Gales1">Mark Gales</a>, <a href="/profile?id=~Cristina_Granziera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cristina_Granziera1">Cristina Granziera</a>, <a href="/profile?id=~Mara_Graziani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mara_Graziani1">Mara Graziani</a>, <a href="/profile?id=~Nikolay_Kartashev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikolay_Kartashev1">Nikolay Kartashev</a>, <a href="/profile?id=~Konstantinos_Kyriakopoulos2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantinos_Kyriakopoulos2">Konstantinos Kyriakopoulos</a>, <a href="/profile?email=p.lu%40unibas.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="p.lu@unibas.ch">Po-Jui Lu</a>, <a href="/profile?email=nataliia.molchanova%40chuv.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="nataliia.molchanova@chuv.ch">Nataliia Molchanova</a>, <a href="/profile?id=~Antonis_Nikitakis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonis_Nikitakis1">Antonis Nikitakis</a>, <a href="/profile?id=~Vatsal_Raina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vatsal_Raina1">Vatsal Raina</a>, <a href="/profile?id=~Francesco_La_Rosa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_La_Rosa1">Francesco La Rosa</a>, <a href="/profile?email=e.sivena%40deepsea.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="e.sivena@deepsea.ai">Eli Sivena</a>, <a href="/profile?email=v.tsarsitalidis%40deepsea.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="v.tsarsitalidis@deepsea.ai">Vasileios Tsarsitalidis</a>, <a href="/profile?id=~Efi_Tsompopoulou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Efi_Tsompopoulou1">Efi Tsompopoulou</a>, <a href="/profile?id=~Elena_Volf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elena_Volf1">Elena Volf</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#mIqsHtCeTQo-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mIqsHtCeTQo-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Distributional Shift, Uncertainty, Robustness, Datasets and Benchmarks, Multiple Sclerosis, 3D MRI Lesion segmentation, Marine Logistics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We extend the Shifts Benchmark on uncertainty and robustness with two new datasets: a medical multiple sclerosis lesion segmentation datasets and a cargo-vessel power estimation dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distributional shift, or the mismatch between training and deployment data, is a significant obstacle to usage of the machine learning in high-stakes industrial applications, such as autonomous driving and medicine. This creates a need to be able to assess how robustly ML models generalize as well as the quality of their uncertainty estimates. Standard ML baseline datasets do not allow these properties to be assessed, as the training, validation and test data are often identically distributed. Recently, a range of dedicated benchmarks have appeared, featuring both distributionally matched and shifted data. Among these benchmarks, the Shifts dataset stands out in terms of the diversity of tasks as well as the data modalities it features. While most of the benchmarks are heavily dominated by 2D image classification tasks, Shifts contains tabular weather forecasting, machine translation, and vehicle motion prediction tasks. This enables the robustness properties of models to be assessed on a diverse set of industrial-scale tasks and either universal or directly applicable task-specific conclusions to be reached. In this paper, we extend the Shifts Dataset with two datasets sourced from industrial, high-risk applications  of high societal importance. Specifically, we consider the tasks of segmentation of white matter Multiple Sclerosis lesions in 3D magnetic resonance brain images and the estimation of power consumption in marine cargo vessels. Both tasks feature ubiquitous distributional shifts and a strict safety requirement due to the high cost of errors. These new datasets will allow researchers to further explore robust generalization and uncertainty estimation in new situations. In this work, we provide a description of the dataset and baseline results for both tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mIqsHtCeTQo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The datasets will be made publicly available at the end of June 2022, when we launch the Shifts Challenge 2.0. The full datasets and full public access to both evaluation leader boards will be made when the competition finishes at the end of November 2022. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The cargo vessel power estimation dataset is shared under a CC BY NC SA 4.0 license.

        With the medical datasets, the situation with licenses is more complex. Part of the data has already been released under a Creative Commons license (PubMRI, ISBI), and part (MSSEG-1) is available only through credentialized access on the Shanoir Platform (https://project.inria.fr/shanoir/). We are currently in the middle of lengthy negotiations with OFSEP, the owners of the MSSEG-1 data, to allow hosting on zenodo (https://zenodo.org), which would enable significantly faster turnaround times for people to get access. Our goal is to made all of the medical data as freely available as is possible, while maintaining patient confidentiality.

        Finally, the medical evaluation set collected in Lausanne was collected in such a way that makes publicly sharing the data itself very difficult (due to patient confidentiality and the ability of patients to retract their data at any time). However, the data owners are happy to allow anyone to evaluate on this data on a permanent leaderboard via dockers.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jbdp9m7nr0R" data-number="346">
        <h4>
          <a href="/forum?id=jbdp9m7nr0R">
              How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios
          </a>


            <a href="/pdf?id=jbdp9m7nr0R" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mantas_Mazeika3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mantas_Mazeika3">Mantas Mazeika</a>, <a href="/profile?id=~Eric_Tang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Tang2">Eric Tang</a>, <a href="/profile?id=~Andy_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andy_Zou1">Andy Zou</a>, <a href="/profile?id=~Steven_Basart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Basart1">Steven Basart</a>, <a href="/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>, <a href="/profile?id=~David_Forsyth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Forsyth1">David Forsyth</a>, <a href="/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>, <a href="/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#jbdp9m7nr0R-details-825" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jbdp9m7nr0R-details-825"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">video, emotions, preference learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce two large-scale video datasets for predicting how videos would the emotional state and wellbeing of viewers.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for subjective wellbeing and emotional response. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables a continuous spectrum of wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. In experiments, we show how video models that are largely trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jbdp9m7nr0R&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ZqntJDzFd3n" data-number="345">
        <h4>
          <a href="/forum?id=ZqntJDzFd3n">
              AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing
          </a>


            <a href="/pdf?id=ZqntJDzFd3n" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Matei_Bejan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matei_Bejan1">Matei Bejan</a>, <a href="/profile?id=~Andrei_Manolache1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrei_Manolache1">Andrei Manolache</a>, <a href="/profile?id=~Marius_Popescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Popescu1">Marius Popescu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#ZqntJDzFd3n-details-401" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZqntJDzFd3n-details-401"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Anomaly Detection, Benchmark, Datasets, Natural Language Processing</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning models have reignited the interest in Anomaly Detection research in recent years. Methods for Anomaly Detection in text have shown strong empirical results on ad-hoc anomaly setups that are usually made by downsampling some classes of a labeled dataset. This can lead to reproducibility issues and models that are biased to detecting particular anomalies while failing to recognize them in more sophisticated scenarios. In the present work, we provide an unified benchmark for detecting various types of anomalies, focusing on problems that can be naturally formulated as Anomaly Detection in text, ranging from syntax to stylistics. In this way, we are hoping to facilitate research in Text Anomaly Detection. We also evaluate and analyze two strong shallow baselines, as well as two of the current state-of-the-art neural approaches, providing insights into the knowledge the neural models are learning when performing the anomaly detection task. We provide the code for evaluation, downloading, and preprocessing the dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ZqntJDzFd3n&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">http://qwone.com/~jason/20Newsgroups/
        http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html
        https://nyu-mll.github.io/CoLA/
        http://www.vismet.org/metcor/documentation/home.html
        https://www.kaggle.com/datasets/mateibejan/15000-gutenberg-books/
        https://www.kaggle.com/datasets/mateibejan/multilingual-lyrics-for-genre-classification/
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0 for all newly-introduced datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bKO6BPtYQA7" data-number="344">
        <h4>
          <a href="/forum?id=bKO6BPtYQA7">
              Change Event Dataset for Discovery from Spatio-temporal Remote Sensing Imagery
          </a>


            <a href="/pdf?id=bKO6BPtYQA7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Utkarsh_Mall1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Utkarsh_Mall1">Utkarsh Mall</a>, <a href="/profile?id=~Bharath_Hariharan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bharath_Hariharan3">Bharath Hariharan</a>, <a href="/profile?id=~Kavita_Bala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kavita_Bala1">Kavita Bala</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#bKO6BPtYQA7-details-566" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bKO6BPtYQA7-details-566"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Satellite imagery is increasingly available, high resolution, and temporally detailed.  Changes in spatio-temporal datasets such as satellite images are particularly interesting as they reveal the many events and forces that shape our world.  However, finding such interesting and meaningful change events from the vast data is challenging.  In this paper, we present new datasets for such change events that include semantically meaningful events like road construction.  Instead of manually annotating the very large corpus of satellite images, we introduce a novel unsupervised approach that takes a large spatio-temporal dataset from satellite images and finds interesting change events.  To evaluate the meaningfulness on these datasets we create 2 benchmarks namely CaiRoad and CalFire which capture the events of road construction and forest fires.  These new benchmarks can be used to evaluate semantic retrieval/classification performance.  We explore these benchmarks qualitatively and quantitatively by using several methods and show that these new datasets are indeed challenging for many existing methods.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bKO6BPtYQA7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.cs.cornell.edu/projects/satellite-change-events/" target="_blank" rel="nofollow noreferrer">https://www.cs.cornell.edu/projects/satellite-change-events/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://www.cs.cornell.edu/projects/satellite-change-events/

        Please look at the Readme.txt on the landing page to get more information on the dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-NonCommercial 4.0 International License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Proso5bUa" data-number="343">
        <h4>
          <a href="/forum?id=Proso5bUa">
              Flare7K: A Phenomenological Nighttime Flare Removal Dataset
          </a>


            <a href="/pdf?id=Proso5bUa" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuekun_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuekun_Dai1">Yuekun Dai</a>, <a href="/profile?id=~Chongyi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongyi_Li1">Chongyi Li</a>, <a href="/profile?id=~Shangchen_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shangchen_Zhou1">Shangchen Zhou</a>, <a href="/profile?id=~Ruicheng_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruicheng_Feng1">Ruicheng Feng</a>, <a href="/profile?id=~Chen_Change_Loy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Change_Loy2">Chen Change Loy</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Proso5bUa-details-479" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Proso5bUa-details-479"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">flare removal, glare, image restoration, nighttime driving, computational photography</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We design a new phenomenological synthetic flare dataset to help us remove the lens flare artifact at night.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Artificial lights commonly leave strong lens flare artifacts on the images when one captures images at night. Nighttime flare not only affects the visual quality but also degrades the performance of vision algorithms. Different from sunshine flare, the nighttime flare has its unique luminance and spectrum of artificial lights and diverse patterns. Models trained on existing sunshine flare removal datasets, however, cannot cope with nighttime flare. Existing flare removal methods mainly focus on the removal of daytime flares while they fail in removing nighttime flares. Nighttime flare removal is challenging because of the unique luminance and spectrum of artificial lights and the diverse patterns and image degradation of the flares captured at night. The scarcity of the nighttime flare removal dataset limits the research on this paramount task. In this paper, we introduce, Flare7K,  the first nighttime flare removal dataset, which is generated based on the observation and statistic of real-world nighttime lens flares. It offers 5,000 scattering flare images and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to the flare-free images, forming the flare-corrupted and flare-free image pairs. With the paired data, deep models can effectively restore the flare-corrupted images taken in real world. Apart from sufficient flare patterns, we also provide rich annotations, including the light source, glare with shimmer, reflective flare, and streak, which are frequently absent from existing datasets. Thus, our dataset can facilitate new work in nighttime flare removal and more. Extensive experiments demonstrate that our dataset can complement the diversity of existing flare datasets and push the frontier of nighttime flare removal. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Proso5bUa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://nukaliad.github.io/projects/Flare7K" target="_blank" rel="nofollow noreferrer">https://nukaliad.github.io/projects/Flare7K</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">You can obtain the data from: https://drive.google.com/file/d/1PPXWxn7gYvqwHX301SuWmjI7IUUtqxab/view?usp=sharing

        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is licensed under CC 4.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="W-VV1r1SBrL" data-number="338">
        <h4>
          <a href="/forum?id=W-VV1r1SBrL">
              Social B(eye)as over Time: Dataset for Temporal Auditing of Image Tagging Algorithms
          </a>


            <a href="/pdf?id=W-VV1r1SBrL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Pinar_Barlas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pinar_Barlas1">Pinar Barlas</a>, <a href="/profile?id=~Maximilian_Krahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Krahn1">Maximilian Krahn</a>, <a href="/profile?email=s.kleanthous%40cyens.org.cy" class="profile-link" data-toggle="tooltip" data-placement="top" title="s.kleanthous@cyens.org.cy">Styliani Kleanthous</a>, <a href="/profile?id=~Kyriakos_Kyriakou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyriakos_Kyriakou1">Kyriakos Kyriakou</a>, <a href="/profile?email=j.otterbacher%40cyens.org.cy" class="profile-link" data-toggle="tooltip" data-placement="top" title="j.otterbacher@cyens.org.cy">Jahna Otterbacher</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#W-VV1r1SBrL-details-233" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W-VV1r1SBrL-details-233"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">temporal audits, image tagging algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Replication of an audit after 3 years demonstrating the importance of temporal auditing.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A key challenge when working with proprietary AI services is that their models -- and thus, behaviors -- can change over time in subtle ways, sometimes without notice. In 2018, we audited six image tagging algorithms for their treatment of different social groups by using standardized portrait images as input. In 2021, we replicated this process with the same images, and collected outputs for additional images depicting other racial/ethnic groups. We include both sets of data (2018 &amp; 2021) in our openly-accessible dataset, and conduct some analyses to establish the scope of the temporal changes and capacity for further analysis. We demonstrate the need for temporal auditing of proprietary computer vision algorithms and datasets produced using such algorithms. As the algorithms and training datasets change, the temporal audit can capture and document those changes. Beyond the current analyses, the data holds a lot of potential for further exploration, particularly regarding the treatment of different social groups by image tagging algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=W-VV1r1SBrL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.7910/DVN/CFHZS3" target="_blank" rel="nofollow noreferrer">https://doi.org/10.7910/DVN/CFHZS3</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.7910/DVN/CFHZS3" target="_blank" rel="nofollow noreferrer">https://doi.org/10.7910/DVN/CFHZS3</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons CC0 1.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fXKMkUF_GAj" data-number="337">
        <h4>
          <a href="/forum?id=fXKMkUF_GAj">
              Bagel: A Benchmark for Assessing Graph Neural Network Explanations
          </a>


            <a href="/pdf?id=fXKMkUF_GAj" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mandeep_Rathee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mandeep_Rathee1">Mandeep Rathee</a>, <a href="/profile?id=~Thorben_Funke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thorben_Funke1">Thorben Funke</a>, <a href="/profile?id=~Avishek_Anand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Avishek_Anand1">Avishek Anand</a>, <a href="/profile?id=~Megha_Khosla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Megha_Khosla1">Megha Khosla</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#fXKMkUF_GAj-details-393" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fXKMkUF_GAj-details-393"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Benchmark, GNNs, Explainibility</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The problem of interpreting the decisions of machine learning is well-researched and important. We are interested in a specific type of machine learning model that deals with graph data called graph neural networks.
        Evaluating interpretability approaches for graph neural networks (GNN) specifically are known to be challenging due to the lack of a commonly accepted benchmark. Given a GNN model, several interpretability approaches exist to explain GNN models with diverse (sometimes conflicting) evaluation methodologies.  In this paper, we propose a benchmark for evaluating the explainability approaches for GNNs called Bagel. In Bagel, we firstly propose four diverse GNN explanation evaluation regimes -- 1) faithfulness, 2) sparsity, 3) correctness. and 4) plausibility. We reconcile multiple evaluation metrics in the existing literature and cover diverse notions for a holistic evaluation. Our graph datasets range from citation networks, document graphs, to graphs from molecules and proteins. We conduct an extensive empirical study on four GNN models and nine post-hoc explanation approaches for node and graph classification tasks. We open both the benchmarks and reference implementations and make them available at https://github.com/Mandeep-Rathee/Bagel-benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fXKMkUF_GAj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Mandeep-Rathee/Bagel-benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/Mandeep-Rathee/Bagel-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rjBYortWdRV" data-number="336">
        <h4>
          <a href="/forum?id=rjBYortWdRV">
              Benchmarking and Analyzing 3D Human Pose and Shape Estimation Beyond Algorithms
          </a>


            <a href="/pdf?id=rjBYortWdRV" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hui_En_Pang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hui_En_Pang1">Hui En Pang</a>, <a href="/profile?id=~Zhongang_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongang_Cai1">Zhongang Cai</a>, <a href="/profile?id=~Lei_Yang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Yang7">Lei Yang</a>, <a href="/profile?id=~Tianwei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianwei_Zhang1">Tianwei Zhang</a>, <a href="/profile?id=~Ziwei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Liu1">Ziwei Liu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#rjBYortWdRV-details-261" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rjBYortWdRV-details-261"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Human Body Reconstruction, SMPL Model, 2D and 3D Pose, Pose and Shape Estimation, Human Mesh Recovery</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Benchmarking different datasets, backbones and training strategies for 3D human pose and shape estimation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">3D human pose and shape estimation (a.k.a. ``human mesh recovery'') has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the \textit{first} comprehensive benchmarking study from three under-explored perspectives beyond algorithms. \emph{1) Datasets.} An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (\emph{i.e.} diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance. \emph{2) Backbones.} Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery. \emph{3) Training strategies.} Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 \(mm\) on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at \url{https://github.com/smplbody/hmr-benchmarks}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rjBYortWdRV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/smplbody/hmr-benchmarks" target="_blank" rel="nofollow noreferrer">https://github.com/smplbody/hmr-benchmarks</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="W_bsDmzwaZ7" data-number="335">
        <h4>
          <a href="/forum?id=W_bsDmzwaZ7">
              K-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous Driving in Various Weather Conditions
          </a>


            <a href="/pdf?id=W_bsDmzwaZ7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dong-Hee_Paek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong-Hee_Paek1">Dong-Hee Paek</a>, <a href="/profile?id=~Seung-Hyun_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seung-Hyun_Kong1">Seung-Hyun Kong</a>, <a href="/profile?id=~Kevin_Tirta_Wijaya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Tirta_Wijaya1">Kevin Tirta Wijaya</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#W_bsDmzwaZ7-details-618" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W_bsDmzwaZ7-details-618"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">4D Radar, 4D Radar tensor, 3D object detection, Adverse weather, Autonomous driving</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unlike RGB cameras that use visible light bands (384∼769 THz) and Lidar that use infrared bands (361∼331 THz), Radars use relatively longer wavelength radio bands (77∼81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at https://github.com/kaist-avelab/k-radar.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=W_bsDmzwaZ7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/kaist-avelab/k-radar" target="_blank" rel="nofollow noreferrer">https://github.com/kaist-avelab/k-radar</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset Url: https://github.com/kaist-avelab/k-radar
        Supplementary video clips Urls
        (1) Sensor Measurements Dynamically Changing during Driving under the Heavy Snow Condition:
        https://www.youtube.com/watch?v=XWAi71AUo5A&amp;t=1s
        (2) 4D Radar Tensor &amp; Lidar Point Cloud Calibration and Annotation Process
        https://www.youtube.com/watch?v=DD3Iks8sB3I
        (3) Annotation Process in the Absence of Lidar Point Cloud Measurements of Objects
        https://www.youtube.com/watch?v=KyDI32Uck4g&amp;t=1s
        (4) 4D Radar Tensor &amp; Lidar Point Cloud Calibration Results
        https://www.youtube.com/watch?v=q0scC3KZlyI
        (5) GUI-based Program for Visualization and Neural Network Inference
        https://www.youtube.com/watch?v=b_9TtOxaN1w</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We will open a project page promptly, but the dataset will be opened around September 2022.
        Since our dataset requires a large amount of memory (~13TB), we expect it will take about three months to set up the server.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The K-Radar dataset is published under the CC BY-NC-ND License, and all codes are published under the Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>