<ul class="list-unstyled submissions-list">
    <li class="note " data-id="4W2UaAZBKK" data-number="161">
        <h4>
          <a href="/forum?id=4W2UaAZBKK">
              Open-vocabulary Attribute Detection
          </a>


            <a href="/pdf?id=4W2UaAZBKK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Maria_Alejandra_Bravo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Alejandra_Bravo1">Maria Alejandra Bravo</a>, <a href="/profile?id=~Sudhanshu_Mittal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sudhanshu_Mittal2">Sudhanshu Mittal</a>, <a href="/profile?id=~Simon_Ging1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Ging1">Simon Ging</a>, <a href="/profile?id=~Thomas_Brox1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Brox1">Thomas Brox</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#4W2UaAZBKK-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4W2UaAZBKK-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">open-vocabulary, weakly-supervised, vision and languange, attribute detection, object detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce the Open-vocabulary Attribute Detection task, a new way to learn and predict attributes of detected objects and propose the Object-vocabulary Attribute benchmark, which is a clean and densely annotated attribute evaluation benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> In this paper, we introduce the Open-vocabulary Attribute Detection task, a new way to learn and predict attributes of detected objects.  We leverage vision-language modeling to enable open-vocabulary prediction, where predictions are not bound to a defined vocabulary. Only via a benchmark, we define a target vocabulary, which could be extended at any time. The open-vocabulary setting helps in circumventing the limitation of large annotation costs. To study this problem, we propose the Object-vocabulary Attribute benchmark, which is a clean and densely annotated attribute evaluation benchmark. The proposed benchmark defines 121 attribute classes, including positive and negative attributes, with over 8.6K object instances manually annotated, resulting in a total of 899K attribute annotations. The defined attributes focus on visual adjectives directly attached to the detected objects. We provide a well-performing baseline method for open-vocabulary attribute detection.  The introduced task and benchmark would enable the study of fine-grained attribute detection methods in a systematic manner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=4W2UaAZBKK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="VtEEpi-dGlt" data-number="160">
        <h4>
          <a href="/forum?id=VtEEpi-dGlt">
              Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?
          </a>


            <a href="/pdf?id=VtEEpi-dGlt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Alexander_Korotin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Korotin2">Alexander Korotin</a>, <a href="/profile?id=~Alexander_Kolesov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Kolesov1">Alexander Kolesov</a>, <a href="/profile?id=~Evgeny_Burnaev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evgeny_Burnaev1">Evgeny Burnaev</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#VtEEpi-dGlt-details-698" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VtEEpi-dGlt-details-698"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">wasserstein gans, optimal transport, neural dual solvers, ray monotone transport plans</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=VtEEpi-dGlt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/justkolesov/Wasserstein1Benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/justkolesov/Wasserstein1Benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/justkolesov/Wasserstein1Benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/justkolesov/Wasserstein1Benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT, see the provided link.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mJWt6pOcHNy" data-number="159">
        <h4>
          <a href="/forum?id=mJWt6pOcHNy">
              Breaking Bad: A Dataset for Geometric Fracture and Reassembly
          </a>


            <a href="/pdf?id=mJWt6pOcHNy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Silvia_Sell%C3%A1n1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Silvia_Sellán1">Silvia Sellán</a>, <a href="/profile?id=~Yun-Chun_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun-Chun_Chen1">Yun-Chun Chen</a>, <a href="/profile?id=~Ziyi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyi_Wu1">Ziyi Wu</a>, <a href="/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>, <a href="/profile?id=~Alec_Jacobson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alec_Jacobson1">Alec Jacobson</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mJWt6pOcHNy-details-305" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mJWt6pOcHNy-details-305"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Breaking Bad, a large-scale dataset of fractured objects. Our dataset consists of over one million fractured objects simulated from ten thousand base models. The fracture simulation is powered by a recent physically based algorithm that efficiently generates a variety of fracture modes of an object. Existing shape assembly datasets decompose objects according to semantically meaningful parts, effectively modeling the construction process. In contrast, Breaking Bad models the destruction process of how a geometric object naturally breaks into fragments. Our dataset serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. We analyze our dataset with several geometry measurements and benchmark three state-of-the-art shape assembly deep learning methods under various settings. Extensive experimental results demonstrate the difficulty of our dataset, calling on future research in model designs specifically for the geometric shape assembly task. We host our dataset at https://breaking-bad-dataset.github.io/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mJWt6pOcHNy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://breaking-bad-dataset.github.io/" target="_blank" rel="nofollow noreferrer">https://breaking-bad-dataset.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">URL: https://breaking-bad-dataset.github.io/
        Please see the instructions on the website.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code: MIT license.
        Dataset: We gather our base models following the licenses specified in each of the source datasets: the MIT license in the PartNet dataset and a variety of open-source licenses in the Thingi10K dataset (see Figure 12 in [65]). We release each model in our dataset with an as-permissive-as-possible license compatible with its underlying base model.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="XU-QYsx40E_" data-number="158">
        <h4>
          <a href="/forum?id=XU-QYsx40E_">
              The ArtBench Dataset: Benchmarking Generative Models with Artworks
          </a>


            <a href="/pdf?id=XU-QYsx40E_" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peiyuan_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peiyuan_Liao1">Peiyuan Liao</a>, <a href="/profile?id=~Xiuyu_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiuyu_Li1">Xiuyu Li</a>, <a href="/profile?id=~Xihui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xihui_Liu1">Xihui Liu</a>, <a href="/profile?id=~Kurt_Keutzer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kurt_Keutzer1">Kurt Keutzer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#XU-QYsx40E_-details-430" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XU-QYsx40E_-details-430"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">generative models, image generation, ML datasets and benchmark, computer vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking generative models for artworks generation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation. It comprises 60,000 images of artwork from 10 distinctive artistic styles, with 5,000 training images and 1,000 testing images per style. ArtBench-10 has several advantages over previous artwork datasets. Firstly, it is class-balanced while most previous artwork datasets suffer from the long tail class distributions. Secondly, the images are of high quality with clean annotations. Thirdly, ArtBench-10 is created with standardized data collection, annotation, filtering, and preprocessing procedures. We provide three versions of the dataset with different resolutions (32 x 32, 256 x 256, and original image size), formatted in a way that is easy to be incorporated by popular machine learning frameworks. We also conduct extensive benchmarking experiments using representative image synthesis models with ArtBench-10 and present in-depth analysis. The dataset is available at https://github.com/liaopeiyuan/artbench under a Fair Use license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=XU-QYsx40E_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/liaopeiyuan/artbench" target="_blank" rel="nofollow noreferrer">https://github.com/liaopeiyuan/artbench</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/liaopeiyuan/artbench" target="_blank" rel="nofollow noreferrer">https://github.com/liaopeiyuan/artbench</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Fair Use license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LyeDcV41Xm" data-number="157">
        <h4>
          <a href="/forum?id=LyeDcV41Xm">
              Parse Semantics from Geometry: A Remote Sensing Benchmark for Multi-modal Semantic Segmentation
          </a>


            <a href="/pdf?id=LyeDcV41Xm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhitong_Xiong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhitong_Xiong2">Zhitong Xiong</a>, <a href="/profile?email=sining.chen%40dlr.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="sining.chen@dlr.de">Sining Chen</a>, <a href="/profile?id=~Yi_Wang31" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wang31">Yi Wang</a>, <a href="/profile?id=~Xiao_Xiang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Xiang_Zhu1">Xiao Xiang Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#LyeDcV41Xm-details-212" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LyeDcV41Xm-details-212"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-modal Learning, Semantic Segmentation, Unsupervised Segmentation, Remote Sensing, Earth obserrvation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Geometric information in the normalized digital surface models (nDSM) is highly correlated with the semantic class of the land cover. Exploiting two modalities (RGB and nDSM (height)) jointly has a great potential to improve the segmentation performance. However, it is still an under-explored field in remote sensing due to the following challenges. First, the scales of existing datasets are relatively small. Second, there is a lack of unified benchmarks as existing methods for comparison are with limited standardization. Last, unsupervised multi-modal representation learning has not been deeply explored on remote sensing data. Towards a fair and comprehensive analysis of existing methods, in this paper, we introduce a remote sensing benchmark for multi-modal semantic segmentation based on RGB-Height (RGB-H) data. The proposed benchmark consists of: 1) a large-scale dataset including co-registered RGB and nDSM pairs; 2) a comprehensive evaluation and analysis of six types of multi-modal fusion strategies on remote sensing data; 3) a comparison of more than ten existing models including both the supervised and unsupervised multi-modal learning methods. The designed benchmark can foster future research on developing new methods for multi-modal learning on remote sensing data. Extensive analyses of those methods are conducted and valuable insights are provided through the experimental results. Code for the benchmark and baselines can be accessed at https://github.com/DeepAI4EO.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LyeDcV41Xm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/DeepAI4EO/Dataset4EO" target="_blank" rel="nofollow noreferrer">https://github.com/DeepAI4EO/Dataset4EO</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/DeepAI4EO/Dataset4EO
        https://syncandshare.lrz.de/getlink/fi8rRALX7JwWtMaSH1jpxiVA/RSUSS.zip
        https://mediatum.ub.tum.de/1661568</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="iAxH-ikIP0I" data-number="155">
        <h4>
          <a href="/forum?id=iAxH-ikIP0I">
              TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training
          </a>


            <a href="/pdf?id=iAxH-ikIP0I" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yulong_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulong_Liu3">Yulong Liu</a>, <a href="/profile?id=~Guibo_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guibo_Zhu1">Guibo Zhu</a>, <a href="/profile?id=~Bin_Zhu6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bin_Zhu6">Bin Zhu</a>, <a href="/profile?id=~Qi_Song5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Song5">Qi Song</a>, <a href="/profile?id=~Guojing_Ge2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guojing_Ge2">Guojing Ge</a>, <a href="/profile?id=~Haoran_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoran_Chen1">Haoran Chen</a>, <a href="/profile?id=~GuanHui_Qiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~GuanHui_Qiao1">GuanHui Qiao</a>, <a href="/profile?id=~Ru_Peng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ru_Peng2">Ru Peng</a>, <a href="/profile?id=~Lingxiang_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lingxiang_Wu1">Lingxiang Wu</a>, <a href="/profile?id=~Jinqiao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinqiao_Wang1">Jinqiao Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#iAxH-ikIP0I-details-92" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iAxH-ikIP0I-details-92"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision-Language Pretraining, Multi-modality, Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release a new large-scale dataset for Chinese vision-language pretraining</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision-Language Pre-training (VLP) has been shown to be an efficient method to improve the performance of models on different vision-and-language downstream tasks.  Substantial studies have shown that neural networks may be able to learn some general rules about language and vision conceptions from a large-scale weakly labeled image-text dataset.  However, most of the public cross-modal datasets that contain more than 100M image-text pairs are in English, there is a lack of available large-scale and high-quality Chinese VLP dataset. In this work, we propose a new framework for automatic dataset acquisition and cleaning. And we construct a new large-scale and high-quality image-text pair dataset named as TaiSu, containing 166 million images and 219 million Chinese captions. This framework can  benefit not only from uni-modal filtering methods, but also from cross-modal semantic interaction. Compared with the recently released Wukong dataset, our dataset is achieved with much stricter restrictions on the semantic correlation of image-text pairs. We also propose to combine texts generated by a pre-trained image-captioning model with texts from webpages. To the best of our knowledge, TaiSu is currently the largest publicly accessible Chinese cross-modal dataset. Besides, we test our dataset on several vision-language downstream tasks. Results demonstrate that TaiSu can serve as a promising benchmark for VLP, both for understanding and generation tasks. Especially, on zero-shot image-text retrieval tasks, TaiSu outperforms Wukong by a considerable margin when the model has a similar scale of parameters.  We will make the dataset, pre-trained models and codes public soon.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=iAxH-ikIP0I&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ksOAn6g5/TaiSu" target="_blank" rel="nofollow noreferrer">https://github.com/ksOAn6g5/TaiSu</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/ksOAn6g5/TaiSu" target="_blank" rel="nofollow noreferrer">https://github.com/ksOAn6g5/TaiSu</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Licenses

        Unless specifically labeled otherwise, these Datasets are provided to You under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (“CC BY-NC-SA 4.0”), with the additional terms included herein. The CC BY-NC-SA 4.0 may be accessed at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode. When You download or use the Datasets from the Website or elsewhere, You are agreeing to comply with the terms of CC BY-NC-SA 4.0, and also agreeing to the Dataset Terms. Where these Dataset Terms conflict with the terms of CC BY-NC-SA 4.0, these Dataset Terms shall prevail. We reiterate once again that this dataset is used only for non-commercial purposes such as academic research, teaching, or scientific publications. We prohibits You from using the dataset or any derivative works for commercial purposes, such as selling data or using it for commercial gain.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="31_U7n18gM7" data-number="154">
        <h4>
          <a href="/forum?id=31_U7n18gM7">
              BackdoorBench: A Comprehensive Benchmark of Backdoor Learning
          </a>


            <a href="/pdf?id=31_U7n18gM7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Baoyuan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baoyuan_Wu1">Baoyuan Wu</a>, <a href="/profile?id=~Hongrui_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongrui_Chen1">Hongrui Chen</a>, <a href="/profile?id=~Mingda_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingda_Zhang2">Mingda Zhang</a>, <a href="/profile?id=~Zihao_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zihao_Zhu2">Zihao Zhu</a>, <a href="/profile?id=~Shaokui_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaokui_Wei1">Shaokui Wei</a>, <a href="/profile?id=~Danni_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danni_Yuan1">Danni Yuan</a>, <a href="/profile?id=~Chao_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Shen2">Chao Shen</a>, <a href="/profile?id=~Hongyuan_Zha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongyuan_Zha1">Hongyuan Zha</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#31_U7n18gM7-details-999" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="31_U7n18gM7-details-999"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">8 backdoor attacks; 9 backdoor defenses; 5 poisoning ratios; 5 models; 4 datasets; 8,000 evaluations</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Backdoor learning is an emerging and important topic of studying the vulnerability of deep neural networks (DNNs). Many pioneering backdoor attack and defense methods are being proposed successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and real performance, mainly due to the rapid development, diverse settings, as well as the difficulties of implementation and reproducibility.  Without thorough evaluations and comparisons, it is difficult to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning, called BackdoorBench. It consists of an extensible modular based codebase (currently including implementations of 8 state-of-the-art (SOTA) attack and 9 SOTA defense algorithms), as well as a standardized protocol of a complete backdoor learning. We also provide comprehensive evaluations covering 8 attacks, 9 defenses, 5 models, 4 datasets, 5 poisoning ratios, thus 8000 evaluations in total. We further present analysis from different perspectives about these 8000 evaluations, studying the effects of attack against defense algorithms, poisoning ratio, model, dataset in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at https://backdoorbench.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=31_U7n18gM7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://backdoorbench.github.io" target="_blank" rel="nofollow noreferrer">https://backdoorbench.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The repository of BackdoorBench is licensed by The Chinese University of Hong Kong, Shenzhen and Shenzhen Research Institute of Big Data under Creative Commons Attribution-NonCommercial 4.0 International Public License (identified as CC BY-NC-4.0 in SPDX). More details of the license could be found at https://github.com/SCLBD/BackdoorBench/blob/main/LICENSE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="5CM7Wtalz6x" data-number="153">
        <h4>
          <a href="/forum?id=5CM7Wtalz6x">
              SMART-Rain: A Degradation Evaluation Dataset for Autonomous Driving in Rain
          </a>


            <a href="/pdf?id=5CM7Wtalz6x" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chen_Zhang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Zhang9">Chen Zhang</a>, <a href="/profile?id=~Zefan_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zefan_Huang1">Zefan Huang</a>, <a href="/profile?email=guo_hongliang%40i2r.a-star.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="guo_hongliang@i2r.a-star.edu.sg">Hongliang Guo</a>, <a href="/profile?email=qqinlei%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="qqinlei@gmail.com">Lei Qin</a>, <a href="/profile?id=~Marcelo_H_Ang_Jr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcelo_H_Ang_Jr1">Marcelo H Ang Jr</a>, <a href="/profile?id=~Daniela_Rus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniela_Rus1">Daniela Rus</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#5CM7Wtalz6x-details-335" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5CM7Wtalz6x-details-335"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">degradation evaluation, autonomous driving dataset, rainy weather</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">a dataset to study degradation for autonomous driving in rain</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Autonomous driving in the rain remains a challenge. One main problem is performance degradation caused by rain. This work introduces a new dataset to study this problem. Our dataset is collected from a full-scale vehicle equipped with a 3D LiDAR sensor and multiple forward-facing cameras under various rainy conditions. In addition, rainfall intensity is recorded in real-time from a rain sensor. The combination of sensor and rainfall intensity measurement is designed for studying algorithm performance under different levels of rainfall. In this work, in addition to presenting dataset creation details, we also introduce three degradation evaluation tasks with baseline results, including rainfall intensity estimation, LiDAR degradation estimation, and 2D object detection evaluation. This dataset samples, development kit, and baseline codes are all available in the supplementary material.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=5CM7Wtalz6x&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The SMART-Rain dataset is published under CC BY-NC-SA 4.0 license for non-commercial research purposes only.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="pcBnes02t3u" data-number="152">
        <h4>
          <a href="/forum?id=pcBnes02t3u">
              SMACv2: A New Benchmark for Cooperative Multi-Agent Reinforcement Learning
          </a>


            <a href="/pdf?id=pcBnes02t3u" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Benjamin_Ellis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Ellis1">Benjamin Ellis</a>, <a href="/profile?id=~Skander_Moalla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Skander_Moalla1">Skander Moalla</a>, <a href="/profile?id=~Mikayel_Samvelyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikayel_Samvelyan1">Mikayel Samvelyan</a>, <a href="/profile?id=~Mingfei_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingfei_Sun1">Mingfei Sun</a>, <a href="/profile?id=~Anuj_Mahajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anuj_Mahajan1">Anuj Mahajan</a>, <a href="/profile?id=~Jakob_Nicolaus_Foerster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_Nicolaus_Foerster1">Jakob Nicolaus Foerster</a>, <a href="/profile?id=~Shimon_Whiteson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shimon_Whiteson1">Shimon Whiteson</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#pcBnes02t3u-details-827" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pcBnes02t3u-details-827"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We show that the popular MARL benchmark SMAC lacks stochasticity and propose a new benchmark, SMACv2, to address this.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The availability of challenging benchmarks has played a key role in the recent progress of cooperative multi-agent reinforcement learning (MARL). Among the benchmarks widely adopted by the community, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for the paradigm of centralised training with decentralised execution. However, after several years of sustained improvement, results on this benchmark have started to reach near-perfect performance. In this work, we conduct a new analysis demonstrating that SMAC is not sufficiently stochastic to present a significant challenge to current MARL algorithms. We show that a policy conditioned only on the timestep and agent ID can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark.  In contrast to the original version, scenarios in SMACv2 are procedurally generated and require agents to generalise to previously unseen settings during evaluation.  Moreover, we reduce feature inferrability by making the underlying state information less observable and requiring agents to explore and gather relevant information through an updated action space. We evaluate several state-of-the-art algorithms on SMACv2 and show that it presents a significant challenge in comparison with the original.  Our thorough analysis shows that SMACv2 addresses the discovered deficiencies of SMAC and can be served for the next generation of MARL methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=pcBnes02t3u&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/oxwhirl/smacv2" target="_blank" rel="nofollow noreferrer">https://github.com/oxwhirl/smacv2</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">SMACv2 is released under the MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MpRgTNLazR" data-number="150">
        <h4>
          <a href="/forum?id=MpRgTNLazR">
              Do We Need Another Explainable AI Method? Toward Unifying Post-hoc XAI Evaluation Methods into an Interactive and Multi-dimensional Benchmark
          </a>


            <a href="/pdf?id=MpRgTNLazR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mohamed_Karim_Belaid1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_Karim_Belaid1">Mohamed Karim Belaid</a>, <a href="/profile?id=~Eyke_H%C3%BCllermeier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eyke_Hüllermeier1">Eyke Hüllermeier</a>, <a href="/profile?id=~Maximilian_Rabus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Rabus1">Maximilian Rabus</a>, <a href="/profile?id=~Ralf_Krestel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ralf_Krestel1">Ralf Krestel</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#MpRgTNLazR-details-580" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MpRgTNLazR-details-580"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Post-hoc Explainable AI, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Unified Approach to Evaluate and Compare Explainable AI methods. Targeting laymen, practionners and researchers at the same time.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, Explainable AI (xAI) attracted a lot of attention as various countries turned explanations into a legal right. xAI allows for improving models beyond the accuracy metric by, e.g., debugging the learned pattern and demystifying the AI's behavior. The widespread use of xAI brought new challenges. On the one hand, the number of published xAI algorithms underwent a boom, and it became difficult for practitioners to select the right tool. On the other hand, some experiments did highlight how easy data scientists could misuse xAI algorithms and misinterpret their results. To tackle the issue of comparing and correctly using feature importance xAI algorithms, we propose Compare-xAI, a benchmark that unifies all exclusive and unitary evaluation methods applied to xAI algorithms. We propose a selection protocol to shortlist non-redundant unit tests from the literature, i.e., each targeting a specific problem in explaining a model. The benchmark encapsulates the complexity of evaluating xAI methods into a hierarchical scoring of three levels, namely, targeting three end-user groups: researchers, practitioners, and laymen in xAI. The most detailed level provides one score per unit test. The second level regroups tests into five categories (fidelity, fragility, stability, simplicity, and stress tests). The last level is the aggregated comprehensibility score, which encapsulates the ease of correctly interpreting the algorithm's output in one easy to compare value.
        Compare-xAI's interactive user interface helps mitigate errors in interpreting xAI results by quickly listing the recommended xAI solutions for each ML task and their current limitations.
        The benchmark is made available at https://karim-53.github.io/cxAI/</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://karim-53.github.io/cxAI/" target="_blank" rel="nofollow noreferrer">https://karim-53.github.io/cxAI/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The benchmark can be downloaded from the Github repo:

        https://github.com/Karim-53/Compare-xAI

        Instructions are in the Readme
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0

        https://github.com/Karim-53/Compare-xAI/blob/main/LICENSE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YSSDy3lYiKM" data-number="149">
        <h4>
          <a href="/forum?id=YSSDy3lYiKM">
              Ishihara MNIST: Is Machine Learning Color Blind?
          </a>


            <a href="/pdf?id=YSSDy3lYiKM" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ammar_Shaker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ammar_Shaker1">Ammar Shaker</a>, <a href="/profile?id=~Sascha_Saralajew1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sascha_Saralajew1">Sascha Saralajew</a>, <a href="/profile?id=~Kiril_Gashteovski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiril_Gashteovski1">Kiril Gashteovski</a>, <a href="/profile?id=~Ian_Charles_Faust1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Charles_Faust1">Ian Charles Faust</a>, <a href="/profile?id=~Zhao_Xu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhao_Xu4">Zhao Xu</a>, <a href="/profile?id=~Bhushan_Kotnis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bhushan_Kotnis1">Bhushan Kotnis</a>, <a href="/profile?id=~Wiem_Ben_Rim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wiem_Ben_Rim1">Wiem Ben Rim</a>, <a href="/profile?id=~Carolin_Lawrence1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carolin_Lawrence1">Carolin Lawrence</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YSSDy3lYiKM-details-685" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YSSDy3lYiKM-details-685"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">MNIST, Ishihara test, Colorblindness, Robustness to misleading colors</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a new dataset, Ishihara MNIST, that allows us to explore how robust the models are with respect to challenging color schemes.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances in machine learning (ML) have led to models that achieve impressive accuracy on many datasets. In accordance with this high accuracy, ML models find entrance into more and more real-world applications. Consequently, new requirements for ML models are arising such as requirements for explainability, robustness to input modifications, and the challenge of transferring easily to new tasks. Here we propose a new dataset, Ishihara MNIST, that allows us to explore these questions with regard to \textit{how models perceive colors}. Ishihara MNIST consists of MNIST images that are represented by color perception plates inspired by the Ishihara test for detecting human color deficiencies.
        A series of circles represent both digit foreground and background. These circles can be colored in various ways. Controlling this coloration allows us to study the behavior of machine learning models with regard to a series of interesting properties: (1) How robust is a model with respect to color, for example, is the model colorblind? Can we improve the performance of a particular coloring pattern by first learning from others? (2) Can explanation methods reveal how models perceive color? Are explainable methods robust with respect to color?</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YSSDy3lYiKM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/shaker82/Ishihara_MNIST" target="_blank" rel="nofollow noreferrer">https://github.com/shaker82/Ishihara_MNIST</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Implementation: https://github.com/shaker82/Ishihara_MNIST
        On Kaggle, DOI:10.34740/kaggle/dsv/3808029

        We also offer an interactive tutorial for the simulation and creation of the Ishihara MNIST images on Kaggle: https://www.kaggle.com/code/ammarshaker/transformmnist.
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">For the data, the license is Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0): https://creativecommons.org/licenses/by-sa/3.0/

        For the code, we adopt NLE's license for ACADEMIC OR NON-PROFIT ORGANIZATION NONCOMMERCIAL RESEARCH USE ONLY; see the supplementary material. Except for one specific part of the framework, "IshiharaCreateSimDal.py", which is licensed under GNU GPL version 2.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mhQpJiWQyyi" data-number="148">
        <h4>
          <a href="/forum?id=mhQpJiWQyyi">
              ImgFact: Triplet Fact Grounding on Images
          </a>


            <a href="/pdf?id=mhQpJiWQyyi" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jingping_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingping_Liu1">Jingping Liu</a>, <a href="/profile?id=~Mingchuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingchuan_Zhang1">Mingchuan Zhang</a>, <a href="/profile?id=~Weichen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weichen_Li1">Weichen Li</a>, <a href="/profile?id=~Chao_Wang25" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Wang25">Chao Wang</a>, <a href="/profile?id=~Shuang_Li10" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuang_Li10">Shuang Li</a>, <a href="/profile?id=~Sihang_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sihang_Jiang1">Sihang Jiang</a>, <a href="/profile?id=~Haiyun_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haiyun_Jiang1">Haiyun Jiang</a>, <a href="/profile?id=~Yanghua_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanghua_Xiao1">Yanghua Xiao</a>, <a href="/profile?id=~Tong_Ruan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tong_Ruan2">Tong Ruan</a>, <a href="/profile?id=~Yunwen_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunwen_Chen1">Yunwen Chen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mhQpJiWQyyi-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mhQpJiWQyyi-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Knowledge graph, Triplet Fact Grounding, Multi-modal Knowledge Graph</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct a multi-modal knowledge graph with triplets and their images, and the model incorporating our collected images achieves 27% improvement on F1 of relation classification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most existing knowledge graphs are represented with pure symbols in the form of natural language, lacking multi-modal information. Previous studies mainly focus on grounding entities to images while ignoring the relations between them. In this paper, we introduce a new task of triplet fact grounding, which aims to find images that embody entities and their relation. To achieve this purpose, we propose a novel pipeline method, including triplet fact filtering, image retrieving, entity-based image filtering, relation-based image filtering, and image clustering. We construct a multi-modal knowledge graph named ImgFact, containing 247,732 triplet facts and their 3,730,805 images. The manual and automated evaluations prove the reliability of our ImgFact. We further use the images from ImgFact to enhance the model performance on two tasks. In particular, the model incorporating our collected images achieves 27% improvement on F1 of relation classification. We release the dataset and code at https://github.com/kleinercubs/ImgFact.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/kleinercubs/ImgFact" target="_blank" rel="nofollow noreferrer">https://github.com/kleinercubs/ImgFact</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset URL: https://github.com/kleinercubs/ImgFact
        The clear informantion are given in the URL.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We now release the dataset at https://github.com/kleinercubs/ImgFact.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="kcjCU06ij7_" data-number="147">
        <h4>
          <a href="/forum?id=kcjCU06ij7_">
              All the World's a (Hyper)Graph: A Data Drama
          </a>


            <a href="/pdf?id=kcjCU06ij7_" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Corinna_Coupette1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Corinna_Coupette1">Corinna Coupette</a>, <a href="/profile?id=~Jilles_Vreeken2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jilles_Vreeken2">Jilles Vreeken</a>, <a href="/profile?id=~Bastian_Rieck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bastian_Rieck1">Bastian Rieck</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#kcjCU06ij7_-details-461" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kcjCU06ij7_-details-461"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">data drama, dataset, graphs, hypergraphs, representations, robustness, Shakespeare</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Raw data stem from all of Shakespeare's plays / We model them as graphs in many ways / And demonstrate representations matter.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Hyperbard, a dataset of diverse relational data representations derived from Shakespeare's plays. Our representations range from simple graphs capturing character co-occurrence in single scenes to hypergraphs encoding complex communication settings and character contributions as hyperedges with edge-specific node weights. By making multiple intuitive representations readily available for experimentation, we facilitate rigorous representation robustness checks in graph learning, graph mining, and network analysis, highlighting the advantages and drawbacks of specific representations. Leveraging the data released in Hyperbard, we demonstrate that many solutions to popular graph mining problems are highly dependent on the representation choice, thus calling current graph curation practices into question. As an homage to our data source, and asserting that science can also be art, we present all our points in the form of a play.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=kcjCU06ij7_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://hyperbard.net/" target="_blank" rel="nofollow noreferrer">https://hyperbard.net/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://hyperbard.net/" target="_blank" rel="nofollow noreferrer">https://hyperbard.net/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset license: CC BY-NC 4.0, https://creativecommons.org/licenses/by-nc/4.0/
        The full text of the dataset license is included in the dataset deposit and the code repository.

        Code license: BSD 3-Clause, https://spdx.org/licenses/BSD-3-Clause.html
        The full text of the code license is included in the code repository.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="iJrhI02hx0S" data-number="146">
        <h4>
          <a href="/forum?id=iJrhI02hx0S">
              HAIR: A Dataset of Historic Aerial Images of Riverscapes for Semantic Segmentation
          </a>


            <a href="/pdf?id=iJrhI02hx0S" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=saeid.shamsaliei%40ntnu.no" class="profile-link" data-toggle="tooltip" data-placement="top" title="saeid.shamsaliei@ntnu.no">Saeid Shamsaliei</a>, <a href="/profile?id=~Odd_Erik_Gundersen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Odd_Erik_Gundersen2">Odd Erik Gundersen</a>, <a href="/profile?email=jo.halleraker%40ntnu.no" class="profile-link" data-toggle="tooltip" data-placement="top" title="jo.halleraker@ntnu.no">Jo Halvard Halleraker</a>, <a href="/profile?email=knut.alfredsen%40ntnu.no" class="profile-link" data-toggle="tooltip" data-placement="top" title="knut.alfredsen@ntnu.no">Knut Alfredsen</a>, <a href="/profile?email=anders.foldvik%40nina.no" class="profile-link" data-toggle="tooltip" data-placement="top" title="anders.foldvik@nina.no">Anders Foldvik</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#iJrhI02hx0S-details-859" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iJrhI02hx0S-details-859"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">semantic segmentation, dataset, historical aerial images, riverscapes</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset of historical aerial images of riverscapes that can help analyzing and understand the pressures on rivers and thus support the restoration of their fragile ecosystems.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Aerial images have been captured systematically since the 1930’s in some geographic areas and is as such a data source that enables tracking, analyzing and understanding land use over time. In this paper, we introduce HAIR, which is a dataset of historical aerial images of riverscapes with high-quality annotations made by experts on riverscapes that can be used for semantic segmentation. The current decade is declared the UN Decade of Ecosystem Restoration, and freshwater ecosystems have been judged as particularly degraded. Hence, this dataset could provide important insights to help make policies for how to maintain and restore the fragile ecosystems around rivers. HAIR has some issues of which some are not typical for other land cover datasets captured close in time or by satellites: 1) camera technology has developed immensely since the first aerial images were taken, and therefore the quality of the images in the dataset is diverse, 2) lighting conditions are affected by time of day and the path of the airplane, so spatially close images might have large differences in shading, 3) most of the complete set of the historic images are in black and white which makes it easy to confuse very different types of areas that could easily be differentiated based on color information, and finally 4) the dataset is highly biased, as the most important of the five classes, gravel, is small compared to the others. We benchmark state of the art semantic segmentation methods and present both quantitative and qualitative results.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://folk.idi.ntnu.no/odderik/HAIR/" target="_blank" rel="nofollow noreferrer">https://folk.idi.ntnu.no/odderik/HAIR/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://folk.idi.ntnu.no/odderik/HAIR/HAIR.zip" target="_blank" rel="nofollow noreferrer">https://folk.idi.ntnu.no/odderik/HAIR/HAIR.zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons 4.0 BY-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_MHKF0V89i3" data-number="144">
        <h4>
          <a href="/forum?id=_MHKF0V89i3">
              CBLab: A Platform for Large Scale Traffic Simulation Using Real World Traffic Data
          </a>


            <a href="/pdf?id=_MHKF0V89i3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chumeng_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chumeng_Liang2">Chumeng Liang</a>, <a href="/profile?id=~Zherui_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zherui_Huang1">Zherui Huang</a>, <a href="/profile?id=~Yicheng_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yicheng_Liu3">Yicheng Liu</a>, <a href="/profile?id=~Zhanyu_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhanyu_Liu1">Zhanyu Liu</a>, <a href="/profile?id=~Guanjie_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guanjie_Zheng1">Guanjie Zheng</a>, <a href="/profile?id=~Hanyuan_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanyuan_Shi1">Hanyuan Shi</a>, <a href="/profile?id=~Yuhao_Du3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhao_Du3">Yuhao Du</a>, <a href="/profile?id=~FULIANG_LI1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~FULIANG_LI1">FULIANG LI</a>, <a href="/profile?id=~Zhenhui_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenhui_Li1">Zhenhui Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#_MHKF0V89i3-details-595" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_MHKF0V89i3-details-595"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Traffic Simulation, Traffic Policy, Large Scale Data</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present CBLab, an online platform for large scale traffic experiments to train traffic policies using real world traffic data. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present CBLab, an online platform for large scale traffic experiments to train traffic policies using real world traffic data. This platform is composed of a highly scalable traffic simulator CBEngine, and a data network CBData gathering various traffic data. CBData converts road network data and traffic flow data to a unified schema and input it to CBEngine. Hence, researchers can easily query CBData with a spatial region and start experiments by running a few lines of code within five minutes, rather than wasting a few days in processing data and setting up environments on local machines. Our platform can enable researchers easily develop advanced methods to solve real-world traffic policy making problems (e.g., traffic signal control). Meanwhile, this platform will also become a challenging benchmark for reinforcement learning methods. This platform has successfully supported the City Brain Challenge @ KDDCUP 2021 with 1,156 participating teams.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_MHKF0V89i3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="RtSWZLdUK7d" data-number="143">
        <h4>
          <a href="/forum?id=RtSWZLdUK7d">
              CodeS: A Distribution Shift Benchmark Dataset for Source Code Learning
          </a>


            <a href="/pdf?id=RtSWZLdUK7d" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Qiang_Hu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Hu3">Qiang Hu</a>, <a href="/profile?id=~Yuejun_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuejun_Guo1">Yuejun Guo</a>, <a href="/profile?id=~Xiaofei_Xie2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaofei_Xie2">Xiaofei Xie</a>, <a href="/profile?id=~Maxime_Cordy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxime_Cordy1">Maxime Cordy</a>, <a href="/profile?id=~Lei_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Ma1">Lei Ma</a>, <a href="/profile?id=~Mike_Papadakis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Papadakis1">Mike Papadakis</a>, <a href="/profile?id=~YVES_LE_TRAON1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~YVES_LE_TRAON1">YVES LE TRAON</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#RtSWZLdUK7d-details-728" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RtSWZLdUK7d-details-728"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over the past few years, deep learning (DL) has been continuously expanding its applications and becoming a driving force for large-scale source code analysis in the big code era. Distribution shift, where the test set follows a different distribution from the training set, has been a longstanding challenge for the reliable deployment of DL models due to the unexpected accuracy degradation. Although recent progress on distribution shift benchmarking has been made in domains such as computer vision and natural language process. Limited progress has been made on distribution shift analysis and benchmarking for source code tasks, on which there comes a strong demand due to both its volume and its important role in supporting the foundations of almost all industrial sectors. To fill this gap, this paper initiates to propose CodeS, a distribution shift benchmark dataset, for source code learning. Specifically, CodeS supports 2 programming languages (i.e., Java and Python) and 5 types of code distribution shifts (i.e., task, programmer, time-stamp, token, and concrete syntax tree (CST)). To the best of our knowledge, we are the first to define the code representation-based (token and CST) distribution shifts. In the experiments, we first evaluate the effectiveness of existing out-of-distribution (OOD) detectors and the reasonability of the distribution shift definitions, and then measure the model generalization of popular code learning models (e.g., the pre-trained language model, CodeBERT) on classification tasks using CodeS. The results demonstrate that 1) only softmax score-based OOD detectors perform well on CodeS, 2) distribution shift causes the accuracy degradation in all code classification models,  3) representation-based distribution shifts have a higher impact on the model than others, and 4) pre-trained models are more resistant to distribution shifts.  We make CodeS publicly available, enabling follow-up research on the quality assessment of code learning models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=RtSWZLdUK7d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/testing-cs/CodeS" target="_blank" rel="nofollow noreferrer">https://github.com/testing-cs/CodeS</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ua_DAfI_Mt" data-number="142">
        <h4>
          <a href="/forum?id=ua_DAfI_Mt">
              Before and After Pseudoscience: An Empirical Challenge to Social Classification using Facial Recognition
          </a>


            <a href="/pdf?id=ua_DAfI_Mt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rohan_Faiyaz_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rohan_Faiyaz_Khan1">Rohan Faiyaz Khan</a>, <a href="/profile?id=~Catherine_Stinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_Stinson1">Catherine Stinson</a>, <a href="/profile?email=georgia.reed%40queensu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="georgia.reed@queensu.ca">Georgia Reed</a>, <a href="/profile?email=16sab%40queensu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="16sab@queensu.ca">Sam Baranek</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ua_DAfI_Mt-details-652" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ua_DAfI_Mt-details-652"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AI ethics, social aspects of ML, psysiognomy, facial recognition, social classification, experimental design</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An empirical challenge to AI research that claims to perform social classification on human faces using a dataset consisting of the same people presenting differently which is then used to perform control experiments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A number of researchers claim to be able to tell social category membership (criminality, sexual orientation, political orientation, etc.) using facial recognition, and infer that these categories have a biological basis the detection of which can be automated. Critics have raised ethical, political and methodological challenges to this body of work. Our contribution is an empirical demonstration that highlights flaws in the methodology typically used in these studies. These studies are instances of the more general phenomenon of machine learning studies that fail to compare results to meaningful benchmarks or use experimental controls. We re-implement a study that claims to be able to discern political orientation from images of faces posted to social media, and show that the predictive power of the model can be attributed entirely to grooming and presentation choices rather than deeper biological characteristics. We create a novel dataset consisting of pairs of images where an individual presents themself two different ways. The dataset allows of control experiments to be performed, and for a benchmark to be established. These images are drawn from searches for “before and after” images from makeovers, haircuts, and drag. We first use the re-implementated political orientation detection model to classify the images from our dataset. Individuals’ predicted political orientations differ for the two images 33% of the time. We also train the model to predict “before” and “after” categories on the new dataset, and achieve prediction accuracy of 68%, matching the political orientation model’s results. These findings show not only that superficial presentation differences can fully explain the prediction accuracy of the model, but also that the methods used in the original study can generate spurious results.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ua_DAfI_Mt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FrHI-Ch3ioW" data-number="141">
        <h4>
          <a href="/forum?id=FrHI-Ch3ioW">
              Multi-Object Tracking with mmWave Radars: RadarMOT Benchmark for Autonomous Driving
          </a>


            <a href="/pdf?id=FrHI-Ch3ioW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yizhou_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhou_Wang4">Yizhou Wang</a>, <a href="/profile?id=~Jiarui_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiarui_Cai1">Jiarui Cai</a>, <a href="/profile?id=~Jui-Te_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jui-Te_Huang1">Jui-Te Huang</a>, <a href="/profile?id=~Yudong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yudong_Li2">Yudong Li</a>, <a href="/profile?id=~Hung-Min_Hsu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hung-Min_Hsu1">Hung-Min Hsu</a>, <a href="/profile?id=~Hui_Liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hui_Liu4">Hui Liu</a>, <a href="/profile?id=~Jenq-Neng_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jenq-Neng_Hwang1">Jenq-Neng Hwang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#FrHI-Ch3ioW-details-910" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FrHI-Ch3ioW-details-910"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">RadarMOT, Multi-Object Tracking, Autonomous Driving, mmWave Radar</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces the radar-based multi-object tracking (RadarMOT) task and proposes the first radio fraquency (RF) image based tracking dataset about RadarMOT for autonomous driving.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As one of the most effective sensors for autonomous vehicles, radar's capability of semantic understanding is significantly underestimated compared with camera and LiDAR, due to its sparse reflections and implicit data representations. Although tracking is a classical topic in the radar community, most solutions are not ``object-aware'', i.e., being able to track multiple distinct foreground objects simultaneously, which is significant for object perception in autonomous driving applications. In this paper, we propose a new radar multi-object tracking (RadarMOT) benchmark to fulfill this urgent request, together with our baseline method to address this challenging problem. First, we introduce the RadarMOT dataset based on ROD2021, a semi-automatic tracking annotation method, and a set of evaluation metrics for RadarMOT. Second, we thoroughly evaluate various widely-used vision-based MOT frameworks on RadarMOT for both visible and vision-hard scenarios. The results indicate tracking objects with radar is a more robust and reliable solution for self-driving vehicles compared to camera-only. Third, we propose a unified neural network to perform radar object detection and generate the corresponding radar instance features for subsequent tracking-by-detection tasks. The detections and radar instance features are sent into a Kalman filter based tracking scheme, which considers object 3D locations and radar instance features. To the best of our knowledge, this is the first work that introduces the RadarMOT task to the community and considers extracting radar instance features for object tracking. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FrHI-Ch3ioW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.cruwdataset.org/" target="_blank" rel="nofollow noreferrer">https://www.cruwdataset.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset: https://www.cruwdataset.org/
        Annotations: https://drive.google.com/drive/folders/1r4HN3IdNFdxM-JQxOuDry2ZMB1M9KQGh?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC
        Creative Commons Attribution-NonCommercial 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="w7VPQWgnn3s" data-number="140">
        <h4>
          <a href="/forum?id=w7VPQWgnn3s">
              Pythae: Unifying Generative Autoencoders in Python - A Benchmarking Use Case
          </a>


            <a href="/pdf?id=w7VPQWgnn3s" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cl%C3%A9ment_Chadebec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Clément_Chadebec1">Clément Chadebec</a>, <a href="/profile?id=~Louis_J._Vincent2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis_J._Vincent2">Louis J. Vincent</a>, <a href="/profile?id=~Stephanie_Allassonniere1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephanie_Allassonniere1">Stephanie Allassonniere</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#w7VPQWgnn3s-details-676" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w7VPQWgnn3s-details-676"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Python Library, Generative Autoencoders, Benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we present Pythae, a versatile python library providing both a unified implementation and a dedicated framework allowing to perform straightforward reproducible and reliable use of generative autoencoder models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present \textbf{Pythae}, a versatile \textit{open-source} Python library providing both a \textit{unified implementation} and a dedicated framework allowing \textit{straightforward}, \emph{reproducible} and \textit{reliable} use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpolation. The open-source library can be found at \url{https://github.com/clementchadebec/benchmark_VAE}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=w7VPQWgnn3s&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/clementchadebec/benchmark_VAE" target="_blank" rel="nofollow noreferrer">https://github.com/clementchadebec/benchmark_VAE</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YVXaxB6L2Pl" data-number="139">
        <h4>
          <a href="/forum?id=YVXaxB6L2Pl">
              The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games
          </a>


            <a href="/pdf?id=YVXaxB6L2Pl" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chao_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Yu1">Chao Yu</a>, <a href="/profile?id=~Akash_Velu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akash_Velu1">Akash Velu</a>, <a href="/profile?id=~Eugene_Vinitsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Vinitsky1">Eugene Vinitsky</a>, <a href="/profile?id=~Jiaxuan_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaxuan_Gao1">Jiaxuan Gao</a>, <a href="/profile?id=~Yu_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Wang3">Yu Wang</a>, <a href="/profile?id=~Alexandre_Bayen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_Bayen2">Alexandre Bayen</a>, <a href="/profile?id=~Yi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wu1">Yi Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YVXaxB6L2Pl-details-650" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YVXaxB6L2Pl-details-650"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-Agent Reinforcement Learning, Proximal Policy Optimization, Cooperative Games</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We demonstrate PPO's effectiveness in popular multi-agent benchmarks and analyze its properties and implementation details through empirical studies. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to strong off-policy methods, PPO often achieves competitive or superior results in both final rewards and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YVXaxB6L2Pl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://github.com/marlbenchmark/on-policy, https://github.com/marlbenchmark/off-policy</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="SKE_J-B3e9X" data-number="138">
        <h4>
          <a href="/forum?id=SKE_J-B3e9X">
              CAESAR: An Embodied Simulator for Generating Multimodal Referring Expression Datasets
          </a>


            <a href="/pdf?id=SKE_J-B3e9X" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Md_Mofijul_Islam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Md_Mofijul_Islam1">Md Mofijul Islam</a>, <a href="/profile?id=~Reza_Manuel_Mirzaiee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Reza_Manuel_Mirzaiee1">Reza Manuel Mirzaiee</a>, <a href="/profile?id=~Alexi_Gladstone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexi_Gladstone1">Alexi Gladstone</a>, <a href="/profile?id=~Haley_N_Green1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haley_N_Green1">Haley N Green</a>, <a href="/profile?id=~Tariq_Iqbal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tariq_Iqbal1">Tariq Iqbal</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#SKE_J-B3e9X-details-453" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SKE_J-B3e9X-details-453"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Embodied Simulator, Referring Expression, Multimodal Spatial Relation Grounding</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel embodied simulator to generate multimodal referring expressions containing both verbal utterances and non-verbal gestures captured from multiple views.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Humans naturally use verbal utterances and nonverbal gestures to refer to various objects (known as $\textit{referring expressions}$) in different interactional scenarios. As collecting real human interaction datasets are costly and laborious, synthetic datasets are often used to train models to unambiguously detect relationships among objects. However, existing synthetic data generation tools that provide referring expressions generally neglect nonverbal gestures. Additionally, while a few small-scale datasets contain multimodal cues (verbal and nonverbal), these datasets only capture the nonverbal gestures from an exo-centric perspective (observer). As models can use complementary information from multimodal cues to recognize referring expressions, generating multimodal data from multiple views can help to develop robust models. To address these critical issues, in this paper, we present a novel embodied simulator, CAESAR, to generate multimodal referring expressions containing both verbal utterances and nonverbal cues captured from multiple views. Using our simulator, we have generated two large-scale embodied referring expression datasets, which we will release publicly. We have conducted experimental analyses on embodied spatial relation grounding using various state-of-the-art baseline models. Our experimental results suggest that visual perspective affects the models' performance; and that nonverbal cues improve spatial relation grounding accuracy. Finally, we will release the simulator publicly to allow researchers to generate new embodied interaction datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=SKE_J-B3e9X&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We plan to release the dataset before the start of the NeurIPS conference. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our datasets can be accessed using the CC BY-NC-SA license (https://creativecommons.org/licenses/by-nc-sa/4.0/). Moreover, our simulator source code will be released under the BSD 3-Clause license (https://opensource.org/licenses/BSD-3-Clause).</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="r8Hv2BtngrH" data-number="137">
        <h4>
          <a href="/forum?id=r8Hv2BtngrH">
              DIVOTrack: A Cross-View Dataset for Multi-Human Tracking in DIVerse Open Scenes
          </a>


            <a href="/pdf?id=r8Hv2BtngrH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gaoang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaoang_Wang2">Gaoang Wang</a>, <a href="/profile?id=~Shengyu_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyu_Hao1">Shengyu Hao</a>, <a href="/profile?id=~Yibing_Zhan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yibing_Zhan2">Yibing Zhan</a>, <a href="/profile?id=~Peiyuan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peiyuan_Liu1">Peiyuan Liu</a>, <a href="/profile?id=~Zuozhu_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zuozhu_Liu1">Zuozhu Liu</a>, <a href="/profile?id=~Mingli_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingli_Song1">Mingli Song</a>, <a href="/profile?id=~Jenq-Neng_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jenq-Neng_Hwang1">Jenq-Neng Hwang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#r8Hv2BtngrH-details-54" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r8Hv2BtngrH-details-54"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">cross-view tracking</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Cross-view multi-human tracking tries to link human subjects between frames and camera views that contain substantial overlaps. Although cross-view multi-human tracking has received increased attention in recent years, existing datasets still have several issues, including 1) missing real-world scenarios, 2) lacking diverse scenes, 3) owning a limited number of tracks, 4) comprising only static cameras, and 5) lacking standard benchmarks, which hinders the exploration and comparison of cross-view tracking methods. To solve the above concerns, we present DIVOTrack: a new cross-view multi-human tracking dataset for DIVerse Open scenes with dense tracking pedestrians in realistic and non-experimental environments. In addition, our DIVOTrack contains ten different types of scenarios and 550 cross-view tracks, which surpasses all existing cross-view human tracking datasets. Furthermore, our DIVOTrack contains videos that are collected by two mobile cameras and one unmanned aerial vehicle, allowing us to evaluate the efficacy of methods while dealing with dynamic views. Finally, we present a summary of current methodologies and a set of standard benchmarks with our DIVOTrack to provide a fair comparison and conduct a thorough analysis of current approaches. The dataset will be available at https://github.com/shengyuhao/DIVOTrack.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=r8Hv2BtngrH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/shengyuhao/DIVOTrack" target="_blank" rel="nofollow noreferrer">https://github.com/shengyuhao/DIVOTrack</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/shengyuhao/DIVOTrack" target="_blank" rel="nofollow noreferrer">https://github.com/shengyuhao/DIVOTrack</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="in7XC5RcjEn" data-number="136">
        <h4>
          <a href="/forum?id=in7XC5RcjEn">
              Long Range Graph Benchmark
          </a>


            <a href="/pdf?id=in7XC5RcjEn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Vijay_Prakash_Dwivedi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vijay_Prakash_Dwivedi1">Vijay Prakash Dwivedi</a>, <a href="/profile?id=~Ladislav_Rampasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ladislav_Rampasek1">Ladislav Rampasek</a>, <a href="/profile?id=~Mikhail_Galkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Galkin1">Mikhail Galkin</a>, <a href="/profile?id=~Ali_Parviz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Parviz1">Ali Parviz</a>, <a href="/profile?id=~Guy_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guy_Wolf1">Guy Wolf</a>, <a href="/profile?id=~Anh_Tuan_Luu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anh_Tuan_Luu2">Anh Tuan Luu</a>, <a href="/profile?id=~Dominique_Beaini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dominique_Beaini1">Dominique Beaini</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#in7XC5RcjEn-details-603" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="in7XC5RcjEn-details-603"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph learning benchmark, long range dependencies, graph transformers, graph datasets, graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the Long Range Graph Benchmark (LRGB) with 5 datasets that can be used for the development of models enabling long range dependencies in graphs, like Graph Transformers.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: $\texttt{PascalVOC-SP}$, $\texttt{COCO-SP}$, $\texttt{PCQM-Contact}$, $\texttt{Peptides-func}$ and $\texttt{Peptides-struct}$ that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP-GNNs and Graph Transformer architectures that are intended to capture LRI.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=in7XC5RcjEn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/vijaydwivedi75/lrgb" target="_blank" rel="nofollow noreferrer">https://github.com/vijaydwivedi75/lrgb</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/vijaydwivedi75/lrgb" target="_blank" rel="nofollow noreferrer">https://github.com/vijaydwivedi75/lrgb</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The licenses of the datasets in the proposed benchmark are listed in the supplementary as well as the README of the code repository at https://github.com/vijaydwivedi75/lrgb.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Zp8YmiQ_bDC" data-number="135">
        <h4>
          <a href="/forum?id=Zp8YmiQ_bDC">
              AirfoilRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged-Navier–Stokes Solutions
          </a>


            <a href="/pdf?id=Zp8YmiQ_bDC" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Florent_Bonnet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florent_Bonnet1">Florent Bonnet</a>, <a href="/profile?id=~Jocelyn_Ahmed_Mazari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jocelyn_Ahmed_Mazari1">Jocelyn Ahmed Mazari</a>, <a href="/profile?email=paola.cinnella%40sorbonne-universite.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="paola.cinnella@sorbonne-universite.fr">Paola Cinnella</a>, <a href="/profile?id=~patrick_gallinari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~patrick_gallinari1">patrick gallinari</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Zp8YmiQ_bDC-details-367" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zp8YmiQ_bDC-details-367"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Computational Fluid Dynamics, Navier-Stokes Equations, Partial Differential Equations, Physical Metrics, Geometric Deep Learning, Graph Neural Networks, Point Clouds, Surrogate Models, Reduced Order Models, Meshes, Physically Constrained Deep Learning, Numerical Simulation, Fluid Mechanics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a high fidelity aerodynamic dataset of Reynolds-Averaged-Navier–Stokes (RANS) simulations over airfoils</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier-Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomenons are lacking. In this work, we develop \textsc{AirfRANS}, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged-Navier-Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study \textsc{AirfRANS} under different constraints for generalization considerations: big and scarce data regime, Reynolds number and angle of attack extrapolation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Zp8YmiQ_bDC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://data.isir.upmc.fr/extrality/NeurIPS_2022/Dataset.zip" target="_blank" rel="nofollow noreferrer">https://data.isir.upmc.fr/extrality/NeurIPS_2022/Dataset.zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Root folder: https://data.isir.upmc.fr/extrality/NeurIPS_2022/
        Machine learning ready dataset : https://data.isir.upmc.fr/extrality/NeurIPS_2022/Dataset.zip
        Full OpenFOAM dataset: https://data.isir.upmc.fr/extrality/NeurIPS_2022/OF_dataset.zip</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Open Data Commons Open Database License v1.0. The Open Database License (ODbL) is a license agreement intended to allow users to freely share, modify, and use a database while maintaining this same freedom for others.
        https://opendatacommons.org/licenses/odbl/1-0/</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YXvGXEmtZ5N" data-number="134">
        <h4>
          <a href="/forum?id=YXvGXEmtZ5N">
              Benchmarking Node Outlier Detection on Graphs
          </a>


            <a href="/pdf?id=YXvGXEmtZ5N" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kay_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kay_Liu1">Kay Liu</a>, <a href="/profile?id=~Yingtong_Dou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingtong_Dou1">Yingtong Dou</a>, <a href="/profile?id=~Yue_Zhao13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Zhao13">Yue Zhao</a>, <a href="/profile?id=~Xueying_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xueying_Ding1">Xueying Ding</a>, <a href="/profile?id=~Xiyang_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiyang_Hu1">Xiyang Hu</a>, <a href="/profile?email=rtzhang%40buaa.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="rtzhang@buaa.edu.cn">Ruitong Zhang</a>, <a href="/profile?id=~Kaize_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaize_Ding1">Kaize Ding</a>, <a href="/profile?id=~Canyu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Canyu_Chen1">Canyu Chen</a>, <a href="/profile?id=~Hao_Peng7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Peng7">Hao Peng</a>, <a href="/profile?id=~Kai_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Shu1">Kai Shu</a>, <a href="/profile?id=~Lichao_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lichao_Sun1">Lichao Sun</a>, <a href="/profile?id=~Jundong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jundong_Li2">Jundong Li</a>, <a href="/profile?id=~George_H._Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~George_H._Chen1">George H. Chen</a>, <a href="/profile?id=~Zhihao_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhihao_Jia1">Zhihao Jia</a>, <a href="/profile?id=~Philip_S._Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_S._Yu1">Philip S. Yu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YXvGXEmtZ5N-details-547" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YXvGXEmtZ5N-details-547"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Mining, Graph Neural Networks, Outlier Detection, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present UNOD, a comprehensive benchmark for unsupervised node outlier detection on graphs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph outlier detection is an emerging but crucial machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years, the lack of a standard and unified setting for performance evaluation limits their advancement and usage in real-world applications. To tap the gap, we present, (to our best knowledge) the first comprehensive unsupervised node outlier detection benchmark for graphs called UNOD, with the following highlights: (1) evaluating fourteen methods with backbone spanning from classical matrix factorization to the latest graph neural networks; (2) benchmarking the method performance with different types of injected outliers and organic outliers on real-world datasets; (3) comparing the efficiency and scalability of the algorithms by runtime and GPU memory usage on synthetic graphs at different scales. Based on the analyses of extensive experimental results, we discuss the pros and cons of current UNOD methods, and point out multiple crucial and promising future research directions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YXvGXEmtZ5N&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pygod-team/pygod/tree/main/benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/pygod-team/pygod/tree/main/benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD-2-Clause license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bSULxOy3On" data-number="133">
        <h4>
          <a href="/forum?id=bSULxOy3On">
              Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs
          </a>


            <a href="/pdf?id=bSULxOy3On" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Seiji_Maekawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seiji_Maekawa1">Seiji Maekawa</a>, <a href="/profile?id=~Koki_Noda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Koki_Noda1">Koki Noda</a>, <a href="/profile?id=~Yuya_Sasaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuya_Sasaki1">Yuya Sasaki</a>, <a href="/profile?id=~Makoto_Onizuka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Makoto_Onizuka1">Makoto Onizuka</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#bSULxOy3On-details-435" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bSULxOy3On-details-435"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph neural networks, classification, heterophily, synthetic graphs</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We empirically study the performance of GNNs with various synthetic graphs by synthetically changing one or a few target characteristic(s) of graphs while keeping other characteristics fixed. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) have achieved great success on a node classification task. Despite the broad interest in developing and evaluating GNNs, they have been assessed with limited benchmark datasets. As a result, the existing evaluation of GNNs lacks fine-grained analysis from various characteristics of graphs. Motivated by this, we conduct extensive experiments with a synthetic graph generator that can generate graphs having controlled characteristics for fine-grained analysis. Our empirical studies clarify the strengths and weaknesses of GNNs from four major characteristics of real-world graphs with class labels of nodes, i.e., 1) class size distributions (balanced vs. imbalanced), 2) edge connection proportions between classes (homophilic vs. heterophilic), 3) attribute values (biased vs. random), and 4) graph sizes (small vs. large). In addition, to foster future research on GNNs, we publicly release our codebase that allows users to evaluate various GNNs with various graphs. We hope this work offers interesting insights for future research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bSULxOy3On&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/seijimaekawa/empirical-study-of-GNNs" target="_blank" rel="nofollow noreferrer">https://github.com/seijimaekawa/empirical-study-of-GNNs</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Omlxi_Vhyci" data-number="132">
        <h4>
          <a href="/forum?id=Omlxi_Vhyci">
              The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior
          </a>


            <a href="/pdf?id=Omlxi_Vhyci" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jennifer_J._Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_J._Sun1">Jennifer J. Sun</a>, <a href="/profile?id=~Andrew_Wesley_Ulmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Wesley_Ulmer1">Andrew Wesley Ulmer</a>, <a href="/profile?id=~Dipam_Chakraborty1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dipam_Chakraborty1">Dipam Chakraborty</a>, <a href="/profile?id=~Brian_Geuther1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_Geuther1">Brian Geuther</a>, <a href="/profile?id=~Edward_Hayes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Hayes1">Edward Hayes</a>, <a href="/profile?id=~Heng_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heng_Jia1">Heng Jia</a>, <a href="/profile?id=~Vivek_Kumar4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vivek_Kumar4">Vivek Kumar</a>, <a href="/profile?id=~Zachary_Partridge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Partridge1">Zachary Partridge</a>, <a href="/profile?id=~Alice_Robie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alice_Robie1">Alice Robie</a>, <a href="/profile?id=~Catherine_E_Schretter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_E_Schretter1">Catherine E Schretter</a>, <a href="/profile?id=~Chao_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Sun3">Chao Sun</a>, <a href="/profile?id=~Keith_Sheppard2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keith_Sheppard2">Keith Sheppard</a>, <a href="/profile?id=~Param_Uttarwar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Param_Uttarwar1">Param Uttarwar</a>, <a href="/profile?id=~Pietro_Perona1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pietro_Perona1">Pietro Perona</a>, <a href="/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yisong_Yue1">Yisong Yue</a>, <a href="/profile?id=~Kristin_Branson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kristin_Branson1">Kristin Branson</a>, <a href="/profile?id=~Ann_Kennedy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ann_Kennedy1">Ann Kennedy</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#Omlxi_Vhyci-details-580" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Omlxi_Vhyci-details-580"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">behavior modeling, animal behavior, trajectory data, behavioral representation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">MABe2022 consists of multi-agent trajectory data with a range of tasks from behavioral neuroscience for studying behavioral representation learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Real-world behavior is often shaped by complex interactions between multiple agents. To scalably study multi-agent behavior, advances in unsupervised and self-supervised learning have enabled a variety of different behavioral representations to be learned from trajectory data. To date, there does not exist a unified set of benchmarks that can enable comparing methods quantitatively and systematically across a broad set of behavior analysis settings. We aim to address this by introducing a large-scale, multi-agent trajectory dataset from real-world behavioral neuroscience experiments that covers a range of behavior analysis tasks. Our dataset consists of trajectory data from common model organisms, with 9.6 million frames of mouse data and 4.4 million frames of fly data, in a variety of experimental settings, such as different strains, lengths of interaction, and optogenetic stimulation. A subset of the frames also consist of expert-annotated behavior labels. Improvements on our dataset corresponds to behavioral representations that work across multiple organisms and is able to capture differences for common behavior analysis tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Omlxi_Vhyci&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://data.caltech.edu/records/20186" target="_blank" rel="nofollow noreferrer">https://data.caltech.edu/records/20186</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset can be downloaded from https://data.caltech.edu/records/20186.

        The dataset is available in .npy format, which can be opened and processed using Python.
        For a code example of loading and visualizing the dataset:
        (1) Mouse Triplets: https://www.aicrowd.com/showcase/getting-started-mabe-2022-mouse-triplets-round-1
        (2) Fly Groups: https://www.aicrowd.com/showcase/getting-started-mabe-challenge-2022-fruit-flies-v-0-2-2kb</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value "><a href="https://creativecommons.org/licenses/by-nc-sa/2.0/" target="_blank" rel="nofollow noreferrer">https://creativecommons.org/licenses/by-nc-sa/2.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ChWo6qLgILf" data-number="131">
        <h4>
          <a href="/forum?id=ChWo6qLgILf">
              SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning
          </a>


            <a href="/pdf?id=ChWo6qLgILf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Changan_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changan_Chen2">Changan Chen</a>, <a href="/profile?id=~Carl_Schissler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carl_Schissler1">Carl Schissler</a>, <a href="/profile?email=sangarg%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sangarg@fb.com">Sanchit Garg</a>, <a href="/profile?id=~Philip_Kobernik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Kobernik1">Philip Kobernik</a>, <a href="/profile?id=~Alexander_Clegg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Clegg1">Alexander Clegg</a>, <a href="/profile?id=~Paul_Calamia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Calamia1">Paul Calamia</a>, <a href="/profile?id=~Dhruv_Batra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dhruv_Batra1">Dhruv Batra</a>, <a href="/profile?id=~Philip_W_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_W_Robinson1">Philip W Robinson</a>, <a href="/profile?id=~Kristen_Grauman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kristen_Grauman1">Kristen Grauman</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ChWo6qLgILf-details-303" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ChWo6qLgILf-details-303"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">embodied learning, audio-visual learning, visual acoustic learning, acoustic simulation, sim2real</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value "> We are releasing SoundSpaces 2.0: a fast, continuous, configurable and generalizable audio-visual simulation platform for visual acoustic machine learning research, e.g., audio-visual navigation, far-field speech recognition, and acoustic matching.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our best knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and  benchmark its performance against real-world audio measurements. In addition, through two downstream tasks covering embodied navigation and far-field automatic speech recognition, highlighting sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ChWo6qLgILf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/facebookresearch/sound-spaces" target="_blank" rel="nofollow noreferrer">https://github.com/facebookresearch/sound-spaces</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The binaries of RLR-Audio-Propagation are released at https://github.com/facebookresearch/rlr-audio-propagation. The integration with Habitat-Sim is available at https://github.com/facebookresearch/habitat-sim/blob/main/docs/AUDIO.md. The high-level APIs for tasks and training scripts are available at https://github.com/facebookresearch/sound-spaces.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License for SoundSpaces:  CC-BY-4.0
        License for RLR-Audio-Propagation: CC-BY-NC
        License for Habitat-Sim: MIT</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="GKOa7yNH8Uh" data-number="130">
        <h4>
          <a href="/forum?id=GKOa7yNH8Uh">
              GLOBEM: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization
          </a>


            <a href="/pdf?id=GKOa7yNH8Uh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xuhai_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuhai_Xu1">Xuhai Xu</a>, <a href="/profile?id=~Han_Zhang15" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Zhang15">Han Zhang</a>, <a href="/profile?id=~Yasaman_S_Sefidgar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yasaman_S_Sefidgar1">Yasaman S Sefidgar</a>, <a href="/profile?id=~Yiyi_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiyi_Ren1">Yiyi Ren</a>, <a href="/profile?id=~Xin_Liu8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Liu8">Xin Liu</a>, <a href="/profile?id=~Woosuk_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Woosuk_Seo1">Woosuk Seo</a>, <a href="/profile?id=~Jennifer_Brown2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_Brown2">Jennifer Brown</a>, <a href="/profile?id=~Kevin_Scott_Kuehn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Scott_Kuehn1">Kevin Scott Kuehn</a>, <a href="/profile?id=~Mike_A_Merrill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_A_Merrill1">Mike A Merrill</a>, <a href="/profile?id=~Paula_S_Nurius1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paula_S_Nurius1">Paula S Nurius</a>, <a href="/profile?id=~Shwetak_Patel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shwetak_Patel1">Shwetak Patel</a>, <a href="/profile?id=~Tim_Althoff2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Althoff2">Tim Althoff</a>, <a href="/profile?id=~Margaret_E_Morris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Margaret_E_Morris1">Margaret E Morris</a>, <a href="/profile?id=~Eve_A._Riskin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eve_A._Riskin1">Eve A. Riskin</a>, <a href="/profile?id=~Jennifer_Mankoff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_Mankoff1">Jennifer Mankoff</a>, <a href="/profile?id=~Anind_Dey2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anind_Dey2">Anind Dey</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#GKOa7yNH8Uh-details-73" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GKOa7yNH8Uh-details-73"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Longitudinal time-series, mobile sensing, human behavior modeling, domain generalization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the first multi-year mobile sensing datasets containing over 700 users to support the ML community in developing generalizable longitudinal behavior modeling algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 users' data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms' generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=GKOa7yNH8Uh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">We plan to leverage the PhysioNet platform to host our data, with credentialed access.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Please find the link in "Official Comments". Thanks!</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">PhysioNet Credentialed Health Data License 1.5.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="70_Wx-dON3q" data-number="129">
        <h4>
          <a href="/forum?id=70_Wx-dON3q">
              Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification
          </a>


            <a href="/pdf?id=70_Wx-dON3q" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ihsan_Ullah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ihsan_Ullah2">Ihsan Ullah</a>, <a href="/profile?email=dustin.carrion%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dustin.carrion@gmail.com">Dustin Carrion</a>, <a href="/profile?id=~Sergio_Escalera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergio_Escalera1">Sergio Escalera</a>, <a href="/profile?id=~Isabelle_M_Guyon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Isabelle_M_Guyon1">Isabelle M Guyon</a>, <a href="/profile?id=~Mike_Huisman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Huisman1">Mike Huisman</a>, <a href="/profile?id=~Felix_Mohr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_Mohr1">Felix Mohr</a>, <a href="/profile?id=~Jan_N._van_Rijn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_N._van_Rijn1">Jan N. van Rijn</a>, <a href="/profile?id=~Haozhe_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haozhe_Sun1">Haozhe Sun</a>, <a href="/profile?id=~Joaquin_Vanschoren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joaquin_Vanschoren1">Joaquin Vanschoren</a>, <a href="/profile?id=~Phan_Anh_Vu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phan_Anh_Vu1">Phan Anh Vu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#70_Wx-dON3q-details-107" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="70_Wx-dON3q-details-107"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">meta-dataset, few-shot learning, meta-learning, cross-domain meta-learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A meta-dataset for few shot image classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes 40 open datasets, each having at least 20 classes with 40 examples per class, with verified licences. They stem from diverse domains, such as ecology (fauna and flora), manufacturing (textures, vehicles), human actions, and optical character recognition, featuring various image scales (microscopic, human scales, remote sensing). All datasets are preprocessed, annotated, and formatted uniformly, and come in 3 versions (Micro ⊂ Mini ⊂ Extended) to match users’ computational resources. We showcase the utility of the first 30 datasets (to bereleased for NeurIPS 2022) on few-shot learning problems. The other 10 will be released shortly after. Meta-Album is already more diverse and larger (in number of datasets) than similar efforts, and we are committed to keep enlarging it via a series of meta-learning competitions. As competitions terminate, their test data are released, thus creating a rolling benchmark, available through OpenML.org. Our website https://meta-album.github.io/ contains the source code of challenge winning methods, baseline methods, data loaders, and instructions for contributing either new datasets or algorithms to our expandable meta-dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=70_Wx-dON3q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://meta-album.github.io/" target="_blank" rel="nofollow noreferrer">https://meta-album.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">NA</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">All Meta-Album datasets are provided for review through a password-protected link. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">Meta-Album datasets will be released according to the following schedule on OpenML ( https://openml.org ):

        Set-0 : 10 datasets - released 06 June 2022
        Set-1 and Set-2 : 20 datasets - to be released on or before 30 November 2022, before NeurIPS (datasets currently used in a NeurIPS’22 challenge)
        Set-3 : 10 datasets - to be released on or before 06 June 2023 (datasets to be used in an upcoming challenge on bias detection)</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Meta-Album is realeased under the license : CC BY-NC 4.0 (https://meta-album.github.io/)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ObD_o92z4p" data-number="128">
        <h4>
          <a href="/forum?id=ObD_o92z4p">
              LIPS - Learning Industrial Physical Simulation benchmark suite
          </a>


            <a href="/pdf?id=ObD_o92z4p" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Milad_Leyli-abadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Milad_Leyli-abadi1">Milad Leyli-abadi</a>, <a href="/profile?email=antoine.marot%40rte-france.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="antoine.marot@rte-france.com">Antoine Marot</a>, <a href="/profile?id=~J%C3%A9r%C3%B4me_Picault2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jérôme_Picault2">Jérôme Picault</a>, <a href="/profile?email=david.danan%40irt-systemx.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="david.danan@irt-systemx.fr">David Danan</a>, <a href="/profile?id=~Mouadh_Yagoubi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mouadh_Yagoubi1">Mouadh Yagoubi</a>, <a href="/profile?email=benjamin.donnot%40rte-france.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="benjamin.donnot@rte-france.com">Benjamin Donnot</a>, <a href="/profile?email=seifeddine.attoui%40irt-systemx.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="seifeddine.attoui@irt-systemx.fr">Seif-Eddine Attoui</a>, <a href="/profile?email=pdimitrov%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pdimitrov@nvidia.com">Pavel Dimitrov</a>, <a href="/profile?email=afarjallah%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="afarjallah@nvidia.com">Asma Farjallah</a>, <a href="/profile?id=~Clement_Etienam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Clement_Etienam1">Clement Etienam</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#ObD_o92z4p-details-858" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ObD_o92z4p-details-858"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmark suite, physical simulations, surrogate model, industrial use case</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces a new benchmark suite "Learning Industrial Physical Simulations" (LIPS), whose purpose is to assess the quality of surrogate models for emulation of a physical system following various evaluation criteria categories</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Physical simulations are at the core of many critical industrial systems. However, today's physical simulators  have some limitations such as computation time, dealing with missing or uncertain data, or even  non-convergence for some feasible cases. Recently, the use of data-driven approaches to learn complex physical simulations has been considered as a promising approach to address those issues. However, this comes often at the cost of some accuracy which may hinder the industrial use. To drive this new research topic towards a better real-world applicability, we propose a new benchmark suite "Learning Industrial Physical Simulations"(LIPS) to meet the need of developing efficient, industrial application-oriented, augmented simulators. To define how to assess such benchmark performance, we propose a set of four generic categories of criteria. The proposed benchmark suite is a modular and configurable framework that can deal with different physical problems. To demonstrate this ability, we propose in this paper to investigate two distinct use-cases with different physical simulations, namely: the power grid and the pneumatic. For each use case, several benchmarks are described and assessed with existing models. None of the models perform well under all expected criteria, inviting the community to develop  new industry-applicable solutions and possibly showcase their performance publicly upon online LIPS instance on Codabench.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ObD_o92z4p&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Mleyliabadi/LIPS" target="_blank" rel="nofollow noreferrer">https://github.com/Mleyliabadi/LIPS</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Mleyliabadi/LIPS/tree/main/reference_data" target="_blank" rel="nofollow noreferrer">https://github.com/Mleyliabadi/LIPS/tree/main/reference_data</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Mozilla Public License Version 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="H0SC-Vmw4R" data-number="127">
        <h4>
          <a href="/forum?id=H0SC-Vmw4R">
              A Benchmark for Explaining Counterfactual Explainers
          </a>


            <a href="/pdf?id=H0SC-Vmw4R" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Diego_Velazquez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diego_Velazquez1">Diego Velazquez</a>, <a href="/profile?id=~Pau_Rodriguez2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pau_Rodriguez2">Pau Rodriguez</a>, <a href="/profile?id=~Alexandre_Lacoste1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_Lacoste1">Alexandre Lacoste</a>, <a href="/profile?id=~Issam_H._Laradji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Issam_H._Laradji1">Issam H. Laradji</a>, <a href="/profile?email=xavier.roca%40uab.cat" class="profile-link" data-toggle="tooltip" data-placement="top" title="xavier.roca@uab.cat">Xavier Roca</a>, <a href="/profile?id=~Jordi_Gonz%C3%A0lez3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jordi_Gonzàlez3">Jordi Gonzàlez</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#H0SC-Vmw4R-details-857" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H0SC-Vmw4R-details-857"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Explainability, Benchmark, Counterfactuals, XAI</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">    Explainability methods have been widely used to interpret and understand the decisions made by classifiers in order to facilitate their adoption in high-stakes applications across various domains within the industry. Counterfactual explanation methods aim to improve our understanding of a model by perturbing samples in a way that would alter its response in an unexpected manner. This information is helpful for users and for machine learning practitioners to understand and improve their models. Given the value provided by counterfactual explanations, there is a growing interest in the research community to investigate and propose new methods. However, we identify two issues that could hinder the progress in this field. (1) With each  method, the authors propose a different evaluation metric to compare with previous literature, thus there is no consensus on what a good counterfactual explanation method is. (2) Such comparisons are usually performed with datasets like CelebA, where images are annotated with attributes that do not fully describe them and with subjective attributes such as ``Attractive''. In this work, we address these problems by proposing a benchmark with a principled metric to evaluate and compare different counterfactual explanation methods. The benchmark is based on a synthetic dataset where images are fully described by their annotated attributes. As a result, we are able to perform a fair comparison of multiple explainability methods in the recent literature, obtaining insights about their performance. We will make the benchmark public for the benefit of the research community.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=H0SC-Vmw4R&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/Bex-15A3/README.md" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/Bex-15A3/README.md</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6616598" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6616598</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MOCZI3h8Ye" data-number="126">
        <h4>
          <a href="/forum?id=MOCZI3h8Ye">
              Model Zoo: A Dataset of Diverse Populations of Neural Network Models
          </a>


            <a href="/pdf?id=MOCZI3h8Ye" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Konstantin_Sch%C3%BCrholt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantin_Schürholt1">Konstantin Schürholt</a>, <a href="/profile?id=~Diyar_Taskiran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diyar_Taskiran1">Diyar Taskiran</a>, <a href="/profile?id=~Boris_Knyazev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Knyazev1">Boris Knyazev</a>, <a href="/profile?id=~Xavier_Gir%C3%B3-i-Nieto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xavier_Giró-i-Nieto1">Xavier Giró-i-Nieto</a>, <a href="/profile?id=~Damian_Borth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Damian_Borth1">Damian Borth</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 10 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#MOCZI3h8Ye-details-786" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MOCZI3h8Ye-details-786"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Model Zoo, Population, Neural Networks, Model Analysis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">To enable the investigation of populations of neural network models, we release a novel dataset of diverse model zoos with this work.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In the last years, neural networks have evolved from laboratory environments to the state-of-the-art for many real-world problems. Our hypothesis is that neural network models (i.e., their weights and biases) evolve on unique, smooth trajectories in weight space during training. Following, a population of such neural network models (refereed to as “model zoo”) would form topological structures in weight space. We think that the geometry, curvature and smoothness of these structures contain information about the state of training and can be reveal latent properties of individual models. With such zoos, one could investigate novel approaches for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn rich representations of such populations, or (iv) exploit the model zoos for generative modelling of neural network weights and biases. Unfortunately, the lack of standardized model zoos and available benchmarks significantly increases the friction for further research about populations of neural networks. With this work, we publish a novel dataset of model zoos containing systematically generated and diverse populations of neural network models for further research. In total the proposed model zoo dataset is based on six image datasets, consist of 24 model zoos with varying hyperparameter combinations are generated and includes 47’360 unique neural network models resulting in over 2’415’360 collected model states. Additionally, to the model zoo data we provide an in-depth analysis of the zoos and provide benchmarks for multiple downstream tasks as mentioned before. The dataset can be found at www.modelzoos.cc.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MOCZI3h8Ye&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">www.modelzoos.cc</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">A landing page can be found under www.modelzoos.cc. Raw and preprocessed datasets are linked there. Code to reproduce, alter or extend the zoos is available, as is code load and preprocess the raw zoos and to reproduce the benchmark results. The landing page will be continuously extended with further documentation and analysis. The datasets are hosted on Zenodo to ensure long term availability.
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is publicly available and licensed under the Creative Commons Attribution 4.0 International license (CC-BY 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="b31d1fTRd4I" data-number="125">
        <h4>
          <a href="/forum?id=b31d1fTRd4I">
              Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks
          </a>


            <a href="/pdf?id=b31d1fTRd4I" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Pedro_Rodriguez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Rodriguez1">Pedro Rodriguez</a>, <a href="/profile?email=azab%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="azab@fb.com">Mahmoud Azab</a>, <a href="/profile?email=rsilvert%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rsilvert@fb.com">Becka Silvert</a>, <a href="/profile?id=~Renato_Sanchez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renato_Sanchez1">Renato Sanchez</a>, <a href="/profile?email=linzyrey%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="linzyrey@fb.com">Linzy Labson</a>, <a href="/profile?id=~Hardik_J_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hardik_J_Shah1">Hardik J Shah</a>, <a href="/profile?id=~Seungwhan_Moon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungwhan_Moon1">Seungwhan Moon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#b31d1fTRd4I-details-494" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b31d1fTRd4I-details-494"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multimodal, retrieval, dataset, benchmark, video, search, evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We assess the validity of text-to-video retrieval benchmarks and find that due to a methodological flaw that leads to false-negative labels, that benchmarks vastly understate model retrieval effectiveness.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Searching vast troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) all other videos as negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which creates false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25% recall points---a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the recomputed metrics are up to 25% recall points higher for the best models, (2) these benchmarks are nearing saturation for Recall@10, and (3) analysis suggests that models with more generalizable text encoders make the most substantial gains. Based on these results, we recommend that the community should consider retiring these benchmarks soon and build a better text-to-video retrieval benchmark. We conclude by highlighting structurally similar tasks that likely have the same problem and make recommendations for future benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=b31d1fTRd4I&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.pedro.ai/multimodal-retrieval-evaluation" target="_blank" rel="nofollow noreferrer">https://www.pedro.ai/multimodal-retrieval-evaluation</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The webpage at https://www.pedro.ai/multimodal-retrieval-evaluation will provide instructions on how to access the code and dataset. We aim to make these available by the supplemental materials deadline on June 16.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We intend to release code under Apache 2 or a similar permissive license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YmacJv0i_UR" data-number="124">
        <h4>
          <a href="/forum?id=YmacJv0i_UR">
              GriddlyJS: A Web IDE for Reinforcement Learning
          </a>


            <a href="/pdf?id=YmacJv0i_UR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Christopher_Bamford1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Bamford1">Christopher Bamford</a>, <a href="/profile?id=~Minqi_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minqi_Jiang1">Minqi Jiang</a>, <a href="/profile?id=~Mikayel_Samvelyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikayel_Samvelyan1">Mikayel Samvelyan</a>, <a href="/profile?id=~Tim_Rockt%C3%A4schel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Rocktäschel1">Tim Rocktäschel</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YmacJv0i_UR-details-365" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YmacJv0i_UR-details-365"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">environment design, tooling, developer tools, reinforcement learning, procedural content generation, integrated development environment, human in the loop.</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">An integrated development environment for reinforcement learning that streamlines the development, debugging, and agent evaluation for procedurally-generated environments</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Progress in reinforcement learning (RL) research is often driven by the design of new, challenging environments---a costly undertaking requiring skills orthogonal to that of a typical machine learning researcher. The complexity of environment development has only increased with the rise of procedural-content generation (PCG) as the prevailing paradigm for producing varied environments capable of testing the robustness and generalization of RL agents. Moreover, existing environments often require complex build processes, making reproducing results difficult. To address these issues, we introduce GriddlyJS, a web-based Integrated Development Environment (IDE) based on the Griddly engine. GriddlyJS allows researchers to easily design and debug arbitrary, complex PCG grid-world environments, as well as visualize, evaluate, and record the performance of trained agent models. By connecting the RL workflow to the advanced functionality enabled by modern web standards, GriddlyJS allows publishing interactive agent-environment demos that reproduce experimental results directly to the web. To demonstrate the versatility of GriddlyJS, we use it to quickly develop a complex compositional puzzle-solving environment alongside arbitrary human-designed environment configurations and their solutions for use in a automatic curriculum learning and offline RL context. The GriddlyJS IDE is open source and freely available at https://griddly.ai.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YmacJv0i_UR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://griddly.ai" target="_blank" rel="nofollow noreferrer">http://griddly.ai</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_HLcjaVlqJ" data-number="123">
        <h4>
          <a href="/forum?id=_HLcjaVlqJ">
              JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search
          </a>


            <a href="/pdf?id=_HLcjaVlqJ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Archit_Bansal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Archit_Bansal1">Archit Bansal</a>, <a href="/profile?id=~Danny_Stoll1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danny_Stoll1">Danny Stoll</a>, <a href="/profile?id=~Maciej_Janowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maciej_Janowski1">Maciej Janowski</a>, <a href="/profile?id=~Arber_Zela1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arber_Zela1">Arber Zela</a>, <a href="/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#_HLcjaVlqJ-details-67" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_HLcjaVlqJ-details-67"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Joint Architecture and Hyperparameter Search, Neural Architecture Search, Hyperparameter Optimization, Surrogate Benchmark, Multi-fidelity, Multi-objective, Cost-aware</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present JAHS-Bench-201, the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search, built to also facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The past few years have seen the development of many benchmarks for Neural Architecture Search (NAS), fueling rapid progress in NAS research. However, recent work, that shows good hyperparameter settings can be more important than using the best architecture, calls for a shift in focus towards Joint Architecture and Hyperparameter Search (JAHS). Therefore, we present JAHS-Bench-201, the first collection of surrogate benchmarks for JAHS, built to also facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To the best of our knowledge, JAHS-Bench-201 is based on the most extensive dataset of neural network performance data in the public domain. It is composed of approximately 140 million million data points and 20 performance metrics for three deep learning tasks, while featuring a 14-dimensional search and fidelity space that extends the popular NAS-Bench-201 space. With JAHS-Bench-201, we hope to democratize research on JAHS and lower the barrier to entry of an extremely compute intensive field, e.g., by reducing the compute time to run a JAHS algorithm from 5 days to only a few seconds.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_HLcjaVlqJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">We provide all instructions and code at https://github.com/automl/jahs_bench_201.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">All relevant information can be accessed at https://github.com/automl/jahs_bench_201.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We release the code used to build our benchmark and perform our experiments under the MIT License (https://mit-license.org/), whereas we release data we created, including the performance metrics collected by us, the splits used to train, validate and test our surrogate models, and our surrogate models, under the CC BY 4.0 License (https://creativecommons.org/licenses/by/4.0/).</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="tTPVefaATp6" data-number="122">
        <h4>
          <a href="/forum?id=tTPVefaATp6">
              OccGen: Selection of Real-world Multilingual Parallel Data Balanced in Gender within Occupations
          </a>


            <a href="/pdf?id=tTPVefaATp6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Marta_R._Costa-juss%C3%A01" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marta_R._Costa-jussà1">Marta R. Costa-jussà</a>, <a href="/profile?id=~Christine_Basta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christine_Basta1">Christine Basta</a>, <a href="/profile?email=oriol%40batou.xyz" class="profile-link" data-toggle="tooltip" data-placement="top" title="oriol@batou.xyz">Oriol Domingo</a>, <a href="/profile?email=andreniyongabo%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="andreniyongabo@fb.com">André Niyongabo Rubungo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 08 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#tTPVefaATp6-details-415" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tTPVefaATp6-details-415"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Balanced Multilingual Data Set, Gender, Occupations, Machine Translation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the OccGen toolkit that builds multilingual parallel data sets balanced in gender within occupations. The toolkit is released together with two datasets in four high-resource languages and in a low-resource language (with English).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper describes the OccGen toolkit, which allows extracting multilingual parallel data balanced in gender within occupations. OccGen can extract datasets that reflect gender diversity (beyond binary) more fairly in society and explicitly mitigate occupational gender stereotypes. We propose two use cases that extract evaluation datasets for machine translation in four high-resource languages from different linguistic families and in a low-resource African language. Our analysis of these use cases shows that translation outputs in high-resource languages tend to worsen in female subsets (compared to males). This can be explained because less attention is paid to the source sentence. Then, more attention is given to the target prefix overgeneralising to the most frequent male forms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=tTPVefaATp6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mt-upc/OccGen_dataset" target="_blank" rel="nofollow noreferrer">https://github.com/mt-upc/OccGen_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/mt-upc/OccGen_dataset" target="_blank" rel="nofollow noreferrer">https://github.com/mt-upc/OccGen_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY-SA 3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="dh_MkX0QfrK" data-number="121">
        <h4>
          <a href="/forum?id=dh_MkX0QfrK">
              PDEBench: An Extensive Benchmark for Scientific Machine Learning
          </a>


            <a href="/pdf?id=dh_MkX0QfrK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Makoto_Takamoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Makoto_Takamoto1">Makoto Takamoto</a>, <a href="/profile?id=~Timothy_Praditia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Praditia1">Timothy Praditia</a>, <a href="/profile?id=~Raphael_Leiteritz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raphael_Leiteritz1">Raphael Leiteritz</a>, <a href="/profile?id=~Dan_MacKinlay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_MacKinlay1">Dan MacKinlay</a>, <a href="/profile?id=~Francesco_Alesiani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Alesiani1">Francesco Alesiani</a>, <a href="/profile?id=~Dirk_Pfl%C3%BCger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dirk_Pflüger1">Dirk Pflüger</a>, <a href="/profile?id=~Mathias_Niepert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathias_Niepert1">Mathias Niepert</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#dh_MkX0QfrK-details-828" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dh_MkX0QfrK-details-828"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Scientific Machine Learning, Benchmark, Partial Differential Equations, PINN, FNO, U-Net, Inverse problem</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a benckmark for Scientific Machine Learning </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning-based modeling of physical systems has gained increasing interest in recent years.
        Despite recent progress, there is still a lack of such benchmarks for scientific ML with sufficient volume and variety that are easy to use but still challenging and representative for a wide range of problems.
        In this paper, we introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs).
        PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines.
        Our proposed set of benchmark problems contributes in particular the following unique features:
        (1) A much wider range of PDEs than existing approaches, ranging from relatively common examples to more realistic and difficult ones;
        (2) much larger ready-to-use datasets than state-of-the-art, comprising multiple simulation-runs across varying initial or boundary conditions and model parameters;
        (3) and it provides easily extensible source codes with user-friendly APIs for data generation and baseline results with advanced machine learning models (FNO, U-Net, PINN,
        Gradient-based inverse method).
        PDEBench allows researchers to extend the dataset freely for their own purposes using a standardized API, and to compare the performance of their new models.
        Finally, we propose new metrics to help to understand and evaluate a given ML model in the context of scientific ML.
        With those metrics we identified tasks which the present ML methods cannot provide acceptable accuracy, and propose them as future challenge-task for the community.
        The code is available at https://github.com/pdebench/PDEBench .
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=dh_MkX0QfrK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pdebench/PDEBench" target="_blank" rel="nofollow noreferrer">https://github.com/pdebench/PDEBench</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">
        Baselines url
        https://github.com/pdebench/PDEBench

        Dataset permanent url
        https://darus.uni-stuttgart.de/dataverse/sciml_benchmark

        Temporary url
        PDEBench Dataset https://darus.uni-stuttgart.de/privateurl.xhtml?token=1be27526-348a-40ed-9fd0-c62f588efc01
        PDEBench Pre-Trained Models https://darus.uni-stuttgart.de/privateurl.xhtml?token=cd862f8c-8e1b-49d2-b4da-b35f8df5ac85

        Permanent url
        PDEBench Dataset https://darus.uni-stuttgart.de/dataset.xhtml?persistentId=doi:10.18419/darus-2986
        PDEBench Pre-Trained Models https://darus.uni-stuttgart.de/dataset.xhtml?persistentId=doi:10.18419/darus-2987

        Dataset DOI
        doi:10.18419/darus-2986
        doi:10.18419/darus-2987

        DOI url
        http://dx.doi.org/10.18419/darus-2986
        http://dx.doi.org/10.18419/darus-2987



        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT for solver code and baseline code
        NLE Academic License (Academic or non-profit organization noncommercial research use only) for selected code (one solver and baseline)
        Dataset license CC BY

        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="11RAWt2dqc6" data-number="120">
        <h4>
          <a href="/forum?id=11RAWt2dqc6">
              SAM, Sketch datA Model for leveraging ML in industrial Computer-Aided Design
          </a>


            <a href="/pdf?id=11RAWt2dqc6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fr%C3%A9d%C3%A9rique_Robin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frédérique_Robin1">Frédérique Robin</a>, <a href="/profile?email=julien.moreau%40ecl19.ec-lyon.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="julien.moreau@ecl19.ec-lyon.fr">julien moreau</a>, <a href="/profile?email=raphael.marc%40edf.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="raphael.marc@edf.fr">Raphaël Marc</a>, <a href="/profile?email=odilon.duranthon%40ens.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="odilon.duranthon@ens.fr">Odilon Duranthon</a>, <a href="/profile?id=~Marc_Lelarge2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Lelarge2">Marc Lelarge</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#11RAWt2dqc6-details-914" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="11RAWt2dqc6-details-914"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Computer-Aided Design, sketch, few-shot learning, data model, transformers</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">a new data model for CAD sketches enabling use of public datasets and few shot learning thanks to transformer-based architectures</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although Computer-Aided Design (CAD) has significantly boosted the production and re-design of objects, the CAD engineer is faced with models of increasing numbers and complexity. It is therefore crucial to develop new techniques to support the design of new CAD models, but also ease the reworking of old sketches. Machine learning (ML) could significantly speed up easily predictable tasks for a more efficient production of CAD models but several issues need to be addressed first. In the current situation, CAD models are not compatible, not shareable and moreover depending on their uses for simulation, construction or design have specific needs in term of conception, detail quality, etc.

        We propose a first step to integrate ML in an industrial context by concentrating our effort on CAD sketches leveraging SketchGraphs dataset made of real-world models collected over the internet. We propose a neutral data model called SAM (Sketch dAtA Model) for sketches written in python. SAM contains all the information about a sketch, allows to compare sketches and is ready for a direct use in a ML pipeline. To demonstrate how SAM enables Few Shot Learning for specific industrial CAD needs, we translate the full SketchGraphs dataset into SAM as well as a small proprietary dataset produced by Shaper (an open-source CAD modeler). With all sketches in the same format, we design criteria based on CAD expertise (complexity, degree of freedom...) to compare the datasets. Then we filter the large SketchGraphs dataset to match our target dataset and thus make it as relevant as possible for ML algorithms and for our tasks of interest. As a final step, we train a Transfomer-based architecture to infer missing constraints.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=11RAWt2dqc6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sketchai/preprocessing" target="_blank" rel="nofollow noreferrer">https://github.com/sketchai/preprocessing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://huggingface.co/datasets/sketchai/sam-dataset/tree/main" target="_blank" rel="nofollow noreferrer">https://huggingface.co/datasets/sketchai/sam-dataset/tree/main</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU Lesser General Public License v3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="oClYTPaxOVA" data-number="119">
        <h4>
          <a href="/forum?id=oClYTPaxOVA">
              OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing
          </a>


            <a href="/pdf?id=oClYTPaxOVA" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Berkan_Lafci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Berkan_Lafci1">Berkan Lafci</a>, <a href="/profile?id=~Firat_Ozdemir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Firat_Ozdemir1">Firat Ozdemir</a>, <a href="/profile?email=xl.deanben%40pharma.uzh.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="xl.deanben@pharma.uzh.ch">Xose Luis Dean-Ben</a>, <a href="/profile?email=daniel.razansky%40pharma.uzh.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="daniel.razansky@pharma.uzh.ch">Daniel Razansky</a>, <a href="/profile?id=~Fernando_Perez-Cruz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fernando_Perez-Cruz1">Fernando Perez-Cruz</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#oClYTPaxOVA-details-490" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oClYTPaxOVA-details-490"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Optoacoustics, Photoacoustics, Medical Imaging, Experimental Data</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide experimental and synthetic (simulated) OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optoacoustic (OA) imaging is based on excitation of biological tissues with nanosecond-duration laser pulses followed by subsequent detection of ultrasound waves generated via light-absorption-mediated thermoelastic expansion. OA imaging features a powerful combination between rich optical contrast and high resolution in deep tissues. This enabled the exploration of a number of attractive new applications both in clinical and laboratory settings. However, no standardized datasets generated with different types of experimental set-up and associated processing methods are available to facilitate advances in broader applications of OA in clinical settings. This complicates an objective comparison between new and established data processing methods, often leading to qualitative results and arbitrary interpretations of the data. In this paper, we provide both experimental and synthetic OA raw signals and reconstructed image domain datasets rendered with different experimental parameters and tomographic acquisition geometries. We further provide trained neural networks to tackle three important challenges related to OA image processing, namely accurate reconstruction under limited view tomographic conditions, removal of spatial undersampling artifacts and anatomical segmentation for improved image reconstruction. Specifically, we define 18 experiments corresponding to the aforementioned challenges as benchmarks to be used as a reference for the development of more advanced processing methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=oClYTPaxOVA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">OADAT DOI (public access not yet available, reviewers; please see our official comment for data access): https://www.research-collection.ethz.ch/handle/20.500.11850/551512
        OADAT (dataset documentation): https://github.com/berkanlafci/oadat
        oadat-evaluate (pretrained models, scripts to train modUNet, evaluating models, data loader examples): https://renkulab.io/gitlab/firat.ozdemir/oadat-evaluate
        oa-armsim (acoustic pressure maps simulation): https://renkulab.io/gitlab/firat.ozdemir/oa-armsim
        pyoat (forward mapping and OA image reconstruction): https://github.com/berkanlafci/pyoat</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC-BY-NC).
        pyoat package, oadat-evaluate, and oa-armsim projects are licensed under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wHvKAic4JFe" data-number="118">
        <h4>
          <a href="/forum?id=wHvKAic4JFe">
              SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency
          </a>


            <a href="/pdf?id=wHvKAic4JFe" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yan_Wang19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yan_Wang19">Yan Wang</a>, <a href="/profile?id=~Yuhang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhang_Li1">Yuhang Li</a>, <a href="/profile?id=~Ruihao_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruihao_Gong1">Ruihao Gong</a>, <a href="/profile?id=~Aishan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aishan_Liu1">Aishan Liu</a>, <a href="/profile?email=wangyanfei%40sensetime.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangyanfei@sensetime.com">Yanfei Wang</a>, <a href="/profile?email=hujian%40sensetime.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hujian@sensetime.com">Jian Hu</a>, <a href="/profile?email=soundbupt%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="soundbupt@gmail.com">Yongqiang Yao</a>, <a href="/profile?email=danache0405%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="danache0405@gmail.com">Tianzi Xiao</a>, <a href="/profile?id=~Fengwei_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fengwei_Yu1">Fengwei Yu</a>, <a href="/profile?id=~Xianglong_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xianglong_Liu3">Xianglong Liu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#wHvKAic4JFe-details-28" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wHvKAic4JFe-details-28"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  Extensive studies have shown that deep learning models are vulnerable to adversarial and natural noises, yet little is known about model robustness on noises caused by different system implementations. In this paper, we for the first time introduce SysNoise, a frequently occurred but often overlooked noise in the deep learning training-deployment cycle. In particular, SysNoise happens when the source training system switches to a disparate target system in deployments, where various tiny system mismatch adds up to a non-negligible difference. We first identify and classify SysNoise into three categories based on the inference stage; we then build a holistic benchmark to quantitatively measure the impact of SysNoise on 20+ models, comprehending image classification, object detection, and instance segmentation tasks. Our extensive experiments revealed that SysNoise could bring certain impacts on model robustness across different tasks and common mitigations like data augmentation and adversarial training show limited effects on it. Together, our findings open a new research topic and we hope this work will raise research attention to deep learning deployment systems accounting for model performance. We have open-sourced the benchmark at \url{https://modeltc.github.io/systemnoise_web}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wHvKAic4JFe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://modeltc.github.io/systemnoise_web" target="_blank" rel="nofollow noreferrer">https://modeltc.github.io/systemnoise_web</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://modeltc.github.io/systemnoise_web/#dataset" target="_blank" rel="nofollow noreferrer">https://modeltc.github.io/systemnoise_web/#dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">   Copyright [2022] [SenseTime]

           Licensed under the Apache License, Version 2.0 (the "License");
           you may not use this file except in compliance with the License.
           You may obtain a copy of the License at

             http://www.apache.org/licenses/LICENSE-2.0

           Unless required by applicable law or agreed to in writing, software
           distributed under the License is distributed on an "AS IS" BASIS,
           WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
           See the License for the specific language governing permissions and
           limitations under the License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jYX_QQuxy65" data-number="117">
        <h4>
          <a href="/forum?id=jYX_QQuxy65">
              SC2EGSet: StarCraft II Esport Replay and Game-state Dataset
          </a>


            <a href="/pdf?id=jYX_QQuxy65" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andrzej_Bia%C5%82ecki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrzej_Białecki1">Andrzej Białecki</a>, <a href="/profile?email=njakubowska%40swps.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="njakubowska@swps.edu.pl">Natalia Jakubowska</a>, <a href="/profile?email=pawel.dobrowolski%40psych.pan.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="pawel.dobrowolski@psych.pan.pl">Paweł Dobrowolski</a>, <a href="/profile?email=bialpio%40o2.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="bialpio@o2.pl">Piotr Białecki</a>, <a href="/profile?email=leafnode%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="leafnode@gmail.com">Leszek Krupiński</a>, <a href="/profile?email=andszc3%40st.amu.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="andszc3@st.amu.edu.pl">Andrzej Szczap</a>, <a href="/profile?email=robert.bialecki%40awf.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="robert.bialecki@awf.edu.pl">Robert Białecki</a>, <a href="/profile?email=jan.gajewski%40awf.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="jan.gajewski@awf.edu.pl">Jan Gajewski</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#jYX_QQuxy65-details-517" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jYX_QQuxy65-details-517"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">StarCraft II, esports, machine learning, dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Largest publicly available collection of esports tournament replays, and pre-processed dataset in StarCraft II.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As a relatively new form of sport, esports offers unparalleled data availability. Despite the vast amounts of data that are generated by game engines, it can be challenging to extract them and verify their integrity for the purposes of practical and scientific use.

        Our work aims to open esports to a broader scientific community by supplying raw and pre-processed files from StarCraft II esports tournaments. These files can be used in statistical and machine learning modeling tasks and related to various laboratory-based measurements (e.g., behavioral tests, brain imaging). We have gathered publicly available game-engine generated "replays" of tournament matches and performed data extraction and cleanup using low-level application programming interface (API) parser library.

        Additionally, we open-sourced and published all the custom tools that were developed in the process of creating our dataset. These tools include a PyTorch and PyTorch Lightning API abstractions to load and model the data.

        Our dataset contains replays from major and premiere StarCraft II tournaments since 2016. To prepare the dataset, we processed 55 tournament "replaypacks" that contained 17930 files with game-state information. Based on initial investigation of available StarCraft II datasets we observed that our dataset is the largest publicly available source of StarCraft II esports data upon its publication.

        Analysis of the extracted data holds promise for further Artificial Intelligence (AI), Machine Learning (ML), psychological, Human-Computer Interaction (HCI), and sports-related studies in a variety of supervised and self-supervised tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jYX_QQuxy65&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Single File Dataset Download (Microsoft OneDrive): https://1drv.ms/u/s!AnSP3aYGVP7lgctJCfgHwf6d2YZq0g?e=6BiWEe Dataset Repository (Zenodo): https://doi.org/10.5281/zenodo.5503997</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We provide single file dataset download to increase the ease of review. This access URL will be deprecated and our open-source application programming interface (API) supports data download, data extraction, access and manipulation.

        Single File Dataset Download (Microsoft OneDrive):
        https://1drv.ms/u/s!AnSP3aYGVP7lgctJCfgHwf6d2YZq0g?e=6BiWEe

        Dataset Repository (cited in text, Zenodo):
        https://doi.org/10.5281/zenodo.5503997

        API to load and access the dataset via PyTorch or PyTorch Lightning (cited in text):
        https://github.com/Kaszanas/SC2EGSet_Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">There is no embargo on the dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset License (as included in the dataset repository)
        Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)

        Various custom tools that we developed have their separate licenses, these are publicly available (GitHub, Zenodo) and properly cited in text.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Zx5qJzNesn0" data-number="116">
        <h4>
          <a href="/forum?id=Zx5qJzNesn0">
              Learning Long-Term Crop Management Strategies with CyclesGym
          </a>


            <a href="/pdf?id=Zx5qJzNesn0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Matteo_Turchetta2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matteo_Turchetta2">Matteo Turchetta</a>, <a href="/profile?id=~Luca_Corinzia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luca_Corinzia1">Luca Corinzia</a>, <a href="/profile?id=~Scott_Sussex1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Sussex1">Scott Sussex</a>, <a href="/profile?email=amanda.burton%40agroscope.admin.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="amanda.burton@agroscope.admin.ch">Amanda Burton</a>, <a href="/profile?email=juan.herrera%40agroscope.admin.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="juan.herrera@agroscope.admin.ch">Juan Herrera</a>, <a href="/profile?id=~Ioannis_N._Athanasiadis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioannis_N._Athanasiadis1">Ioannis N. Athanasiadis</a>, <a href="/profile?id=~Joachim_M._Buhmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joachim_M._Buhmann1">Joachim M. Buhmann</a>, <a href="/profile?id=~Andreas_Krause1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Krause1">Andreas Krause</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Zx5qJzNesn0-details-139" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zx5qJzNesn0-details-139"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, Sustainable agriculture</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A novel reinforcement learning environment for long-term sustainable smart agriculture.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To improve the sustainability and resilience of modern food systems, designing improved crop management strategies is crucial. The increasing abundance of data on agricultural systems suggests that future strategies could benefit from adapting to environmental conditions, but how to design these adaptive policies poses a new frontier. A natural technique for learning policies in these kinds of sequential decision-making problems is reinforcement learning (RL). To obtain the large number of samples required to learn effective RL policies, existing work has used mechanistic crop growth models (CGMs) as simulators. These solutions focus on single-year simulations specifically for learning strategies for a single agricultural management practice. However, to learn sustainable long-term policies we must be able to train in multi-year environments, with multiple crops, and consider a wider array of management techniques. We introduce CYCLESGYM, an RL environment based on the multi-year, multi-crop CGM Cycles. CYCLESGYM allows for long-term planning in agroecosystems, provides modular state spaces and rewards, and allows for complex actions. For RL researchers, this is a novel benchmark to investigate issues arising in real-world applications, including meta-learning in RL, contextual RL, and more. For agronomists, we demonstrate the potential of RL as a powerful optimization tool for agricultural systems management in multi-year case studies on N fertilization and crop planning scenarios.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Zx5qJzNesn0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/kora-labs/cyclesgym" target="_blank" rel="nofollow noreferrer">https://github.com/kora-labs/cyclesgym</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD 3-Clause License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="c7f9uoPnzgE" data-number="115">
        <h4>
          <a href="/forum?id=c7f9uoPnzgE">
              Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark
          </a>


            <a href="/pdf?id=c7f9uoPnzgE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiaxi_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaxi_Gu1">Jiaxi Gu</a>, <a href="/profile?id=~Xiaojun_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojun_Meng1">Xiaojun Meng</a>, <a href="/profile?id=~Guansong_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guansong_Lu1">Guansong Lu</a>, <a href="/profile?id=~Lu_Hou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Hou2">Lu Hou</a>, <a href="/profile?id=~Minzhe_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minzhe_Niu1">Minzhe Niu</a>, <a href="/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>, <a href="/profile?id=~Lewei_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lewei_Yao1">Lewei Yao</a>, <a href="/profile?id=~Runhui_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runhui_Huang1">Runhui Huang</a>, <a href="/profile?id=~Wei_Zhang45" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Zhang45">Wei Zhang</a>, <a href="/profile?id=~Xin_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Jiang1">Xin Jiang</a>, <a href="/profile?id=~Chunjing_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chunjing_Xu1">Chunjing Xu</a>, <a href="/profile?id=~Hang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Xu1">Hang Xu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#c7f9uoPnzgE-details-835" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c7f9uoPnzgE-details-835"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision-language pre-training, Chinese cross-modal pre-training, Classification and retrieval benchmarks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A large-scale Chinese cross-modal dataset, called Wukong, containing 100 million image-text pairs is released. Models with either global similarity or token-wise similarity are pre-trained and benchmarked on extensive downstream tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_\text{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to https://wukong-dataset.github.io/wukong-dataset/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=c7f9uoPnzgE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://wukong-dataset.github.io/wukong-dataset/" target="_blank" rel="nofollow noreferrer">https://wukong-dataset.github.io/wukong-dataset/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://wukong-dataset.github.io/wukong-dataset/" target="_blank" rel="nofollow noreferrer">https://wukong-dataset.github.io/wukong-dataset/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="BOgrizTCTm6" data-number="114">
        <h4>
          <a href="/forum?id=BOgrizTCTm6">
              MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models
          </a>


            <a href="/pdf?id=BOgrizTCTm6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Paul_Pu_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Pu_Liang1">Paul Pu Liang</a>, <a href="/profile?id=~Yiwei_Lyu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiwei_Lyu1">Yiwei Lyu</a>, <a href="/profile?id=~Gunjan_Chhablani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunjan_Chhablani1">Gunjan Chhablani</a>, <a href="/profile?id=~Nihal_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nihal_Jain1">Nihal Jain</a>, <a href="/profile?id=~Zihao_Deng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zihao_Deng2">Zihao Deng</a>, <a href="/profile?id=~Xingbo_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingbo_Wang1">Xingbo Wang</a>, <a href="/profile?id=~Louis-Philippe_Morency1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis-Philippe_Morency1">Louis-Philippe Morency</a>, <a href="/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#BOgrizTCTm6-details-446" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BOgrizTCTm6-details-446"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multimodal learning, representation learning, interpretation, visualization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">MultiViz is a benchmark for visualizing and understanding the behavior of multimodal models across 4 stages: unimodal importance, cross-modal interactions, multimodal representations, and multimodal prediction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=BOgrizTCTm6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pliang279/MultiViz" target="_blank" rel="nofollow noreferrer">https://github.com/pliang279/MultiViz</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/pliang279/MultiViz" target="_blank" rel="nofollow noreferrer">https://github.com/pliang279/MultiViz</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="BubxnHpuMbG" data-number="113">
        <h4>
          <a href="/forum?id=BubxnHpuMbG">
              EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine
          </a>


            <a href="/pdf?id=BubxnHpuMbG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiayi_Weng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayi_Weng1">Jiayi Weng</a>, <a href="/profile?id=~Min_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Min_Lin1">Min Lin</a>, <a href="/profile?id=~Shengyi_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyi_Huang1">Shengyi Huang</a>, <a href="/profile?id=~Bo_Liu17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Liu17">Bo Liu</a>, <a href="/profile?id=~Denys_Makoviichuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denys_Makoviichuk1">Denys Makoviichuk</a>, <a href="/profile?id=~Viktor_Makoviychuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Viktor_Makoviychuk1">Viktor Makoviychuk</a>, <a href="/profile?id=~Zichen_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zichen_Liu1">Zichen Liu</a>, <a href="/profile?id=~Yufan_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yufan_Song2">Yufan Song</a>, <a href="/profile?id=~Ting_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ting_Luo1">Ting Luo</a>, <a href="/profile?id=~Yukun_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yukun_Jiang1">Yukun Jiang</a>, <a href="/profile?id=~Zhongwen_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongwen_Xu1">Zhongwen Xu</a>, <a href="/profile?id=~Shuicheng_YAN3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuicheng_YAN3">Shuicheng YAN</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#BubxnHpuMbG-details-673" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BubxnHpuMbG-details-673"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Highly Parallel Reinforcement Learning Environment Execution Engine</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others aim to improve the system's overall throughput. In this paper, we try to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop, and a modest workstation, to a high-end machine like NVIDIA DGX-A100. On a high-end machine, EnvPool achieves 1 million frames per second for the environment execution on Atari environments and 3 million frames per second on MuJoCo environments. When running on a laptop, the speed of EnvPool is 2.8 times of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has the great potential to become the de facto RL environment execution engine. Example runs show that it takes only 5 minutes to train Atari Pong and MuJoCo Ant, both on a laptop.  Envpool has already been open-sourced at https://github.com/sail-sg/envpool.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=BubxnHpuMbG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sail-sg/envpool" target="_blank" rel="nofollow noreferrer">https://github.com/sail-sg/envpool</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache2</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1s3exTIb-1p" data-number="112">
        <h4>
          <a href="/forum?id=1s3exTIb-1p">
              Referring Image Matting
          </a>


            <a href="/pdf?id=1s3exTIb-1p" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jizhizi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jizhizi_Li1">Jizhizi Li</a>, <a href="/profile?id=~Jing_Zhang17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Zhang17">Jing Zhang</a>, <a href="/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dacheng_Tao1">Dacheng Tao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#1s3exTIb-1p-details-20" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1s3exTIb-1p-details-20"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">image matting, cross modal, dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we propose a new task Referring Image Matting by presenting the first large-scale dataset RefMatte. With the dataset and method, we are able to extract an accurate soft mask for an object in the image described by natural language.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Image matting refers to extracting the accurate foregrounds in the image. Current automatic methods tend to extract all the salient objects in the image indiscriminately. In this paper, we propose a new task named Referring Image Matting (RIM), referring to extracting the meticulous alpha matte of the specific object that can best match the given natural language description. However, prevalent visual grounding methods are all limited to the segmentation level, probably due to the lack of high-quality datasets for RIM. To fill the gap, we establish the first large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to produce synthetic images on top of current public high-quality matting foregrounds with flexible logics and re-labelled diverse attributes. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions, which can be further extended easily in the future. Besides this, we also construct a real-world test set with manually generated phrase annotations consisting of 100 natural images to further evaluate the generalization of RIM models. We first define the task of RIM in two settings, i.e., prompt-based and expression-based, and then benchmark several representative methods together with specific model designs for image matting. The results provide empirical insights into the limitations of existing methods as well as possible solutions. We believe the new task RIM along with the RefMatte dataset will open new research directions in this area and facilitate future studies. The dataset and code will be made publicly available at https://github.com/JizhiziLi/RIM.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1s3exTIb-1p&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/JizhiziLi/RIM" target="_blank" rel="nofollow noreferrer">https://github.com/JizhiziLi/RIM</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/JizhiziLi/RIM" target="_blank" rel="nofollow noreferrer">https://github.com/JizhiziLi/RIM</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset and code are under the Attribution-NonCommercial CC BY-NC license. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xUqpKEl0n2" data-number="111">
        <h4>
          <a href="/forum?id=xUqpKEl0n2">
              WikiDT:  Visual-based Table Recognition and Question Answering Dataset
          </a>


            <a href="/pdf?id=xUqpKEl0n2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hui_Shi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hui_Shi3">Hui Shi</a>, <a href="/profile?id=~Yusheng_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusheng_Xie1">Yusheng Xie</a>, <a href="/profile?id=~Luis_Goncalves1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luis_Goncalves1">Luis Goncalves</a>, <a href="/profile?id=~Sicun_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sicun_Gao1">Sicun Gao</a>, <a href="/profile?id=~Jishen_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jishen_Zhao1">Jishen Zhao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">03 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#xUqpKEl0n2-details-247" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xUqpKEl0n2-details-247"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Table Question Answering, Visual Question Answering, Table Recognition, Table Structure Recognition, Table Detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduced a multi-modal dataset with images, tables, QA and SQL, which advance the tasks including table extraction, table retrieval, and table/visual question answering</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce the WikiDT dataset which comprises a table extraction and a question-answering task. The dataset features a large collection of images (38k) with diverse tables (102k), which serve as the corpus for the question-answering task. The creation of \myname is fully automated from existing datasets with verified heuristics, and therefore minimizes the human errors. A novel focus of \myname and its designed goal is to answer questions that require locating the target information fragment and in-depth reasoning, given web-style document images. The WikiDT dataset provides multilevel labels that allow the decomposition and diagnosis of many modern ``end-to-end'' models. We evaluate current state-of-the-art models for table extraction and question answering tasks and illustrate that the WikiDT introduces complementary challenges to the existing datasets in both tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=xUqpKEl0n2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://www.kaggle.com/datasets/wikidocumentdataset/WikiDT-TableRecognition
        https://www.kaggle.com/datasets/wikidocumentdataset/questionanswering</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">New dataset is released with CC BY SA 3.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mV4EKzUVI96" data-number="109">
        <h4>
          <a href="/forum?id=mV4EKzUVI96">
              APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking
          </a>


            <a href="/pdf?id=mV4EKzUVI96" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuxiang_Yang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxiang_Yang3">Yuxiang Yang</a>, <a href="/profile?id=~Junjie_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junjie_Yang4">Junjie Yang</a>, <a href="/profile?id=~Yufei_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yufei_Xu1">Yufei Xu</a>, <a href="/profile?id=~Jing_Zhang17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Zhang17">Jing Zhang</a>, <a href="/profile?id=~Long_Lan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Lan2">Long Lan</a>, <a href="/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dacheng_Tao1">Dacheng Tao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mV4EKzUVI96-details-89" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mV4EKzUVI96-details-89"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. The lack of APT datasets hinders the development and evaluation of video-based animal pose estimation and tracking methods, limiting the applications in real world, e.g., understanding animal behavior in wildlife conservation. To fill this gap, we make the first step and propose APT-36K, i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a useful animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. The code and dataset will be made publicly available at https://github.com/pandorgan/APT-36K.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mV4EKzUVI96&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pandorgan/APT-36K" target="_blank" rel="nofollow noreferrer">https://github.com/pandorgan/APT-36K</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/pandorgan/APT-36K" target="_blank" rel="nofollow noreferrer">https://github.com/pandorgan/APT-36K</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xC5DF3ImHxk" data-number="108">
        <h4>
          <a href="/forum?id=xC5DF3ImHxk">
              Data-Driven Network Neuroscience: On Data Collection and Benchmark
          </a>


            <a href="/pdf?id=xC5DF3ImHxk" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~David_Tse_Jung_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Tse_Jung_Huang1">David Tse Jung Huang</a>, <a href="/profile?email=sophi.sg%40ntu.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="sophi.sg@ntu.edu.sg">Sophi Shilpa Gururajapathy</a>, <a href="/profile?id=~Yiping_Ke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiping_Ke1">Yiping Ke</a>, <a href="/profile?id=~Miao_Qiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Miao_Qiao1">Miao Qiao</a>, <a href="/profile?email=alan.wang%40auckland.ac.nz" class="profile-link" data-toggle="tooltip" data-placement="top" title="alan.wang@auckland.ac.nz">Alan Wang</a>, <a href="/profile?id=~Haribalan_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haribalan_Kumar1">Haribalan Kumar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#xC5DF3ImHxk-details-116" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xC5DF3ImHxk-details-116"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Brain networks, graph, functional connectivity</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents a comprehensive collection of functional human brain network data for graph-based analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a comprehensive and quality collection of functional human brain network data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images of the brain have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains richer structural and positional information that traditional examination methods are unable to capture. However, the lack of brain networks data transformed from functional MRI images prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps and the exhaustive computation required to convert data from MRI images into brain networks. We bridge this gap by collecting a large amount of available MRI images from existing studies, working with domain experts to make sensible design choices, and preprocessing the MRI images to produce a collection of brain network datasets. The datasets originate from 5 different sources, cover 3 neurodegenerative conditions, and consists of a total of 2,606 subjects. We test our graph datasets on 5 machine learning models commonly used in neuroscience to validate the data quality and to provide domain baselines to lower the barrier to entry and promote the research in this interdisciplinary field. We release our complete preprocessing details, experimental codes and brain network data.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=xC5DF3ImHxk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>