<ul class="list-unstyled submissions-list">
    <li class="note " data-id="6E66VyzpCOd" data-number="218">
        <h4>
          <a href="/forum?id=6E66VyzpCOd">
              A Large Scale Dataset for Image Restoration
          </a>


            <a href="/pdf?id=6E66VyzpCOd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yawei_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yawei_Li1">Yawei Li</a>, <a href="/profile?id=~Kai_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Zhang8">Kai Zhang</a>, <a href="/profile?id=~Jingyun_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingyun_Liang1">Jingyun Liang</a>, <a href="/profile?id=~Jiezhang_Cao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiezhang_Cao2">Jiezhang Cao</a>, <a href="/profile?id=~Ce_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ce_Liu3">Ce Liu</a>, <a href="/profile?id=~Rui_Gong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Gong2">Rui Gong</a>, <a href="/profile?id=~Yulun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulun_Zhang1">Yulun Zhang</a>, <a href="/profile?id=~Hao_Tang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Tang6">Hao Tang</a>, <a href="/profile?id=~Yun_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun_Liu1">Yun Liu</a>, <a href="/profile?id=~Denis_Demandolx1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denis_Demandolx1">Denis Demandolx</a>, <a href="/profile?id=~Rakesh_Ranjan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rakesh_Ranjan2">Rakesh Ranjan</a>, <a href="/profile?id=~Radu_Timofte1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Radu_Timofte1">Radu Timofte</a>, <a href="/profile?id=~Luc_Van_Gool1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luc_Van_Gool1">Luc Van Gool</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#6E66VyzpCOd-details-270" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6E66VyzpCOd-details-270"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">image restoration, large scale dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a large scale dataset for image restoration</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  The aim of this paper is to propose a large scale dataset for image restoration (LSDIR). Recent work in image restoration has  focused on the design of deep neural networks. The datasets used to train these networks `only' contain some thousands of images, which is still incomparable with the large scale datasets for other vision tasks such as visual recognition and object detection. The small training set limits the performance of image restoration networks. To solve that problem, we collect high-resolution (HR) images from the Internet for image restoration. To ensure the pixel-level quality of the collected dataset, experts in image restoration were invited to manually inspect each of the collected image and remove the low-quality ones. The final dataset contains 84,991 high-quality training images, 1,000 validation images, and 1,000 test images. In addition, we showed that the benefit of the large scale dataset is that the model capacity of large networks could be fully exploited by training on the dataset with significantly increased patch size and prolonged training iterations. We also benchmarked several typical deep neural networks with the proposed dataset. The experimental results on image super-resolution (SR) and image denoising show that the proposed dataset could lead to a significant performance boost.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=6E66VyzpCOd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://data.vision.ee.ethz.ch/yawli/index.html" target="_blank" rel="nofollow noreferrer">https://data.vision.ee.ethz.ch/yawli/index.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://data.vision.ee.ethz.ch/yawli/index.html" target="_blank" rel="nofollow noreferrer">https://data.vision.ee.ethz.ch/yawli/index.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Data: Creative Commons Attribution 4.0 License (https://creativecommons.org/licenses/by/4.0/)
        Code: Apache License 2.0 (https://github.com/ofsoundof/lsdir_tools/blob/main/LICENSE)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="CwXxfUe-oy-" data-number="216">
        <h4>
          <a href="/forum?id=CwXxfUe-oy-">
              Taxonomy of Benchmarks in Graph Representation Learning
          </a>


            <a href="/pdf?id=CwXxfUe-oy-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=liurenmi%40msu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liurenmi@msu.edu">Renming Liu</a>, <a href="/profile?id=~Semih_Cant%C3%BCrk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Semih_Cantürk1">Semih Cantürk</a>, <a href="/profile?id=~Frederik_Wenkel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederik_Wenkel1">Frederik Wenkel</a>, <a href="/profile?id=~Sarah_McGuire1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_McGuire1">Sarah McGuire</a>, <a href="/profile?id=~Xinyi_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyi_Wang4">Xinyi Wang</a>, <a href="/profile?id=~Anna_Little1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Little1">Anna Little</a>, <a href="/profile?id=~Leslie_O%26%23x27%3BBray1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leslie_O'Bray1">Leslie O'Bray</a>, <a href="/profile?id=~Michael_Perlmutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Perlmutter1">Michael Perlmutter</a>, <a href="/profile?id=~Bastian_Rieck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bastian_Rieck1">Bastian Rieck</a>, <a href="/profile?id=~Matthew_Hirn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Hirn1">Matthew Hirn</a>, <a href="/profile?id=~Guy_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guy_Wolf1">Guy Wolf</a>, <a href="/profile?id=~Ladislav_Rampasek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ladislav_Rampasek1">Ladislav Rampasek</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#CwXxfUe-oy--details-467" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CwXxfUe-oy--details-467"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph datasets, dataset taxonomy, graph representation learning, graph classification, node classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a systematic approach for categorization of graph learning datasets based on their empirical properties.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Networks (GNNs) extend the success of neural networks to graph-structured data by accounting for their intrinsic geometry. While extensive research has been done on developing GNN models with superior performance according to a collection of graph representation learning benchmarks, it is currently not well understood what aspects of a given model are probed by them. For example, to what extent do they test the ability of a model to leverage graph structure vs. node features? Here, we develop a principled approach to taxonomize benchmarking datasets according to a $\textit{sensitivity profile}$ that is based on how much GNN performance changes due to a collection of graph perturbations. Our data-driven analysis provides a deeper understanding of which benchmarking data characteristics are leveraged by GNNs. Consequently, our taxonomy can aid in selection and development of adequate graph benchmarks, and better informed evaluation of future GNN methods. Finally, our approach is designed to be extendable to multiple graph prediction task types and future datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=CwXxfUe-oy-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/G-Taxonomy-Workgroup/GTaxoGym" target="_blank" rel="nofollow noreferrer">https://github.com/G-Taxonomy-Workgroup/GTaxoGym</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2022

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rku0AcpYr50" data-number="215">
        <h4>
          <a href="/forum?id=rku0AcpYr50">
              Code4ML: a Large-scale Dataset of annotated Machine Learning Code
          </a>


            <a href="/pdf?id=rku0AcpYr50" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ekaterina_Trofimova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekaterina_Trofimova1">Ekaterina Trofimova</a>, <a href="/profile?email=anastasiadrozdova57%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="anastasiadrozdova57@gmail.com">Anastasia Drozdova</a>, <a href="/profile?email=p.guseva00%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="p.guseva00@gmail.com">Polina Guseva</a>, <a href="/profile?email=aniezka.sherbakova%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="aniezka.sherbakova@gmail.com">Anna Scherbakova</a>, <a href="/profile?id=~Andrey_E_Ustyuzhanin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_E_Ustyuzhanin1">Andrey E Ustyuzhanin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#rku0AcpYr50-details-834" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rku0AcpYr50-details-834"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, programming language processing, notebooks, markup</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A dataset of marked-up code snippets collected from public ML notebooks solving Kaggle challenges</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automated code generation is a gaining momentum topic. However, the specific task of code synthesis for machine learning (ML) tasks is still unexplored. It is primarily hindered by the lack of relevant data. We present the Code4ML annotated dataset to cover this gap, embracing a corpus of Python code snippets, task summaries, and data descriptions publicly available from the Kaggle - the leading platform for hosting data science competitions. The corpus consists of $\approx2$ million snippets of ML code collected from $\approx100$ thousand Jupyter notebooks. All of the snippets are annotated either by a human or an algorithm. The framework for contributing to the annotation is also presented. Code4ML dataset can help solve various problems, including semantic code classification, code autocompletion, and code generation for an ML task expressed in natural language.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rku0AcpYr50&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6607065" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6607065</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6607065" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6607065</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="745krxayrcW" data-number="214">
        <h4>
          <a href="/forum?id=745krxayrcW">
              A Benchmark for Modeling Violation-of-Expectation in Physical Reasoning Across Event Categories
          </a>


            <a href="/pdf?id=745krxayrcW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Arijit_Dasgupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arijit_Dasgupta1">Arijit Dasgupta</a>, <a href="/profile?id=~Jiafei_Duan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiafei_Duan1">Jiafei Duan</a>, <a href="/profile?id=~Marcelo_H_Ang_Jr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcelo_H_Ang_Jr1">Marcelo H Ang Jr</a>, <a href="/profile?email=yl8476%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yl8476@nyu.edu">Yi Lin</a>, <a href="/profile?id=~Su-hua_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Su-hua_Wang1">Su-hua Wang</a>, <a href="/profile?email=rbaillar%40illinois.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rbaillar@illinois.edu">Renée Baillargeon</a>, <a href="/profile?id=~Cheston_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheston_Tan1">Cheston Tan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#745krxayrcW-details-4" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="745krxayrcW-details-4"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Computer Vision, Violation-of-Expectation, Physical Reasoning, 3D Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A New Computer Vision Dataset and Benchmark for the Violation-of-Expectation Paradigm in Physical Reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work in computer vision and cognitive reasoning has given rise to an increasing adoption of the Violation-of-Expectation (VoE) paradigm in synthetic datasets. Inspired by infant psychology, researchers are now evaluating a model’s ability to label scenes as either expected or surprising with knowledge of only expected scenes. However, existing VoE-based 3D datasets in physical reasoning provide mainly vision data with little to no heuristics or inductive biases. Cognitive models of physical reasoning reveal infants create high-level abstract representations of objects and interactions. Capitalizing on this knowledge, we established a benchmark to study physical reasoning by curating a novel large-scale synthetic 3D VoE dataset armed with ground-truth heuristic labels of causally relevant features and rules. To validate our dataset in five event categories of physical reasoning, we benchmarked and analyzed human performance. We also proposed the Object File Physical Reasoning Network (OFPR-Net) which exploits the dataset's novel heuristics to outperform our baseline and ablation models. The OFPR-Net is also flexible in learning an alternate physical reality, showcasing its ability to learn universal causal relationships in physical reasoning to create systems with better interpretability.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=745krxayrcW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://arijitnoobstar.github.io/sites/voe.html" target="_blank" rel="nofollow noreferrer">https://arijitnoobstar.github.io/sites/voe.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://arijitnoobstar.github.io/sites/voe.html" target="_blank" rel="nofollow noreferrer">https://arijitnoobstar.github.io/sites/voe.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jIIzJaMbfw" data-number="213">
        <h4>
          <a href="/forum?id=jIIzJaMbfw">
              StrokeRehab: A Benchmark Dataset for Sub-second Action Identification
          </a>


            <a href="/pdf?id=jIIzJaMbfw" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Aakash_Kaku1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aakash_Kaku1">Aakash Kaku</a>, <a href="/profile?id=~Kangning_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kangning_Liu1">Kangning Liu</a>, <a href="/profile?id=~Avinash_Parnandi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Avinash_Parnandi1">Avinash Parnandi</a>, <a href="/profile?id=~Haresh_Rengaraj_Rajamohan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haresh_Rengaraj_Rajamohan1">Haresh Rengaraj Rajamohan</a>, <a href="/profile?email=kv942%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kv942@nyu.edu">Kannan Venkataramanan</a>, <a href="/profile?email=anitavenkatesan1190%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="anitavenkatesan1190@gmail.com">Anita Venkatesan</a>, <a href="/profile?email=awirtanen%40bennington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="awirtanen@bennington.edu">Audre Wirtanen</a>, <a href="/profile?email=ngp238%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ngp238@nyu.edu">Natasha Pandit</a>, <a href="/profile?email=heidi.schambra%40nyulangone.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="heidi.schambra@nyulangone.org">Heidi Schambra</a>, <a href="/profile?id=~Carlos_Fernandez-Granda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlos_Fernandez-Granda1">Carlos Fernandez-Granda</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 08 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#jIIzJaMbfw-details-350" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jIIzJaMbfw-details-350"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deep learning, Action segmentation, Action recognition, Benchmark dataset, Fine-grained actions, Stroke rehabilitation, Seq2seq models, sequence prediction</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a new benchmark dataset for the identification of subtle and short-duration actions. We also propose a novel seq2seq approach, which outperforms the existing methods on the new as well as standard benchmark datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Automatic action identification from video and kinematic data is an important machine learning problem with applications ranging from robotics to smart health. Most existing works focus on identifying coarse actions such as running, climbing,  or cutting vegetables, which have relatively long durations and a complex series of motions. This is an important limitation for applications that require identification of more elemental motions at high temporal resolution. For example, in the rehabilitation of arm impairment after stroke, quantifying the training dose (number of repetitions) requires differentiating motions with sub-second durations. Our goal is to bridge this gap. To this end, we introduce a large-scale, multimodal dataset, StrokeRehab, as a new action-recognition benchmark that includes elemental short-duration actions labeled at a high temporal resolution. StrokeRehab consists of a high-quality inertial measurement unit sensor and video data of 51 stroke-impaired patients and 20 healthy subjects performing activities of daily living like feeding, brushing teeth, etc. Because it contains data from both healthy and impaired individuals, StrokeRehab can be used to study the influence of distribution shift in action-recognition tasks. When evaluated on StrokeRehab, current state-of-the-art models for action segmentation produce noisy predictions, which reduces their accuracy in identifying the corresponding sequence of actions. To address this, we propose a novel approach for high-resolution action identification, inspired by speech-recognition techniques, which is based on a sequence-to-sequence model that directly predicts the sequence of actions. This approach outperforms current state-of-the-art methods on StrokeRehab, as well as on the standard benchmark datasets 50Salads, Breakfast, and Jigsaws.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jIIzJaMbfw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://simtk.org/projects/primseq" target="_blank" rel="nofollow noreferrer">https://simtk.org/projects/primseq</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">The dataset can be access via https://simtk.org/. One can create a free account on simtk and access the dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://simtk.org/projects/primseq
        https://github.com/aakashrkaku/seq2seq_hrar
        Sample data: https://drive.google.com/drive/folders/1_a48XeRjFRdwaiaQXAV3tvAYb-4VmDbM?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We have released a part of the dataset. There are three parts of the data: sensor data for stroke-impaired patients, extracted video features for stroke-impaired patients, sensor data for healthy subjects. The first part is already released. The remaining two parts would be released by the end of the review period. We have provided sample data from reviewers' reference.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="43KCE87kCZy" data-number="212">
        <h4>
          <a href="/forum?id=43KCE87kCZy">
              SNOW: A Large-scale Synthetic Histopathological Dataset Generated by Off-the-shelf Models
          </a>


            <a href="/pdf?id=43KCE87kCZy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kexin_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kexin_Ding1">Kexin Ding</a>, <a href="/profile?id=~Mu_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mu_Zhou2">Mu Zhou</a>, <a href="/profile?id=~Shaoting_Zhang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaoting_Zhang5">Shaoting Zhang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#43KCE87kCZy-details-743" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="43KCE87kCZy-details-743"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Synthetic dataset generartion, Medical imaging, Pathology, Health-care AI</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Large-scale Synthetic Histopathological Dataset Generated by Off-the-shelf Models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The success of training computer-vision models is heavily relied on the support of large-scale, real-world images. Yet such a dataset is scarce in histopathology due to the privacy protection and annotation burden. To aid computational analysis of pathology, synthetic data generation and annotation present a cost-effective means to quickly enable clinical data diversity and can boost model supervision at all stages. In this study, we introduce a large-scale synthetic histopathological image dataset paired with weak annotation, termed as synthetic nuclei and annotation wizard (SNOW). SNOW is collected via a standardized workflow by applying the off-the-shelf generator and annotator to refine nuclei segmentation performance. SNOW contains overall 20k tiles and 1,448,522 annotated nuclei with CC-BY license and complete attribution metadata. We show that SNOW can be used on both supervised and semi-supervised training (e.g., self-training) settings. Especially, the self-training training pipeline leverages the mix of labeled and unlabeled synthetic data to boost the segmentation performance without adding human efforts. We verify the performance of synthetics dataset by training models only on SNOW and evaluating the model performance on an independent real-world dataset. We show compelling results that synthetic-data-trained models are highly competitive under a variety of training settings and even outperform strong baselines trained on real-world data.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=43KCE87kCZy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Cassie07/SNOW-Dataset" target="_blank" rel="nofollow noreferrer">https://github.com/Cassie07/SNOW-Dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Cassie07/SNOW-Dataset" target="_blank" rel="nofollow noreferrer">https://github.com/Cassie07/SNOW-Dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="2ptbv_JjYKA" data-number="211">
        <h4>
          <a href="/forum?id=2ptbv_JjYKA">
              pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning
          </a>


            <a href="/pdf?id=2ptbv_JjYKA" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Daoyuan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daoyuan_Chen1">Daoyuan Chen</a>, <a href="/profile?id=~Dawei_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawei_Gao1">Dawei Gao</a>, <a href="/profile?id=~Weirui_Kuang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weirui_Kuang2">Weirui Kuang</a>, <a href="/profile?id=~Yaliang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaliang_Li1">Yaliang Li</a>, <a href="/profile?id=~Bolin_Ding3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bolin_Ding3">Bolin Ding</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#2ptbv_JjYKA-details-399" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2ptbv_JjYKA-details-399"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated Learning, Personalized Federated Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the first comprehensive benchmark for personalized Federated Learning, containing more than 10 datasets, 20 pFL baselines, and systematic evaluation with highlighted benefits and potential of pFL.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Personalized Federated Learning (pFL), which utilizes and deploys distinct local models, has gained increasing attention in recent years due to its success in handling the statistical heterogeneity of FL clients. However, standardized evaluation and systematical analysis of diverse pFL methods remain a challenge. Firstly, the highly varied datasets, FL simulation settings and pFL implementations prevent fast and fair comparisons of pFL methods. Secondly, the effectiveness and robustness of pFL methods are under-explored in various practical scenarios, such as new clients generalization and resource-limited clients participation. Finally, the current pFL literature diverges in the adopted evaluation and ablation protocols. To tackle these challenges, we propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating rapid, reproducible, standardized and thorough pFL evaluation. The proposed benchmark contains more than 10 datasets in diverse application domains with unified data partition and realistic heterogeneous settings; a modular and easy-to-extend pFL codebase with more than 20 competitive pFL baseline implementations; and systematic evaluations under containerized environments in terms of generalization, fairness, system overhead, and convergence. We highlight the benefits and potential of state-of-the-art pFL methods and hope pFL-Bench enables further pFL research and broad applications that would otherwise be difficult owing to the absence of a dedicated benchmark. The code is released at https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=2ptbv_JjYKA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench" target="_blank" rel="nofollow noreferrer">https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The proposed pFL-Bench is built upon the open-sourced Federated Learning package, FederatedScope (https://github.com/alibaba/FederatedScope). Both the proposed benchmark and the leveraged package follow the Apache-2.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Y0DTXOrEduj" data-number="210">
        <h4>
          <a href="/forum?id=Y0DTXOrEduj">
              ESTA: An Esports Trajectory and Action Dataset
          </a>


            <a href="/pdf?id=Y0DTXOrEduj" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peter_Xenopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Xenopoulos1">Peter Xenopoulos</a>, <a href="/profile?id=~Cl%C3%A1udio_Silva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cláudio_Silva1">Cláudio Silva</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 08 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Y0DTXOrEduj-details-51" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y0DTXOrEduj-details-51"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">esports, sports analytics, spatiotemporal data</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A dataset of millions of trajectories and player actions from a professional esports circuit</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Sports are an exciting domain to which one can apply machine learning due to sports' wide appeal, well-defined environments, and a slew of impact-rich applications. However, data from conventional sports is often unsuitable for research use due to its size, veracity, and accessibility. To address these issues, we turn to esports, also known as competitive video gaming. Since esports data is acquired through server logs rather than peripheral sensors, esports provides a unique opportunity to obtain a massive collection of clean and detailed spatiotemporal data. To parse esports data, we develop awpy, an open-source esports game log parsing library that can extract player trajectories and actions from game logs. Using awpy, we parse 8.6m actions, 7.9m game frames, and 417k trajectories from 1,558 game logs from professional Counter-Strike tournaments, which forms the Esports Trajectory and Actions (ESTA) dataset. ESTA is one of the largest and most granular publicly available sports data sets to date. We use ESTA to develop benchmarks for win probability prediction using player-specific information. The ESTA data is available at https://github.com/pnxenopoulos/esta and awpy is made public through PyPI.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Y0DTXOrEduj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/pnxenopoulos/esta" target="_blank" rel="nofollow noreferrer">https://github.com/pnxenopoulos/esta</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/pnxenopoulos/esta" target="_blank" rel="nofollow noreferrer">https://github.com/pnxenopoulos/esta</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">ESTA is released with a CC BY-SA 4.0 license. The awpy Python package is released under an MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="S_zdg4ygZK" data-number="209">
        <h4>
          <a href="/forum?id=S_zdg4ygZK">
              Iterative Vision-and-Language Navigation
          </a>


            <a href="/pdf?id=S_zdg4ygZK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jacob_Krantz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Krantz1">Jacob Krantz</a>, <a href="/profile?id=~Shurjo_Banerjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shurjo_Banerjee1">Shurjo Banerjee</a>, <a href="/profile?id=~Wang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wang_Zhu1">Wang Zhu</a>, <a href="/profile?id=~Jason_J_Corso1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_J_Corso1">Jason J Corso</a>, <a href="/profile?id=~Peter_Anderson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Anderson1">Peter Anderson</a>, <a href="/profile?id=~Stefan_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Lee1">Stefan Lee</a>, <a href="/profile?id=~Jesse_Thomason1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Thomason1">Jesse Thomason</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#S_zdg4ygZK-details-241" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="S_zdg4ygZK-details-241"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Embodied AI, Vision-and-language, Navigation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for evaluating language-guided agents navigating in a persistent environment over time.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for evaluating language-guided agents navigating in a persistent environment over time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the agent's memory at the beginning of every episode, testing the ability to perform cold-start navigation with no prior information. However, deployed robots occupy the same environment for long periods of time. The IVLN paradigm addresses this disparity by training and evaluating VLN agents that maintain memory across tours of scenes that consist of up to 100 ordered instruction-following Room-to-Room (R2R) episodes, each defined by an individual language instruction and a target path. We present discrete and continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours each in 80 indoor scenes. We find that extending the implicit memory of high-performing transformer VLN agents is not sufficient for IVLN, but agents that build maps can benefit from environment persistence, motivating a renewed focus on map-building agents in VLN.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=S_zdg4ygZK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Private.</span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We will provide a private comment for reviewers to access our data, benchmark, and code.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We will release the dataset and code.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Data derived from the Matterport3D dataset (including dataset files and trained models) will be released under the Matterport3D Terms of Use: http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf.
        Code will be released under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="S385EoHZMR" data-number="208">
        <h4>
          <a href="/forum?id=S385EoHZMR">
              HYPE-C: Evaluating Image Completion Models Through Standardized Crowdsourcing
          </a>


            <a href="/pdf?id=S385EoHZMR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Emily_Walters1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Walters1">Emily Walters</a>, <a href="/profile?id=~Weifeng_Chen4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weifeng_Chen4">Weifeng Chen</a>, <a href="/profile?id=~Jia_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia_Deng1">Jia Deng</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#S385EoHZMR-details-594" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="S385EoHZMR-details-594"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Generative image models, image completion models, image completion evaluation, human evaluation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A significant obstacle to the development of new image completion models is the lack of a standardized evaluation metric that reflects human judgement. Recent work has proposed the use of standardized human evaluation protocols for image synthesis models, which can reliably evaluate the visual quality of generated images. However, there does not yet exist a standardized human evaluation protocol for image completion. In this work, we propose such a protocol. We also provide the results of our evaluation method applied to a wide range of generative image models and compare these results to various automated metrics. Our evaluation yields a number of interesting findings: GAN-based image completion models are outperformed by autoregressive approaches, commonly used automatic evaluation metrics do not correspond well with human evaluations, and better automatic metrics can be created by directly optimizing them to align with a sample set of human evaluations. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=S385EoHZMR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bBff294gqLp" data-number="207">
        <h4>
          <a href="/forum?id=bBff294gqLp">
              NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search
          </a>


            <a href="/pdf?id=bBff294gqLp" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yijian_Qin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yijian_Qin2">Yijian Qin</a>, <a href="/profile?id=~Ziwei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Zhang1">Ziwei Zhang</a>, <a href="/profile?id=~Xin_Wang17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Wang17">Xin Wang</a>, <a href="/profile?id=~Zeyang_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeyang_Zhang1">Zeyang Zhang</a>, <a href="/profile?id=~Wenwu_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenwu_Zhu1">Wenwu Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#bBff294gqLp-details-187" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bBff294gqLp-details-187"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph neural architecture search, neural architecture search benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we have trained and evaluated all of these architectures on nine representative graph datasets, recording detailed metrics including train, validation, and test performance in each epoch, the latency, the number of parameters, etc. Based on our proposed benchmark, the performance of GNN architectures can be directly obtained by a look-up table without any further computation, which enables fair, fully reproducible, and efficient comparisons.  To demonstrate its usage, we make in-depth analyses of our proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS. We also showcase how the benchmark can be easily compatible with GraphNAS open libraries such as AutoGL and NNI. To the best of our knowledge, our work is the first benchmark for graph neural architecture search.   </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bBff294gqLp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/THUMNLab/NAS-Bench-Graph" target="_blank" rel="nofollow noreferrer">https://github.com/THUMNLab/NAS-Bench-Graph</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">code: Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8hHg-zs_p-h" data-number="206">
        <h4>
          <a href="/forum?id=8hHg-zs_p-h">
              GOOD: A Graph Out-of-Distribution Benchmark
          </a>


            <a href="/pdf?id=8hHg-zs_p-h" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shurui_Gui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shurui_Gui1">Shurui Gui</a>, <a href="/profile?id=~Xiner_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiner_Li1">Xiner Li</a>, <a href="/profile?id=~Limei_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Limei_Wang1">Limei Wang</a>, <a href="/profile?id=~Shuiwang_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuiwang_Ji1">Shuiwang Ji</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#8hHg-zs_p-h-details-785" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8hHg-zs_p-h-details-785"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Machine Learning, Graph Analysis, Out-of-distribution, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Out-of-distribution (OOD) learning deals with scenarios in which training and test data follow different distributions. Although general OOD problems have been intensively studied in machine learning, graph OOD is only an emerging area of research. Currently, there lacks a systematic benchmark tailored to graph OOD method evaluation. In this work, we aim at developing an OOD benchmark, known as GOOD, for graphs specifically. We explicitly make distinctions between covariate and concept shifts and design data splits that accurately reflect different shifts. We consider both graph and node prediction tasks as there are key differences when designing shifts. Overall, GOOD contains 8 datasets with 14 domain selections. When combined with covariate, concept, and no shifts, we obtain 42 different splits. We provide performance results on 7 commonly used baseline methods with 10 random runs. This results in 294 dataset-model combinations in total. Our results show significant performance gaps between in-distribution and OOD settings. Our results also shed light on different performance trends between covariate and concept shifts by different methods. Our GOOD benchmark is a growing project and expects to expand in both quantity and variety of resources as the area develops. The GOOD benchmark can be accessed via https://github.com/divelab/GOOD/</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=8hHg-zs_p-h&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/divelab/GOOD/" target="_blank" rel="nofollow noreferrer">https://github.com/divelab/GOOD/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1EcSGRkNxBLOUoRoLuhaazQZTKRGwaquX?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1EcSGRkNxBLOUoRoLuhaazQZTKRGwaquX?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Zmosb2KfzYd" data-number="205">
        <h4>
          <a href="/forum?id=Zmosb2KfzYd">
              TAP-Vid: A Benchmark for Tracking Any Point in a Video
          </a>


            <a href="/pdf?id=Zmosb2KfzYd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Carl_Doersch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carl_Doersch1">Carl Doersch</a>, <a href="/profile?id=~Ankush_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankush_Gupta1">Ankush Gupta</a>, <a href="/profile?id=~Larisa_Markeeva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Larisa_Markeeva1">Larisa Markeeva</a>, <a href="/profile?id=~Adria_Recasens_Continente1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adria_Recasens_Continente1">Adria Recasens Continente</a>, <a href="/profile?id=~Lucas_Smaira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucas_Smaira1">Lucas Smaira</a>, <a href="/profile?id=~Yusuf_Aytar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusuf_Aytar1">Yusuf Aytar</a>, <a href="/profile?id=~Joao_Carreira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joao_Carreira1">Joao Carreira</a>, <a href="/profile?id=~Andrew_Zisserman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Zisserman1">Andrew Zisserman</a>, <a href="/profile?id=~Yi_Yang10" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Yang10">Yi Yang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Zmosb2KfzYd-details-461" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zmosb2KfzYd-details-461"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">point tracking, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now.  In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark,TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of the video. We validate our pipeline on synthetic data and propose a simple end-to-end point tracking modelTAP-Net, showing that it outperforms all prior methods on our benchmark when trained on synthetic data.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Zmosb2KfzYd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://storage.googleapis.com/dm-tapnet/index.html" target="_blank" rel="nofollow noreferrer">https://storage.googleapis.com/dm-tapnet/index.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NV23ijZStUn" data-number="204">
        <h4>
          <a href="/forum?id=NV23ijZStUn">
              AppraiSet: Discussions on a New Art Dataset
          </a>


            <a href="/pdf?id=NV23ijZStUn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Thomas_Serban_von_Davier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Serban_von_Davier1">Thomas Serban von Davier</a>, <a href="/profile?email=max.van.kleek%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="max.van.kleek@cs.ox.ac.uk">Max Van Kleek</a>, <a href="/profile?id=~Nigel_Shadbolt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nigel_Shadbolt1">Nigel Shadbolt</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 08 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#NV23ijZStUn-details-974" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NV23ijZStUn-details-974"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, art, auction, valuation, appraisal</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces AppraiSet an art dataset with over 10 thousand individual art auction lots with data split across 22 variables.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces a new art dataset on the price and valuation of individual artworks. The dataset's creation was motivated by the growing academic interest in art datasets and the current challenge of accessing open financial art datasets. There are two apparent gaps the dataset aims to fill. The first is that most art datasets primarily explore the problem space of classifying artworks and their visual appearance. The second is that while financial art datasets exist, they are heavily gated behind paywalls and limited data access permissions. Our dataset classifies artworks on up to 22 individual variables. We provide details surrounding the collection, cleaning, and dataset contents. We conclude with some thoughts on potential approaches to establishing benchmarks depending on the research and model approach. Specifically, we mention the potential for this dataset to expand on the current work in visual art recommendation algorithms by adding the unique financial details contained within our dataset. Additionally, we describe how solely relying on quantitative analysis of art is limited in capturing the true meaning of valuable art.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NV23ijZStUn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://data.mendeley.com/datasets/2nfvz8g27c/1" target="_blank" rel="nofollow noreferrer">https://data.mendeley.com/datasets/2nfvz8g27c/1</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://data.mendeley.com/datasets/2nfvz8g27c/1" target="_blank" rel="nofollow noreferrer">https://data.mendeley.com/datasets/2nfvz8g27c/1</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY 4.0 license description
        The files associated with this dataset are licensed under a Creative Commons Attribution 4.0 International license.

        What does this mean?
        You can share, copy and modify this dataset so long as you give appropriate credit, provide a link to the CC BY license, and indicate if changes were made, but you may not do so in a way that suggests the rights holder has endorsed you or your use of the dataset. Note that further permission may be required for any content within the dataset that is identified as belonging to a third party.


        This text is courtesy of Mendeley Data which attaches the license to the dataset upon upload.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="47qVX2pa-2" data-number="202">
        <h4>
          <a href="/forum?id=47qVX2pa-2">
              A new dataset for multilingual keyphrase generation
          </a>


            <a href="/pdf?id=47qVX2pa-2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fr%C3%A9d%C3%A9ric_Piedboeuf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frédéric_Piedboeuf1">Frédéric Piedboeuf</a>, <a href="/profile?id=~Philippe_Langlais2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Langlais2">Philippe Langlais</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 13 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#47qVX2pa-2-details-541" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="47qVX2pa-2-details-541"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Keyphrase generation, multilingual keyphrase generation, dataset, keyphrases</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value "> Keyphrases  are an important tool for efficiently dealing with the ever-increasing amount of information present on the internet. While there are many recent papers on English keyphrase generation, keyphrase generation for other languages remains vastly understudied, mostly due to the absence of datasets. To address this, we present a novel dataset called Papyrus, composed of 16427 pairs of abstracts and keyphrases. We release four versions of this dataset, corresponding to different subtasks. Papyrus-e considers only English keyphrases, Papyrus-f considers French keyphrases, Papyrus-m considers keyphrase generation in any language, and Papyrus-a considers keyphrase generation in several languages. We train a state-of-the-art model on all four tasks and show that they lead to better results for non-English languages, with an average improvement of 14.2\% on keyphrase extraction and 2.0\% on generation. We also show an improvement of 0.4\% on extraction and 0.7\% on generation over English state-of-the-art results by concatenating Papyrus-e with the Kp20K training set.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=47qVX2pa-2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/smolPixel/French-keyphrase-generation" target="_blank" rel="nofollow noreferrer">https://github.com/smolPixel/French-keyphrase-generation</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/smolPixel/French-keyphrase-generation/tree/main/data

        The dataset is available in the folders Papyrus (corresponding to Papyrus-a), Papyrus-e, Papyrus-f, and Papyrus-m, in tsv format.

        Not curated data is available at https://github.com/smolPixel/French-keyphrase-generation/tree/main/Preprocessing, under the name dataset.jsonl</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We release the dataset and code under the Creative Commons public license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="TscdNx8udf5" data-number="200">
        <h4>
          <a href="/forum?id=TscdNx8udf5">
              SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments
          </a>


            <a href="/pdf?id=TscdNx8udf5" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mohan_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohan_Zhang2">Mohan Zhang</a>, <a href="/profile?id=~Xiaozhou_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaozhou_Wang1">Xiaozhou Wang</a>, <a href="/profile?id=~Benjamin_Decardi-Nelson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Decardi-Nelson1">Benjamin Decardi-Nelson</a>, <a href="/profile?email=sbo%40ualberta.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="sbo@ualberta.ca">Bo Song</a>, <a href="/profile?id=~An_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~An_Zhang3">An Zhang</a>, <a href="/profile?email=jinfeng%40ualberta.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="jinfeng@ualberta.ca">Jinfeng Liu</a>, <a href="/profile?email=bill.tao%40quartic.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="bill.tao@quartic.ai">Sile Tao</a>, <a href="/profile?email=jerry%40quartic.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="jerry@quartic.ai">Jiayi Cheng</a>, <a href="/profile?id=~Xiaohong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohong_Liu2">Xiaohong Liu</a>, <a href="/profile?id=~Dengdeng_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dengdeng_Yu1">Dengdeng Yu</a>, <a href="/profile?email=matthew%40quartic.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="matthew@quartic.ai">Matthew Poon</a>, <a href="/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#TscdNx8udf5-details-24" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TscdNx8udf5-details-24"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Collection of Manufacturing Simulation Environments, their advanced control baselines and reinforcement learning benchmarks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Traditional biological and pharmaceutical manufacturing plants are controlled by human workers or pre-defined thresholds. Modernized factories have advanced process control algorithms such as model predictive control (MPC). However, there is little exploration of applying deep reinforcement learning to control manufacturing plants. One of the reasons is the lack of high fidelity simulations and standard APIs for benchmarking. To bridge this gap, we develop an easy-to-use library that includes five high-fidelity simulation environments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which cover a wide range of manufacturing processes. We build these environments on published dynamics models. Furthermore, we benchmark online and offline, model-based and model-free reinforcement learning algorithms for comparisons of follow-up research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=TscdNx8udf5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/smpl-env/smpl" target="_blank" rel="nofollow noreferrer">https://github.com/smpl-env/smpl</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The URL to the smpl package: https://github.com/smpl-env/smpl
        The URL to the documentation of smpl: https://smpl-env.readthedocs.io/en/latest/index.html
        The URL to the smpl experiments: https://github.com/smpl-env/smpl-experiments</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="VF9f79cCYdZ" data-number="199">
        <h4>
          <a href="/forum?id=VF9f79cCYdZ">
              OpenFilter: A Framework to Democratize Research Access to Social Media AR Filters
          </a>


            <a href="/pdf?id=VF9f79cCYdZ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Piera_Riccio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piera_Riccio1">Piera Riccio</a>, <a href="/profile?id=~Bill_Psomas2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bill_Psomas2">Bill Psomas</a>, <a href="/profile?id=~Francesco_Galati1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Galati1">Francesco Galati</a>, <a href="/profile?id=~Francisco_Escolano1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francisco_Escolano1">Francisco Escolano</a>, <a href="/profile?id=~Thomas_Hofmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Hofmann1">Thomas Hofmann</a>, <a href="/profile?id=~Nuria_M_Oliver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nuria_M_Oliver1">Nuria M Oliver</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#VF9f79cCYdZ-details-281" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VF9f79cCYdZ-details-281"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">beauty filters, social media, societal impact, accessible research, augmented reality</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents two datasets of beautified faces -- FairBeauty and B-LFW -- and insights obtained through experiments; the datasets were created using a custom framework (OpenFilter).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Augmented Reality or AR filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. Given the wide adoption of AR face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. However, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied AR filters. The proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available AR face filters. Scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. In this paper, we present OpenFilter, a flexible framework to apply AR filters available in social media platforms on existing large collections of human faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the publicly available FairFace and LFW datasets and we outline insights derived from the analysis of these beautified datasets. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=VF9f79cCYdZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ellisalicante/OpenFilter" target="_blank" rel="nofollow noreferrer">https://github.com/ellisalicante/OpenFilter</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://fairbeauty.z6.web.core.windows.net/" target="_blank" rel="nofollow noreferrer">https://fairbeauty.z6.web.core.windows.net/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons CC BY-NC 4.0 for FairBeauty and B-LFW
        GNU General Public License for OpenFilter</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="4HueWqwduic" data-number="198">
        <h4>
          <a href="/forum?id=4HueWqwduic">
              DetectBench: An Object Detection Benchmark for OOD Generalization Algorithms
          </a>


            <a href="/pdf?id=4HueWqwduic" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fan_Wu14" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fan_Wu14">Fan Wu</a>, <a href="/profile?id=~Nanyang_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanyang_Ye1">Nanyang Ye</a>, <a href="/profile?id=~Chensheng_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chensheng_Peng1">Chensheng Peng</a>, <a href="/profile?id=~Bikang_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bikang_Pan1">Bikang Pan</a>, <a href="/profile?id=~lyu_huaihai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~lyu_huaihai1">lyu huaihai</a>, <a href="/profile?id=~Heyuan_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heyuan_Shi1">Heyuan Shi</a>, <a href="/profile?id=~Lanqing_HONG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lanqing_HONG1">Lanqing HONG</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#4HueWqwduic-details-536" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4HueWqwduic-details-536"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The consensus about practical machine learning tasks, such as object detection, is still the test data are drawn from the same distribution as the training data, which is known as IID (Independent and Identically Distributed). However, it can not avoid being confronted with OOD (Out-of-Distribution) scenarios in real practice. It is risky to apply an object detection algorithm without figuring out its OOD generalization performance. On the other hand, a plethora of OOD generalization algorithms have been proposed to amortize the gap between in-house and open-world performances of machine learning systems. However, their effectiveness was only demonstrated on the standard image classification tasks. It is still an opening question of how these algorithms perform on complex and practical tasks. In this paper, we first specify the setting of OOD-OD (OOD object detection). Then, we propose DetectBench consisting of four OOD-OD benchmark datasets to evaluate various object detection and OOD generalization algorithms. From extensive experiments on DetectBench, we find that existing OOD generalization algorithms fail dramatically when applied to the more practical object detection tasks. This raises questions over the current progress on a large number of these algorithms whether they can be effective in practice beyond simple toy examples. For future work, we sincerely hope that DetectBench can serve as a foothold for OOD-OD research. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=4HueWqwduic&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="0fAOe8YOjhy" data-number="197">
        <h4>
          <a href="/forum?id=0fAOe8YOjhy">
              KOLOMVERSE: An open large-scale image dataset for object detection in the maritime universe
          </a>


            <a href="/pdf?id=0fAOe8YOjhy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Abhilasha_Nanda2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhilasha_Nanda2">Abhilasha Nanda</a>, <a href="/profile?id=~Sung_Won_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Won_Cho1">Sung Won Cho</a>, <a href="/profile?id=~Hyeopwoo_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyeopwoo_Lee1">Hyeopwoo Lee</a>, <a href="/profile?id=~Jin_Hyoung_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jin_Hyoung_Park1">Jin Hyoung Park</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#0fAOe8YOjhy-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0fAOe8YOjhy-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">maritime dataset, large-scale dataset, object detection, neural network, computer vision, maritime autonomous surface ships, maritime safety</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">To our knowledge, KOLOMVERSE is by far the largest image open dataset for object detection in the maritime domain.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Over the years, datasets have been developed for various object detection tasks. Object detection in the maritime domain is essential for the safety and navigation of ships. However, there is still a lack of publicly available large-scale datasets in the maritime domain. To overcome this challenge, we present KOLOMVERSE, an open large-scale image dataset for object detection in the maritime domain. We collected 5,845 hours of video data captured from 21 territorial waters of South Korea. Through an elaborate data quality assessment process, we gathered around 2,151,470 4K resolution images from the video data. This dataset considers various environments: weather, time, illumination, occlusion, viewpoint, background, wind speed, and visibility. The KOLOMVERSE consists of five classes (ship, buoy, fishnet buoy, lighthouse and wind farm) for maritime object detection. The dataset has images of 3840$\times$2160 pixels and to our knowledge, it is by far the largest publicly available dataset for object detection in the maritime domain. We performed object detection experiments and evaluated our dataset on several pre-trained state-of-the-art architectures to show the effectiveness and usefulness of our dataset. The dataset is available at: \url{https://github.com/MaritimeDataset/KOLOMVERSE}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=0fAOe8YOjhy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://bit.ly/3mjACWr" target="_blank" rel="nofollow noreferrer">https://bit.ly/3mjACWr</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://bit.ly/3mjACWr

        The dataset URL contains the dataset in .jpg and labels as .csv files in the folder /dataset. Sampled dataset is under the folder /dataset_sampled. Codes and weight with Readme files are provided for TF2 object detectors in the folder /tf2_object_detector. Similarly YOLO models with sampled data in YOLO format, weights, cfg files and instructions to reproduce the results are provided in the /yolo folder.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">https://github.com/MaritimeDataset/KOLOMVERSE

        It is the official site to download KOLOMVERSE. It has a Google request form and we provide the dataset to anyone wishing to use the dataset after carefully going through their answers. Our dataset repository is under GPL License.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GPL 3.0 License </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1aEI9RH1n4f" data-number="194">
        <h4>
          <a href="/forum?id=1aEI9RH1n4f">
              Benchmarking Safe Constrained Reinforcement Learning Algorithms in Complex Robot Environments
          </a>


            <a href="/pdf?id=1aEI9RH1n4f" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hengrui_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hengrui_Zhang3">Hengrui Zhang</a>, <a href="/profile?id=~Youfang_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youfang_Lin1">Youfang Lin</a>, <a href="/profile?id=~Sheng_han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheng_han1">Sheng han</a>, <a href="/profile?id=~Kai_Lv1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Lv1">Kai Lv</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#1aEI9RH1n4f-details-815" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1aEI9RH1n4f-details-815"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Constrained Reinforcement Learning, Safe Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release a new constrained reinforcement learning benchmark that supports single-constraint, multi-constraint, and multi-agent constraint.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Constrained Reinforcement Learning (CRL) lacks commonly-used benchmark environments and performance metrics, making it difficult to compare different algorithms. Specifically, past benchmarks utilize a simple constraint configuration named $0/1$ constraint, where $cost=1$ indicates the constraint violation. However, the above setting is hard to reflect on reality. In addition, the tasks in past benchmarks have low difficulty, which is not conducive to the accurate evaluation of algorithm performance. In this work, we provide a new benchmark Safe Constrained Isaac Gym (SCIG) for CRL research. Our work focuses on characterizing complex constraints in the real world (single-constraint, multi-constraint, and multi-agent constraint). SCIG is built within Isaac Gym and supports running thousands of environments simultaneously. We evaluate the state-of-the-art CRL agents in SCIG and present an in-depth analysis of them. Experiments show that current algorithms cannot well balance the relationship between rewards and constraints in some complex multi-constrained scene tasks, and the off-policy algorithm has a more prominent oscillation in the constraint threshold than the on-policy algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1aEI9RH1n4f&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/feidieufo/CRL_ISAAC_GYM" target="_blank" rel="nofollow noreferrer">https://github.com/feidieufo/CRL_ISAAC_GYM</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD 3-Clause License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="E7bQqLHVW85" data-number="193">
        <h4>
          <a href="/forum?id=E7bQqLHVW85">
              Urban Scene Understanding via Hyperspectral Images: Dataset and Benchmark
          </a>


            <a href="/pdf?id=E7bQqLHVW85" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Qiu_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiu_Shen1">Qiu Shen</a>, <a href="/profile?id=~Yuxing_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxing_Huang1">Yuxing Huang</a>, <a href="/profile?id=~Tianqi_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianqi_Ren1">Tianqi Ren</a>, <a href="/profile?id=~Ying_Fu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Fu3">Ying Fu</a>, <a href="/profile?id=~Shaodi_You3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaodi_You3">Shaodi You</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#E7bQqLHVW85-details-392" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E7bQqLHVW85-details-392"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hyperspectral image, semantic segmentation, hyperspectral image classification, urban scene dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Light in all spectrum travel in the physical world. The trichromatism (RGB) human vision captures and understands it. Machine vision makes an analogy which use RGB camera for semantic segmentation and scene understanding. We argue that such machine vision suffers from metamerism, that different objects may appear in same RGB color while actually distinctive in spectrum. While learning based solutions, especially deep learning, have been heavily explored, they do not solve the fundamental physical limitation. In this paper, we propose to use Hyperspectral images (HSIs), which capture hundreds of consecutive narrow bands from the real visible world and therefore metamerism no longer exists. In short, we aim to 'see beyond human vision'. In practice, we introduce a novel large scale high quality HSI dataset for semantic segmentation in cityscapes. Namely, Hyperspectral City dataset. The dataset contains 1330 HSIs which are captured in typical urban driving scenes. Each HSI has 1889$\times$1422 spatial resolution and 128 spectral channels ranged from 450nm to 950nm. The dataset provides semantic annotation at pixel level which is done manually by professional annotators. We believe this dataset enables a new direction for scene understanding. Using the dataset, we also provide a benchmark and insightful analysis. The dataset is currently public accessible.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=E7bQqLHVW85&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://isis-data.science.uva.nl/cv/HyperspectralCityV2.0/" target="_blank" rel="nofollow noreferrer">https://isis-data.science.uva.nl/cv/HyperspectralCityV2.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://isis-data.science.uva.nl/cv/HyperspectralCityV2.0/" target="_blank" rel="nofollow noreferrer">https://isis-data.science.uva.nl/cv/HyperspectralCityV2.0/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="N6-ABrmQMqD" data-number="192">
        <h4>
          <a href="/forum?id=N6-ABrmQMqD">
              CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains
          </a>


            <a href="/pdf?id=N6-ABrmQMqD" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Bonifaz_Stuhr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bonifaz_Stuhr1">Bonifaz Stuhr</a>, <a href="/profile?id=~Johann_Kaspar_Ludwig_Haselberger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johann_Kaspar_Ludwig_Haselberger1">Johann Kaspar Ludwig Haselberger</a>, <a href="/profile?id=~Julian_Gebele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julian_Gebele1">Julian Gebele</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#N6-ABrmQMqD-details-314" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N6-ABrmQMqD-details-314"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">unsupervised, domain adaptation, lane detection, benchmark, dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=N6-ABrmQMqD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://carlanebenchmark.github.io" target="_blank" rel="nofollow noreferrer">https://carlanebenchmark.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The landing page https://carlanebenchmark.github.io leads to the dataset page via the "Dataset" button.

        In addition, the landing page leads to a tutorial on how to read the dataset via the "Tutorial" button.

        Dataset page: https://www.kaggle.com/datasets/carlanebenchmark/carlane-benchmark
        Tutorial: https://www.kaggle.com/code/carlanebenchmark/getting-started </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CARLANE Benchmark Datasets: Apache License Version 2.0, January 2004
        Code: MIT License

                    </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="euFWBmYvPbL" data-number="191">
        <h4>
          <a href="/forum?id=euFWBmYvPbL">
              Neuro-Symbolic Action Planning: A Benchmark for Visual and Manipulation Reasoning
          </a>


            <a href="/pdf?id=euFWBmYvPbL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Michal_Nazarczuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michal_Nazarczuk1">Michal Nazarczuk</a>, <a href="/profile?id=~Krystian_Mikolajczyk3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krystian_Mikolajczyk3">Krystian Mikolajczyk</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#euFWBmYvPbL-details-292" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="euFWBmYvPbL-details-292"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robotics, simulation, visual reasoning, action planning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">NS-AP provides manipulator simulation environment with high-quality visual observations, and a challenging visual and manipulation reasoning dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There is a striking similarity between a typical subroutine executed with a robotic manipulator and a symbolic program from neuro-symbolic approaches to visual reasoning tasks. Inspired by this observation, we present a neuro-symbolic action planning (NS-AP) framework that enables combined Visual and Manipulation Reasoning, as well as the use of symbolic programs for robotic manipulation. Our environment makes use of a physics engine and high-quality renderer to provide realistic visual observations that are also accurate to the physical state of the scene. We provide a modular pipeline that addresses various aspects of the reasoning-manipulation process. Additionally, the framework is easily extendable and allows to generate new, synthetic data. Finally, we propose a dataset generated with our NS-AP that is composed of 10 benchmarking scenarios focused on closely related, simultaneous visual and physical measurements.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=euFWBmYvPbL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://michaal94.github.io/NS-AP/" target="_blank" rel="nofollow noreferrer">https://michaal94.github.io/NS-AP/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://michaal94.github.io/NS-AP/
        From the landing page head to the Code&amp;Dataset tab via menu or the 'Download' button on the main page. Further, use download links there to obtain the dataset (due to the large size of the dataset we include links to a smaller number of training examples extracted from the same dataset).</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is released under standard MIT-license, we use several assets that are of CC-BY-4.0 (author must be credited) - list of credits available in Supplementary Material and on https://michaal94.github.io/SHOP-VRB/#credits</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zLL23PnbqMW" data-number="190">
        <h4>
          <a href="/forum?id=zLL23PnbqMW">
              Benchmark of Deep Active Learning: A Deep Active Learning Toolkit and A Comparative Survey
          </a>


            <a href="/pdf?id=zLL23PnbqMW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xueying_Zhan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xueying_Zhan1">Xueying Zhan</a>, <a href="/profile?id=~Qingzhong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qingzhong_Wang3">Qingzhong Wang</a>, <a href="/profile?id=~Kuan-Hao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuan-Hao_Huang1">Kuan-Hao Huang</a>, <a href="/profile?id=~Haoyi_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoyi_Xiong1">Haoyi Xiong</a>, <a href="/profile?id=~Dejing_Dou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dejing_Dou3">Dejing Dou</a>, <a href="/profile?id=~Antoni_B._Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antoni_B._Chan1">Antoni B. Chan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#zLL23PnbqMW-details-451" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zLL23PnbqMW-details-451"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep active learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct a Deep Active Learning toolkit, called DeepAL+, by re-implementing many highly-cited Deep Active Learning  methods. We also provide a comparative survey on multiple tasks based on DeepAL+ toolkit.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While deep learning (DL) is data-hungry and usually relies on large labeled data to deliver good performance, Active Learning (AL) reduces labeling cost by selecting a small proportion of samples from unlabeled data for labeling and then training. Therefore, in recent years, Deep Active Learning (DAL) has risen as a feasible solution for maximizing model performance under limited labeling cost/budget. Although abundant methods of DAL have been developed and various literature reviews conducted, the performance evaluation of DAL methods under fair comparison settings is not yet available. Our work intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by re-implementing 19 highly-cited DAL methods. We survey and categorize DAL-related works and construct comparative experiments across frequently used datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch size, number of epochs in the training process) that influence the efficacy of DAL, which provides better references for researchers to design their own DAL experiments or carry out DAL-related applications. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zLL23PnbqMW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/SineZHAN/deepALplus" target="_blank" rel="nofollow noreferrer">https://github.com/SineZHAN/deepALplus</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="W9x4j2pRuTe" data-number="189">
        <h4>
          <a href="/forum?id=W9x4j2pRuTe">
              GTAV-NightRain: Photometric Realistic Large-scale Dataset for Night-time Rain Streak Removal
          </a>


            <a href="/pdf?id=W9x4j2pRuTe" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fan_Zhang14" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fan_Zhang14">Fan Zhang</a>, <a href="/profile?id=~Shaodi_You3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaodi_You3">Shaodi You</a>, <a href="/profile?id=~Yu_Li4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Li4">Yu Li</a>, <a href="/profile?id=~Ying_Fu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Fu3">Ying Fu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#W9x4j2pRuTe-details-829" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W9x4j2pRuTe-details-829"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Deraining, Dataset, Night-time, Synthetic, Photometry</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This is a work presenting a new large-scale synthetic night-time deraining dataset rendered by computer game, which takes the photometry of rain streak into account.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Rain is transparent, which reflects and refracts light in the scene to the camera. In outdoor vision, rain, especially rain streaks degrade visibility and therefore need to be removed. In existing rain streak removal datasets, although density, scale, direction and intensity have been considered; transparency is not fully considered. This problem is particularly serious in night scenes, where the appearance of rain is largely depending on the interaction with scene illuminations and changes drastically within the image. This is problematic, because unrealistic dataset causes serious domain bias. In this paper, we propose GTAV-NightRain dataset, which is a large-scale synthetic night-time rain streak removal dataset. Unlike existing datasets, by using 3D computer graphic platform (namely GTA V), we are allowed to infer the three dimensional interaction between rain and illuminations, which insures the photometric realness. Current release of the dataset contains 12,860 HD rainy images and 1,286 corresponding HD ground truth images in diversified night scenes. A systematic benchmark and analysis are provided along with the dataset to inspire further research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=W9x4j2pRuTe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6638011" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6638011</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Zenodo: https://doi.org/10.5281/zenodo.6638011
        Github: https://github.com/zkawfanx/GTAV-NightRain</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mox_KHo3_Fk" data-number="188">
        <h4>
          <a href="/forum?id=mox_KHo3_Fk">
              Never mind the metrics---what about the uncertainty? Visualising confusion matrix metric distributions
          </a>


            <a href="/pdf?id=mox_KHo3_Fk" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~David_Lovell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Lovell1">David Lovell</a>, <a href="/profile?email=d24.miller%40qut.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="d24.miller@qut.edu.au">Dimity Miller</a>, <a href="/profile?id=~Jaiden_Capra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaiden_Capra1">Jaiden Capra</a>, <a href="/profile?id=~Andrew_P._Bradley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_P._Bradley1">Andrew P. Bradley</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 07 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mox_KHo3_Fk-details-662" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mox_KHo3_Fk-details-662"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Confusion matrix, performance metric, class imbalance, beta-binomial distribution, ROC</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Uncertainty in classifier performance metrics can easily eclipse differences in classifier performance; rather than searching for the "best" performance metric, we should put more effort into gathering representative data.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There are strong incentives to build classification systems that demonstrate outstanding predictive performance on various datasets and benchmarks. We believe these incentives risk a narrow focus on models and on the performance metrics used to evaluate and compare them---resulting in a growing body of literature to evaluate and compare metrics. This paper strives for a more balanced perspective on classifier performance metrics by highlighting their distributions under different models of uncertainty and showing how this uncertainty can easily eclipse differences in the empirical performance of classifiers. We begin by emphasising the fundamentally discrete nature of empirical confusion matrices and show how binary matrices can be meaningfully represented in a three dimensional compositional lattice, whose cross-sections form the basis of the space of receiver operating characteristic (ROC) curves. We develop equations, animations and interactive visualisations of the contours of performance metrics within (and beyond) this ROC space, showing how some are affected by class imbalance. We provide interactive visualisations that show the discrete posterior predictive probability mass functions of true and false positive rates in ROC space, and how these relate to uncertainty in performance metrics such as Balanced Accuracy (BA) and the Matthews Correlation Coefficient (MCC). Our hope is that these insights and visualisations will raise greater awareness of the substantial uncertainty in performance metric estimates that can arise when classifiers are evaluated on empirical datasets and benchmarks, and that classification model performance claims should be tempered by this understanding.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mox_KHo3_Fk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU General Public License v3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="zHNNSzo10xN" data-number="187">
        <h4>
          <a href="/forum?id=zHNNSzo10xN">
              Dungeons and Data: A Large-Scale NetHack Dataset
          </a>


            <a href="/pdf?id=zHNNSzo10xN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eric_Hambro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Hambro1">Eric Hambro</a>, <a href="/profile?id=~Roberta_Raileanu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roberta_Raileanu2">Roberta Raileanu</a>, <a href="/profile?id=~Danielle_Rothermel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danielle_Rothermel1">Danielle Rothermel</a>, <a href="/profile?id=~Vegard_Mella1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vegard_Mella1">Vegard Mella</a>, <a href="/profile?id=~Tim_Rockt%C3%A4schel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Rocktäschel1">Tim Rocktäschel</a>, <a href="/profile?id=~Heinrich_Kuttler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heinrich_Kuttler1">Heinrich Kuttler</a>, <a href="/profile?id=~Naila_Murray1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Naila_Murray1">Naila Murray</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#zHNNSzo10xN-details-75" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zHNNSzo10xN-details-75"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, offline RL, RL dataset, procedural generation, human demonstrations</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce and evaluate a new large-scale dataset for the game of NetHack, including 10 billion transitions from humans, 3 billion from a symbolic bot, and code for researchers to record and load their own trajectories.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent breakthroughs in the development of agents to solve challenging sequential decision making problems such as Go, StarCraft, or DOTA, have relied on both simulated environments and large-scale datasets.  However, progress on this research has been hindered by the scarcity of open-sourced datasets and the prohibitive computational cost to work with them.  Here we present the NetHack Learning Dataset (NLD), a large and highly-scalable dataset of trajectories from the popular game of NetHack, which is both extremely challenging for current methods and very fast to run. NLD consists of three parts: 10 billion state transitions from 1.5 million human trajectories collected on the NAO public NetHack server from 2009 to 2020; 3 billion state-action-score transitions from 100,000 trajectories collected from the symbolic bot winner of the NetHack Challenge 2021; and, accompanying code for users to record, load and stream any collection of such trajectories in a highly compressed form.  We evaluate a wide range of existing algorithms for learning from demonstrations, showing that significant research advances are needed to fully leverage large-scale datasets for challenging sequential decision making tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=zHNNSzo10xN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/dungeonsdatasubmission/dungeonsdata-neurips2022" target="_blank" rel="nofollow noreferrer">https://github.com/dungeonsdatasubmission/dungeonsdata-neurips2022</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/dungeonsdatasubmission/dungeonsdata-neurips2022" target="_blank" rel="nofollow noreferrer">https://github.com/dungeonsdatasubmission/dungeonsdata-neurips2022</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset and code are submitted under the NetHack General License which can be found here: https://github.com/facebookresearch/nle/blob/main/LICENSE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="4u252OfG-xh" data-number="185">
        <h4>
          <a href="/forum?id=4u252OfG-xh">
              A Greek Parliament Proceedings Dataset for Computational Linguistics and Political Analysis
          </a>


            <a href="/pdf?id=4u252OfG-xh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Konstantina_Dritsa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantina_Dritsa1">Konstantina Dritsa</a>, <a href="/profile?id=~Aikaterini_Thoma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aikaterini_Thoma1">Aikaterini Thoma</a>, <a href="/profile?id=~John_Pavlopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Pavlopoulos1">John Pavlopoulos</a>, <a href="/profile?id=~Panos_Louridas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Panos_Louridas1">Panos Louridas</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#4u252OfG-xh-details-23" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4u252OfG-xh-details-23"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, Greek language, Greek Parliament, computational linguistics, semantic shift, language change, machine learning, distributional semantics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Greek Parliament Proceedings Dataset for computational linguistics and political analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large, diachronic datasets of political discourse are hard to come across, especially for resource-lean languages such as Greek. In this paper, we introduce a curated dataset of the Greek Parliament Proceedings that extends chronologically from 1989 up to 2020. It consists of more than 1 million speeches with extensive meta-data, extracted from 5,355 parliamentary sitting record files. We explain how it was constructed and the challenges that had to be overcome. The dataset can be used for both computational linguistics and political analysis---ideally, combining the two. We present such an application, showing (i) how the dataset can be used to study the change of word usage through time, (ii) between significant historical events and political parties, (iii) by evaluating and employing algorithms for detecting semantic shifts.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=4u252OfG-xh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6626316" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6626316</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6626316" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6626316</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution Non Commercial 4.0 International (CC BY-NC)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="VV7rKa5A1Lm" data-number="184">
        <h4>
          <a href="/forum?id=VV7rKa5A1Lm">
              Collection and Evaluation of a Multitask Video-Based Drawing Dataset
          </a>


            <a href="/pdf?id=VV7rKa5A1Lm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nolan_B_Gutierrez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nolan_B_Gutierrez1">Nolan B Gutierrez</a>, <a href="/profile?id=~William_Beksi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Beksi1">William Beksi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 12 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#VV7rKa5A1Lm-details-876" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VV7rKa5A1Lm-details-876"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A core challenge for a learning agent is the ability solve multiple tasks simultaneously, while exploiting commonalities and differences across the tasks. Contemporary multitask systems fail to take advantage of supervisory signal information. Video-only control (VOC), defined as the problem of acquiring policies derived from systems trained with both video predictions and targets, can provide rapid learning, rich planning, and easy interpretability via supervisory signals. However, neither a multitask dataset nor an evaluation for VOC frameworks exists. As a first step towards multitask VOC, we put forward an attribute-focused video drawing dataset with precise 2D trajectories. Furthermore, we introduce a novel attribute accuracy score and attribute inception score for automatically evaluating and enhancing qualitative evaluations of trajectories under the presence of specific task characteristics. Our experiments show that predicted videos achieve improved clarity when trained on more compared to fewer tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=VV7rKa5A1Lm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="kfD3SggOGOe" data-number="183">
        <h4>
          <a href="/forum?id=kfD3SggOGOe">
              Realistic Large-Scale Fine-Depth Dehazing Dataset from 3D Videos
          </a>


            <a href="/pdf?id=kfD3SggOGOe" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ruoteng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruoteng_Li1">Ruoteng Li</a>, <a href="/profile?id=~Xiaoyi_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyi_Zhang4">Xiaoyi Zhang</a>, <a href="/profile?id=~Shaodi_You3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaodi_You3">Shaodi You</a>, <a href="/profile?id=~Yu_Li4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Li4">Yu Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#kfD3SggOGOe-details-315" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kfD3SggOGOe-details-315"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dehaze, defog, fine-depth, high-definition, stereo, 3D videos</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper describes a dehaze/defog dataset that is rendered using the find-depth from high-definition 3D videos.  </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Image dehazing is one of the important and popular topics in computer vision and machine learning. A reliable real-time dehazing method with reliable performance is highly desired for many applications such as autonomous driving, security surveillance, etc. While recent learning-based methods require datasets containing pairs of hazy images and clean ground truth, it is impossible to capture them in real scenes.  Many existing works compromise this difficulty to generate hazy images by rendering the haze from depth on common RGBD datasets using the haze imaging model. However, there is still a gap between the synthetic datasets and real hazy images as large datasets with high-quality depth are mostly indoor and depth maps for outdoor are imprecise. In this paper, we complement the existing datasets with a new, large, and diverse dehazing dataset containing real outdoor scenes from High-Definition (HD) 3D movies. We select a large number of high-quality frames of real outdoor scenes and render haze on them using depth from stereo. Our dataset is clearly more realistic and more diversified with better visual quality than existing ones. More importantly, we demonstrate that using this dataset greatly improves the dehazing performance in real scenes. In addition to the dataset, we also evaluate a series state of the art methods on the proposed benchmarking datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=kfD3SggOGOe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6642191#.YqswD3ZBxD8" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6642191#.YqswD3ZBxD8</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="XrkUwu7ju11" data-number="182">
        <h4>
          <a href="/forum?id=XrkUwu7ju11">
              Driver Dojo: A Benchmark for Generalizable Reinforcement Learning for Autonomous Driving
          </a>


            <a href="/pdf?id=XrkUwu7ju11" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sebastian_Rietsch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Rietsch1">Sebastian Rietsch</a>, <a href="/profile?email=clairebb1005%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="clairebb1005@gmail.com">Shih-Yuan Huan</a>, <a href="/profile?id=~Georgios_Kontes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgios_Kontes1">Georgios Kontes</a>, <a href="/profile?email=axel.plinge%40iis.fraunhofer.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="axel.plinge@iis.fraunhofer.de">Axel Plinge</a>, <a href="/profile?id=~Christopher_Mutschler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Mutschler1">Christopher Mutschler</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#XrkUwu7ju11-details-485" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XrkUwu7ju11-details-485"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, benchmark, autonomous driving, generalization, ADAS, SUMO, Carla, PPO, DQN, FQF, SAC</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement Learning (RL) has shown to reach super human-level performance across a wide range of tasks. However, unlike supervised machine learning (ML), learning strategies that generalize well to a wide range of situations remains one of the most challenging problems for real-world RL. Autonomous driving (AD) provides a multi-faceted experimental field, as it is necessary to learn the correct behavior over many variations of road layouts and large distributions of possible traffic situations, including individual driver personalities and hard-to-predict traffic events. We propose a challenging benchmark for generalizable RL for AD based on a configurable, flexible, and performant code base. Our benchmark uses a catalog of randomized scenario generators, including multiple mechanisms for road layout and traffic variations, different numerical and visual observation types, distinct action spaces, and diverse vehicle models, and also allows for use under static scenario definitions. In addition to purely algorithmic insights, our application-oriented benchmark also enables a better understanding of the impact of design decisions such as action and observation space on the generalizability of policies.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=XrkUwu7ju11&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/seawee1/driver-dojo" target="_blank" rel="nofollow noreferrer">https://github.com/seawee1/driver-dojo</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Software Copyright License for Academic Use of the Fraunhofer Software, Version 2.0

        © Copyright (Year) Fraunhofer‐Gesellschaft zur Förderung der angewandten Forschung e.V.
        1. INTRODUCTION
        The Fraunhofer Software which means any source code, object code or binary files provided by Fraunhofer excluding third party software and materials, is made available under this Software Copyright License.
        2. COPYRIGHT LICENSE
        Internal use of the Fraunhofer Software, in source and binary forms, with or without modification, is permitted without payment of copyright license fees for non‐commercial purposes of evaluation, testing and academic research.
        No right or license, express or implied, is granted to any part of the Fraunhofer Software except and solely to the extent as expressly set forth herein. Any commercial use or exploitation of the Fraunhofer Software and/or any modifications thereto under this license are prohibited.
        For any other use of the Fraunhofer Software than permitted by this software copyright license You need another license from Fraunhofer. In such case please contact Fraunhofer under the CONTACT INFORMATION below.
        3. LIMITED PATENT LICENSE
        If Fraunhofer patents are implemented by the Fraunhofer Software their use is permitted for internal non‐commercial purposes of evaluation, testing and academic research. No patent grant express or implied, is provided for any other use, including but not limited to commercial use or exploitation.
        Fraunhofer provides no warranty of patent non‐infringement with respect to the Fraunhofer Software. 4. DISCLAIMER
        The Fraunhofer Software is provided by Fraunhofer "AS IS" and WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES, including but not limited to the implied warranties of fitness for a particular purpose. IN NO EVENT SHALL FRAUNHOFER BE LIABLE for any direct, indirect, incidental, special, exemplary, or consequential damages, including but not limited to procurement of substitute goods or services; loss of use, data, or profits, or business interruption, however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence), arising in any way out of the use of the Fraunhofer Software, even if advised of the possibility of such damage.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="O_OXVNaI-lM" data-number="181">
        <h4>
          <a href="/forum?id=O_OXVNaI-lM">
              Human-Centric Visual Diversity Auditing
          </a>


            <a href="/pdf?id=O_OXVNaI-lM" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jerone_Theodore_Alexander_Andrews1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jerone_Theodore_Alexander_Andrews1">Jerone Theodore Alexander Andrews</a>, <a href="/profile?id=~Przemyslaw_Joniak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Przemyslaw_Joniak1">Przemyslaw Joniak</a>, <a href="/profile?email=alice.xiang%40sony.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="alice.xiang@sony.com">Alice Xiang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#O_OXVNaI-lM-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O_OXVNaI-lM-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">similarity, faces, bias, computer vision, cognitive, mental representations, diversity, auditing, dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We implicitly uncover face-varying axes by learning on a novel dataset of 638,180 human judgments of face similarity, which can be used to audit dataset diversity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Biases in human-centric computer vision models are often attributed to a lack of sufficient data diversity, with many demographics insufficiently represented. Auditing human image datasets for diversity can be difficult in practice, however,  due to an absence of ground-truth labels of relevant features. Few datasets contain self-reported demographic information, inferring demographic information risks introducing additional biases, and storing data on sensitive attributes can carry legal risks. Moreover, demographic labels do not necessarily capture all the relevant dimensions of human diversity that are important for developing fair and robust models. We propose a methodology for auditing the visual diversity of unlabeled human face image datasets using a learned set of human-interpretable dimensions corresponding to perceptual face-varying axes. We implicitly uncover the dimensions by learning on a novel dataset of 638,180 human judgments of face similarity (FAX). Our model-inferred face embeddings are well aligned with human mental representations of faces, where dimensional values express both feature presence and extent. Since our model is learned entirely from open-ended comparisons, the learned dimensions are not constrained by the limits of categorical attribute definitions. We demonstrate the utility of our model as a tool for comparative dataset diversity auditing, collecting continuous, as opposed to discrete, attribute values for novel faces, and surfacing disparities in model behavior. Moreover, by learning subspaces that encode each annotator's internal notion of similarity, we show that feature importance varies based on the demographics of the annotators, adding to calls for a diversity of perspectives when annotating datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=O_OXVNaI-lM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">The dataset will be made publicly available in August 2022 on Zenodo: https://zenodo.org/. The reviewers will be provided access to the data now.</span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">The dataset will be made publicly available in August 2022 on Zenodo: https://zenodo.org/. The reviewers will be provided access to the data now.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset will be made publicly available in August 2022 on Zenodo: https://zenodo.org/. The reviewers will be provided access to the data now.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The dataset will be made publicly available in August 2022 on Zenodo: https://zenodo.org/. The reviewers will be provided access to the data now.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ZeeswGSOw7r" data-number="180">
        <h4>
          <a href="/forum?id=ZeeswGSOw7r">
              IKEA-Manual: Seeing Shape Assembly Step by Step
          </a>


            <a href="/pdf?id=ZeeswGSOw7r" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ruocheng_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruocheng_Wang1">Ruocheng Wang</a>, <a href="/profile?id=~Yunzhi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunzhi_Zhang1">Yunzhi Zhang</a>, <a href="/profile?id=~Jiayuan_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayuan_Mao1">Jiayuan Mao</a>, <a href="/profile?id=~Ran_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ran_Zhang1">Ran Zhang</a>, <a href="/profile?id=~Chin-Yi_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chin-Yi_Cheng1">Chin-Yi Cheng</a>, <a href="/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ZeeswGSOw7r-details-453" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZeeswGSOw7r-details-453"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on three tasks related to shape assembly: assembly plan generation, part segmentation, and 3D part assembly.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ZeeswGSOw7r&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We licensed the annotations of visual manuals under CC BY-NC-SA 4.0. Copyright of the original visual manuals and 3D models are owned by their creators respectively. A more detailed breakdown of licenses will be included in the metadata of the dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="g1QBc3LwGuG" data-number="179">
        <h4>
          <a href="/forum?id=g1QBc3LwGuG">
              Neural Protein Ligand Docking: Dataset and Evaluation
          </a>


            <a href="/pdf?id=g1QBc3LwGuG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fei_Li13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Li13">Fei Li</a>, <a href="/profile?id=~Yatao_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yatao_Bian1">Yatao Bian</a>, <a href="/profile?id=~Lu_Zhang8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Zhang8">Lu Zhang</a>, <a href="/profile?id=~Shuigeng_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuigeng_Zhou1">Shuigeng Zhou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#g1QBc3LwGuG-details-302" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g1QBc3LwGuG-details-302"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Protein ligand docking, machine learning, drug design</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning is being utilized more and more to speed up and empower drug design and discovery. Recently, machine learning models have played a key role in structure-based virtual screening, assisting in the analysis of protein-ligand interactions and the prediction of binding affinity levels more rapidly and correctly, a process known as protein-ligand docking. Existing datasets, such as PDBbind, allow for the training and evaluation of classical protein-ligand docking approaches, but they lack specialized facilities for machine learning models, such as large amounts of data, rich features, and structured data splits for evaluating generalization capabilities. In this work, we introduce a comprehensive neural protein-ligand docking dataset, called PLDock, and accompany it with a tool for training and evaluating machine learning-based protein-ligand docking models. We designed real scenario-based protein-ligand docking tasks, splits, baselines and metrics, with the goal of training and evaluating machine learning-based protein-ligand docking models. Currently, PLDock provides more than 70,000 protein-ligand complex structures, more than 150,000 protein-ligand affinity data, 3 typical tasks, 5 types of structured data splits and 9 evaluation metrics. For ease of use, all data and scripts are displayed in a commonly used format and are freely available online.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=g1QBc3LwGuG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://anonyanony0.github.io/PLDock/" target="_blank" rel="nofollow noreferrer">https://anonyanony0.github.io/PLDock/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Landing page: https://anonyanony0.github.io/PLDock/
        Github: https://github.com/anonyanony0/PLDock
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="D53QcGyuL8" data-number="177">
        <h4>
          <a href="/forum?id=D53QcGyuL8">
              RefIndoor: An RGB-D Visual Grounding Dataset with Cluttered Indoor Scenes for Robotics
          </a>


            <a href="/pdf?id=D53QcGyuL8" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuhao_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhao_Lu2">Yuhao Lu</a>, <a href="/profile?id=~Yixuan_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yixuan_Fan1">Yixuan Fan</a>, <a href="/profile?id=~Beixing_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beixing_Deng1">Beixing Deng</a>, <a href="/profile?id=~Fangfu_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fangfu_Liu2">Fangfu Liu</a>, <a href="/profile?id=~Ya-Li_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ya-Li_Li1">Ya-Li Li</a>, <a href="/profile?id=~Shengjin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengjin_Wang1">Shengjin Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#D53QcGyuL8-details-219" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D53QcGyuL8-details-219"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Visual Grounding, Robotics, Referring Expression Comprehension, Referring Expression Segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a new challenging visual grounding dataset, RefIndoor, for robotic perception and reasoning in indoor environments!</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The intelligence and diversity of robot applications have been closely related to the technical achievements of computer vision and natural language processing fields. Some current researches of voice interactive robot manipulation attempt to integrate the visual grounding tasks into the robot operation engine. However, the existing visual grounding datasets are hard to be used as a suitable practical test bed for robotics because of their low consistency in the following points: the relevance of instruction language, the practicality of objects and scenes, and the diversity of ambiguous comparison. In this work, we propose a new challenging visual grounding dataset for robotic perception and reasoning in indoor environments, called RefIndoor. The RefIndoor dataset collects 10,891 real-world RGB images and depth images from cluttered daily life scenes, and generates 50,758 referring expressions in the form of robot instructions for objects of these images. Moreover, nearly half of the images contain samples of ambiguity confrontation. Experiments demonstrate the availability and reasonability  of the RefIndoor. And the RefIndoor is potential and promising to provide a research basis for the visual grounding with 3D data or scene adaptation. The dataset and code are available at \url{https://github.com/luyh20/RefIndoor}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=D53QcGyuL8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://luyh20.github.io/RefIndoor.github.io/" target="_blank" rel="nofollow noreferrer">https://luyh20.github.io/RefIndoor.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://drive.google.com/uc?export=download&amp;id=1-kEfagTovmcbEFG6-C_xatZfFiLWQmiU
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The RefIndoor dataset is distributed under the CC BY-NC-SA 4.0 license.
        The code is under MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Nr-FMpvrcXY" data-number="176">
        <h4>
          <a href="/forum?id=Nr-FMpvrcXY">
              An Empirical Study of Personalized Federated Learning
          </a>


            <a href="/pdf?id=Nr-FMpvrcXY" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Koji_Matsuda2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Koji_Matsuda2">Koji Matsuda</a>, <a href="/profile?id=~Yuya_Sasaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuya_Sasaki1">Yuya Sasaki</a>, <a href="/profile?id=~Chuan_Xiao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuan_Xiao2">Chuan Xiao</a>, <a href="/profile?id=~Makoto_Onizuka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Makoto_Onizuka1">Makoto Onizuka</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Nr-FMpvrcXY-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nr-FMpvrcXY-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated learning, Deep learning, Benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We benchmark the performance of existing personalized federated learning methods through comprehensive experiments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated learning is a distributed machine learning approach in which a single server and multiple clients collaboratively build machine learning models without sharing datasets on clients. A challenging issue of federated learning is data heterogeneity (i.e., data distributions may differ across clients). To cope with this issue, numerous federated learning methods aim at personalized federated learning and build optimized models for clients. Whereas existing studies empirically evaluated their own methods, the experimental settings (e.g., comparison methods, datasets, and client setting) in these studies differ from each other, and it is unclear which personalized federate learning method achieves the best performance and how much progress can be made by using these methods instead of standard (i.e., non-personalized) federated learning.In this paper, we benchmark the performance of existing personalized federated learning through comprehensive experiments to evaluate the characteristics of each method. Our experimental study shows that (1) there are no champion methods, (2) large data heterogeneity often leads to high accurate predictions, and (3) standard federated learning methods (e.g. FedAvg) with fine-tuning often outperform personalized federated learning methods. We open our benchmark tool $FedBench$ for researchers to conduct experimental studies with various experimental settings.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Nr-FMpvrcXY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/OnizukaLab/FedBench" target="_blank" rel="nofollow noreferrer">https://github.com/OnizukaLab/FedBench</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2022 Koji Matsuda

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="82F-nM6PQix" data-number="175">
        <h4>
          <a href="/forum?id=82F-nM6PQix">
              Planet-CR: A Multi-Modal and Multi-Resolution Dataset for Cloud Removal in High Resolution Optical Remote Sensing Imagery
          </a>


            <a href="/pdf?id=82F-nM6PQix" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Fang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fang_Xu1">Fang Xu</a>, <a href="/profile?id=~Patrick_Ebel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Ebel1">Patrick Ebel</a>, <a href="/profile?id=~Yilei_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yilei_Shi1">Yilei Shi</a>, <a href="/profile?id=~Wen_Yang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wen_Yang2">Wen Yang</a>, <a href="/profile?id=~Xiao_Xiang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Xiang_Zhu1">Xiao Xiang Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#82F-nM6PQix-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="82F-nM6PQix-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Cloud Removal, Remote Sensing, Image Reconstruction</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we introduce Planet-CR, a multi-modal and multi-resolution dataset for cloud removal in PlanetScope optical remote sensing imagery. Planet-CR is the first public dataset for cloud removal to feature globally sampled high resolution optical observations, in combination with paired radar measurements as well as pixel-level land cover annotations. With this dataset, we consider the problem of cloud removal in high resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. We benchmark and in-depth analysis of representative cloud removal methods, highlighting the challenges and future research directions for high resolution remote sensing imagery cloud removal. The dataset and benchmarked models are available at https://github.com/xufangchn/Planet-CR, and hope this will inspire future research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=82F-nM6PQix&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/xufangchn/Planet-CR" target="_blank" rel="nofollow noreferrer">https://github.com/xufangchn/Planet-CR</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/xufangchn/Planet-CR" target="_blank" rel="nofollow noreferrer">https://github.com/xufangchn/Planet-CR</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="c0l2YolqD2T" data-number="174">
        <h4>
          <a href="/forum?id=c0l2YolqD2T">
              How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?
          </a>


            <a href="/pdf?id=c0l2YolqD2T" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chengxu_Zhuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengxu_Zhuang1">Chengxu Zhuang</a>, <a href="/profile?id=~Violet_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Violet_Xiang1">Violet Xiang</a>, <a href="/profile?id=~Yoon_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoon_Bai1">Yoon Bai</a>, <a href="/profile?id=~Xiaoxuan_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoxuan_Jia1">Xiaoxuan Jia</a>, <a href="/profile?id=~Nicholas_Turk-Browne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Turk-Browne1">Nicholas Turk-Browne</a>, <a href="/profile?id=~Kenneth_Norman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenneth_Norman1">Kenneth Norman</a>, <a href="/profile?id=~James_J._DiCarlo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_J._DiCarlo1">James J. DiCarlo</a>, <a href="/profile?id=~Daniel_LK_Yamins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_LK_Yamins1">Daniel LK Yamins</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#c0l2YolqD2T-details-417" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c0l2YolqD2T-details-417"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">human visual learning, human-like curriculum, real-time visual learning, life-long visual learning, unsupervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms must straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=c0l2YolqD2T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rxG43Zwjwz" data-number="173">
        <h4>
          <a href="/forum?id=rxG43Zwjwz">
              Beyond Information Gain: An Empirical Benchmark for Low-Switching-Cost Reinforcement Learning
          </a>


            <a href="/pdf?id=rxG43Zwjwz" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shusheng_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shusheng_Xu1">Shusheng Xu</a>, <a href="/profile?id=~Yancheng_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yancheng_Liang1">Yancheng Liang</a>, <a href="/profile?id=~Yunfei_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunfei_Li1">Yunfei Li</a>, <a href="/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>, <a href="/profile?id=~Yi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wu1">Yi Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 08 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#rxG43Zwjwz-details-896" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rxG43Zwjwz-details-896"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">switching cost, information gain, reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We tackle reinforcement learning with a low-switching-cost constraint and benchmark a collection of policy switching criterias on wide range RL testbeds.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A ubiquitous requirement in many practical reinforcement learning (RL) applications is that the deployed policy that actually interacts with the environment cannot change frequently. Such an RL setting is called low-switching-cost RL, i.e., achieving the highest reward while reducing the number of policy switches during training. It has been a recent trend in theoretical RL research to develop provably efficient RL algorithms with low switching cost. The core idea in these theoretical works is to measure the information gain and switch the policy when the information gain is doubled. Despite of the theoretical advances, none of the existing approaches have been validated empirically. We conduct the first empirical evaluation of different policy switching criteria on popular RL testbeds, including a medical treatment environment, the Atari games, and robotic control tasks. Surprisingly, although information-gain-based methods do recover the optimal rewards, they often lead to a substantially higher switching cost. By contrast, we find that a feature-based criterion, which has been largely ignored in the theoretical research, consistently produces the best performances over all the domains. We hope our benchmark could bring insights to the community and inspire future research. Our code and complete results can be found at https://sites.google.com/view/low-switching-cost-rl.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rxG43Zwjwz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/low-switching-cost-rl." target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/low-switching-cost-rl.</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code is hosted at GitHub under the MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="76w7bsdViZf" data-number="172">
        <h4>
          <a href="/forum?id=76w7bsdViZf">
              Hard ImageNet: Segmentations for Objects with Strong Spurious Cues
          </a>


            <a href="/pdf?id=76w7bsdViZf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mazda_Moayeri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mazda_Moayeri1">Mazda Moayeri</a>, <a href="/profile?id=~Sahil_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sahil_Singla1">Sahil Singla</a>, <a href="/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#76w7bsdViZf-details-262" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="76w7bsdViZf-details-262"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, classification, spurious features, segmentations</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A new perspective on classification performance: how can we learn to predict *for the right reasons* when our data is suboptimal (i.e. riddled with spurious cues)</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep classifiers are known to rely on spurious features, leading to reduced generalization. The severity of this problem varies significantly by class. We identify $15$ classes in ImageNet with very strong spurious cues, and collect segmentation masks for these challenging objects to form \emph{Hard ImageNet}. Leveraging noise, saliency, and ablation based metrics, we demonstrate that models rely on spurious features in Hard ImageNet far more than in RIVAL10, an ImageNet analog to CIFAR10. We observe Hard ImageNet objects are less centered and occupy much less space in their images than RIVAL10 objects, leading to greater spurious feature reliance. Further, we use robust neural features to automatically rank our images based on the degree of spurious cues present. Comparing images with high and low rankings within a class reveals the exact spurious features models rely upon, and shows reduced performance when spurious features are absent. With Hard ImageNet's image rankings, object segmentations, and our extensive evaluation suite, the community can begin to address the problem of learning to detect challenging objects \emph{for the right reasons}, despite the presence of strong spurious cues.  </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=76w7bsdViZf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">mmoayeri.github.io/HardImagenet</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">mmoayeri.github.io/HardImagenet

        This page and accompanying github repo contains all code to download the data, evaluate models on the benchmark, and generate plots shown in the paper. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-0: Creative Commons Public Domain Dedication</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-drWAiNAKX8" data-number="171">
        <h4>
          <a href="/forum?id=-drWAiNAKX8">
              On the Transferability of Visual Features in Generalized Zero-Shot Learning
          </a>


            <a href="/pdf?id=-drWAiNAKX8" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Paola_Cascante-Bonilla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paola_Cascante-Bonilla1">Paola Cascante-Bonilla</a>, <a href="/profile?id=~Yanjun_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanjun_Qi1">Yanjun Qi</a>, <a href="/profile?id=~Vicente_Ordonez2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vicente_Ordonez2">Vicente Ordonez</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#-drWAiNAKX8-details-599" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-drWAiNAKX8-details-599"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">large scale analysis, generalized zero-shot learning, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Our work provides a comprehensive benchmark for Generalized Zero-Shot Learning (GZSL). We benchmark extensively the utility of different GZSL methods which we characterize as embedding-based, generative-based, and based on semantic disentanglement. We particularly investigate how these previous methods for GZSL fare against CLIP, a more recent large scale pretrained model that claims zero-shot performance by means of being trained with internet scale multimodal data. Our findings indicate that through prompt engineering over an off-the-shelf CLIP model, it is possible to surpass all previous methods on standard benchmarks for GZSL: CUB (Birds), SUN (scenes), and AWA2 (animals). While it is possible that CLIP has actually seen many of the unseen categories in these benchmarks, we also show that GZSL methods in combination with the feature backbones obtained through CLIP contrastive pretraining (e.g. ViT~L/14) still provide advantages in standard GZSL benchmarks over off-the-shelf CLIP with prompt engineering. In summary, some GZSL methods designed to transfer information from seen categories to unseen categories still provide valuable gains when paired with a comparable feature backbone such as the one in CLIP. Surprisingly, we find that generative-based GZSL methods provide more advantages compared to more recent methods based on semantic disentanglement. We release a well-documented codebase which both replicates our findings and provides a modular framework for analyzing representation learning issues in GZSL.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-drWAiNAKX8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/gzsl-framework/anonymized_code" target="_blank" rel="nofollow noreferrer">https://github.com/gzsl-framework/anonymized_code</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="kQUOIyPg-ux" data-number="170">
        <h4>
          <a href="/forum?id=kQUOIyPg-ux">
              SurDis: A Surface Discontinuity Dataset for Wearable Technology to Assist Blind Navigation in Urban Environments
          </a>


            <a href="/pdf?id=kQUOIyPg-ux" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kuan_Yew_Leong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuan_Yew_Leong1">Kuan Yew Leong</a>, <a href="/profile?id=~Siew_Mooi_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siew_Mooi_Lim1">Siew Mooi Lim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#kQUOIyPg-ux-details-483" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kQUOIyPg-ux-details-483"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">surface discontinuity, depth map, stereo vision, blind and low vision navigation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">According to World Health Organization, there is an estimated 2.2 billion people with a near or distance vision impairment worldwide. Difficulty in self-navigation is one of the greatest challenges to independence for the blind and low vision (BLV) people. Through consultations with several BLV service providers, we realized that negotiating surface discontinuities is one of the very prominent challenges when navigating an outdoor environment within the urban. Surface discontinuities are commonly formed by rises and drop-offs along a pathway. They could be a threat to balancing during a walk and perceiving such a threat is highly challenging to the BLVs. In this paper, we introduce SurDis, a novel dataset of depth maps and stereo images that exemplifies the issue of surface discontinuity in the urban areas of Klang Valley, Malaysia. We seek to address the limitation of existing datasets of such nature in these areas. Current mobility tools for the BLVs predominantly focus on furniture, indoor built environments, traffic signs, vehicles, humans and various types of objects' detection above the surface of a pathway. We emphasize a specific purpose for SurDis – to support the development of assistive wearable technology for the BLVs to negotiate surface discontinuity. We consulted BLV volunteers on the specifications of surface condition that could become hazardous for navigation using 3D printed replicas of actual scaled-down scenes, and identified locations that are frequented by the BLVs as our target data collection fields. With feedback from these volunteers, we developed a lightweight, small and unobtrusive prototype equipped with a tiny stereo camera and an embedded system on a single board computer to capture the samples from 10 different locations. We describe instrument development, data collection, preprocessing, and annotation methodology involved. The dataset contains: (1) more than 17000 depth maps generated from 200 sets of stereo image sequences, (2) annotations of surface discontinuity in the depth maps, and (3) bitmap stereo image pairs corresponding to the depth maps in (1).</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=kQUOIyPg-ux&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/kuanyewleong/surdis" target="_blank" rel="nofollow noreferrer">https://github.com/kuanyewleong/surdis</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/kuanyewleong/surdis

        Please follow this landing page for extra information, the download link to the dataset is given here.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This dataset is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License and is intended for non-commercial uses. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="B7CEdxcT7hT" data-number="169">
        <h4>
          <a href="/forum?id=B7CEdxcT7hT">
              BenMO: Benchmarking predictive machine learning algorithms using MultiOmics data
          </a>


            <a href="/pdf?id=B7CEdxcT7hT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Maria_Xenochristou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Xenochristou1">Maria Xenochristou</a>, <a href="/profile?id=~Natalie_Stanley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Natalie_Stanley1">Natalie Stanley</a>, <a href="/profile?id=~Camilo_Espinosa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Camilo_Espinosa1">Camilo Espinosa</a>, <a href="/profile?id=~Anthony_E_Culos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anthony_E_Culos1">Anthony E Culos</a>, <a href="/profile?id=~Thanaphong_Phongpreecha2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thanaphong_Phongpreecha2">Thanaphong Phongpreecha</a>, <a href="/profile?id=~Neal_G_Ravindra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neal_G_Ravindra1">Neal G Ravindra</a>, <a href="/profile?email=ivanam%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ivanam@stanford.edu">Ivana Maric</a>, <a href="/profile?email=raminf%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="raminf@stanford.edu">Ramin Fallahzadeh</a>, <a href="/profile?email=mgbckr%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mgbckr@stanford.edu">Martin Becker</a>, <a href="/profile?email=yuqitan%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuqitan@stanford.edu">Yuqi Tan</a>, <a href="/profile?email=melan.thuraiappah%40unisg.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="melan.thuraiappah@unisg.ch">Melan Thuraiappah</a>, <a href="/profile?email=alanleec%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="alanleec@stanford.edu">Alan Chang</a>, <a href="/profile?email=istelzer%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="istelzer@stanford.edu">Ina Stelzer</a>, <a href="/profile?email=dfeyaer%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dfeyaer@stanford.edu">Dorien Feyaerts</a>, <a href="/profile?id=~Ronald_J_Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ronald_J_Wong1">Ronald J Wong</a>, <a href="/profile?email=gmshaw%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gmshaw@stanford.edu">Gary Shaw</a>, <a href="/profile?email=dks750%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dks750@stanford.edu">David Stevenson</a>, <a href="/profile?email=ang%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ang@stanford.edu">Martin Angst</a>, <a href="/profile?email=gbrice%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gbrice@stanford.edu">Brice Gaudiiliere</a>, <a href="/profile?email=naghaeep%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="naghaeep@stanford.edu">Nima Aghaeepour</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#B7CEdxcT7hT-details-39" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B7CEdxcT7hT-details-39"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multiomics, benchmark, supervised learning, predictive algorithms</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a novel benchmarking approach used to evaluate 9 regression algorithms against 12 multiomics studies and compare their prediction accuracy, as well as runtime and memory requirements.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Technological advances in producing omics assays have resulted in an increasing number of multiomics studies, measuring a broad range of biological modalities in translational settings. Effectively integrating these modalities can be valuable for gaining biological insights and predicting clinical outcomes. However, the dimensionality of these high-throughput assays, along with the limited number of patients and the heterogeneity across datasets, complicate multiomics predictive modeling. Several methods exist for integrating multi-omics data, from traditional machine learning approaches to specialized methods. Here, we present a novel benchmarking approach used to evaluate 9 regression algorithms against 12 multiomics studies and compare their prediction accuracy, as well as runtime and memory requirements. Our methodology can be used by translational scientists to independently evaluate integration strategies in other clinical and biological
        settings to maximize the accuracy of multiomics models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=B7CEdxcT7hT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mariaxen/multiomics-benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/mariaxen/multiomics-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The MIT License (MIT)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jNdLszxdtra" data-number="168">
        <h4>
          <a href="/forum?id=jNdLszxdtra">
              NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning
          </a>


            <a href="/pdf?id=jNdLszxdtra" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rong-Jun_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rong-Jun_Qin1">Rong-Jun Qin</a>, <a href="/profile?id=~Xingyuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingyuan_Zhang1">Xingyuan Zhang</a>, <a href="/profile?email=songyi.gao%40polixir.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="songyi.gao@polixir.ai">Songyi Gao</a>, <a href="/profile?id=~Xiong-Hui_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiong-Hui_Chen1">Xiong-Hui Chen</a>, <a href="/profile?id=~Zewen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zewen_Li2">Zewen Li</a>, <a href="/profile?id=~Weinan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weinan_Zhang1">Weinan Zhang</a>, <a href="/profile?id=~Yang_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Yu5">Yang Yu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#jNdLszxdtra-details-186" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jNdLszxdtra-details-186"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">offline reinforcement learning, conservative datasets, offline policy validation, benchmarks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">NeoRL presents conservative datasets for offline RL, highlights the complete pipeline for deploying offline RL in real-world applications, and also benchmarks recent offline RL algorithms on NeoRL under the complete pipeline.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Offline reinforcement learning (RL) aims at learning effective policies from historical data without extra environment interactions. During our experience of applying offline RL, we noticed that previous offline RL benchmarks commonly involve significant reality gaps, which we have identified include rich and overly exploratory datasets, degraded baseline, and missing policy validation. In many real-world situations, to ensure system safety, running an overly exploratory policy to collect various data is prohibited, thus only a narrow data distribution is available. The resulting policy is regarded as effective if it is better than the working behavior policy; the policy model can be deployed only if it has been well validated, rather than accomplished the training. In this paper, we present a Near real-world offline RL benchmark, named NeoRL, to reflect these properties. NeoRL datasets are collected with a more conservative strategy. Moreover, NeoRL contains the offline training and offline validation pipeline before the online test, corresponding to real-world situations. We then evaluate recent state-of-the-art offline RL algorithms in NeoRL. The empirical results demonstrate that some offline RL algorithms are less competitive to the behavior cloning and the deterministic behavior policy, implying that they could be less effective in real-world tasks than in the previous benchmarks. We also disclose that current offline policy evaluation methods could hardly select the best policy. We hope this work will shed some light on future research and deploying RL in real-world systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jNdLszxdtra&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/polixir/NeoRL" target="_blank" rel="nofollow noreferrer">https://github.com/polixir/NeoRL</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/polixir/NeoRL" target="_blank" rel="nofollow noreferrer">https://github.com/polixir/NeoRL</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All datasets are licensed under the [Creative Commons Attribution 4.0 License (CC BY)](https://creativecommons.org/licenses/by/4.0/), and code is licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0.html).</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="XCl6ylbr_uy" data-number="167">
        <h4>
          <a href="/forum?id=XCl6ylbr_uy">
              Benchmarking the Multi-task Learning for Unified Perception in Autonomous Driving
          </a>


            <a href="/pdf?id=XCl6ylbr_uy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xiwen_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiwen_Liang1">Xiwen Liang</a>, <a href="/profile?id=~Minzhe_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minzhe_Niu1">Minzhe Niu</a>, <a href="/profile?id=~Hang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Xu1">Hang Xu</a>, <a href="/profile?id=~Jianhua_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhua_Han1">Jianhua Han</a>, <a href="/profile?id=~Yangxin_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangxin_Wu2">Yangxin Wu</a>, <a href="/profile?id=~Chunjing_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chunjing_Xu1">Chunjing Xu</a>, <a href="/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#XCl6ylbr_uy-details-922" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XCl6ylbr_uy-details-922"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multi-task learning has emerged as a powerful paradigm to solve a range of tasks simultaneously with good efficiency in both computation resources and inference time. However, these algorithms have been hard to develop and compare in autonomous driving due to the lack of a standardized benchmark for unified perception in autonomous driving. To this end, we introduce the self-driving multi-task benchmark (MTL-Driv), aiming to enable the comprehensive evaluation of present multi-task learning methods in autonomous driving. Specifically, this benchmark covers four popular perception tasks, i.e., object detection, semantic segmentation, drivable area segmentation, and lane detection. We provide an in-depth analysis of current multi-task learning methods under different settings with visual transformer backbones and find that the existing methods make progress but there is still a gap in performance compared with single-task baselines. We note that the fixed single-scale window in the visual transformer may limit the modeling potential for task-specific multi-scale information. To address this problem, we present the task-prompt driven window selection and weighting module (shortened as TPrompt), in which the task prompts encode task-specific information and guide the model to choose optimal window sizes for each task. Comprehensive experimental results on large-scale self-driving datasets (i.e., BDD100K and nuImages) show that the plug-and-play TPrompt module improves multiple multi-task baselines. Sincerely, we hope this benchmark can accelerate the research for unified perception in autonomous driving. More information can refer to \url{https://self-driving-multi-task-benchmark.github.io}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=XCl6ylbr_uy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://self-driving-multi-task-benchmark.github.io" target="_blank" rel="nofollow noreferrer">https://self-driving-multi-task-benchmark.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="yPZ7w29qSNK" data-number="166">
        <h4>
          <a href="/forum?id=yPZ7w29qSNK">
              Towards Video Text Visual Question Answering: Benchmark and Baseline
          </a>


            <a href="/pdf?id=yPZ7w29qSNK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Minyi_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minyi_Zhao1">Minyi Zhao</a>, <a href="/profile?id=~Bingjia_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bingjia_Li1">Bingjia Li</a>, <a href="/profile?id=~Jie_Wang17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Wang17">Jie Wang</a>, <a href="/profile?id=~Wanqing_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wanqing_Li3">Wanqing Li</a>, <a href="/profile?id=~Wenjing_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenjing_Zhou1">Wenjing Zhou</a>, <a href="/profile?id=~Lan_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lan_Zhang3">Lan Zhang</a>, <a href="/profile?id=~Shijie_Xuyang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shijie_Xuyang1">Shijie Xuyang</a>, <a href="/profile?id=~Zhihang_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhihang_Yu1">Zhihang Yu</a>, <a href="/profile?id=~Xinkun_Yu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinkun_Yu2">Xinkun Yu</a>, <a href="/profile?id=~Guangze_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guangze_Li1">Guangze Li</a>, <a href="/profile?id=~Aobotao_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aobotao_Dai1">Aobotao Dai</a>, <a href="/profile?id=~Shuigeng_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuigeng_Zhou1">Shuigeng Zhou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#yPZ7w29qSNK-details-787" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yPZ7w29qSNK-details-787"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">video text visual question answering, text-based visual question answering, video question answering</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A new dataset M4-ViteQA and a new method T5-ViteVQA for a new task video text visual question answering.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There are already some text-based visual question answering (TextVQA) benchmarks for developing machine's ability to answer questions based on texts in images in recent years. However, models developed on these benchmarks cannot work effectively in many real-life scenarios (e.g. traffic monitoring, shopping ads and e-learning videos) where temporal reasoning ability is required. To this end, we propose a new task named Video Text Visual Question Answering (ViteVQA in short) that aims at answering questions by reasoning texts and visual information spatiotemporally in a given video. In particular, on the one hand, we build the first ViteVQA benchmark dataset named M4-ViteVQA --- the abbreviation of Multi-category Multi-frame Multi-resolution Multi-modal benchmark for ViteVQA, which contains 7,620 video clips of 9 categories (i.e., shopping, traveling, driving, vlog, sport, advertisement, movie, game and talking) and 3 kinds of resolutions (i.e., 720p, 1080p and 1176x664), and 25,123 question-answer pairs. On the other hand, we develop a baseline method named T5-ViteVQA for the ViteVQA task. T5-ViteVQA consists of five transformers. It first extracts optical character recognition (OCR) tokens, question features, and video representations via two OCR transformers, one language transformer and one video-language transformer, respectively. Then, a multimodal fusion transformer and an answer generation module are applied to fusing multimodal information and generating the final prediction. Extensive experiments on M4-ViteVQA demonstrate the superiority of T5-ViteVQA to the existing approaches of TextVQA and VQA tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=yPZ7w29qSNK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://github.com/bytedance/VTVQA for annotations; https://drive.google.com/file/d/1XuPMW9hcWWjuTgjjQBb89j7WFn-tiCJU/view for videos, frames and features. Please note that when  the reviewers download the videos, we assume that they have accepted the responsibility agreement.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/bytedance/VTVQA" target="_blank" rel="nofollow noreferrer">https://github.com/bytedance/VTVQA</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The researcher shall use the M4-ViteVQA dataset only for non-commercial algorithm research and educational purposes. The researcher can not use the M4-ViteVQA dataset for any other purposes, including but not limited to distribution, commercial usage, etc...
        The researcher takes full responsibility for his or her use of the M4-ViteVQA dataset and shall defend and indemnify the dataset, including their affiliates, employees, trustees, officers and agents, against any and all claims arising from the researcher’s use of the M4-ViteVQA dataset.
        The researcher agrees and confirms that authors reserve the right to terminate the researcher’s access to the M4-ViteVQA dataset at any time.
        If the researcher is employed by a for-profit business entity, the researcher’s employer shall also be bound by these terms and conditions, and the researcher hereby shall represent that he or she is fully authorized to enter into this agreement on behalf of such employer.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="foA_SFQ9zo0" data-number="165">
        <h4>
          <a href="/forum?id=foA_SFQ9zo0">
              ADBench: Anomaly Detection Benchmark
          </a>


            <a href="/pdf?id=foA_SFQ9zo0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Songqiao_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songqiao_Han1">Songqiao Han</a>, <a href="/profile?id=~Xiyang_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiyang_Hu1">Xiyang Hu</a>, <a href="/profile?id=~Hailiang_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hailiang_Huang1">Hailiang Huang</a>, <a href="/profile?id=~Minqi_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minqi_Jiang2">Minqi Jiang</a>, <a href="/profile?id=~Yue_Zhao13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Zhao13">Yue Zhao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#foA_SFQ9zo0-details-125" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="foA_SFQ9zo0-details-125"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">anomaly detection, outlier detection, tabular data, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The most comprehensive anomaly detection benchmark including 30 algorithms and 55 datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 55 benchmark datasets, named ADBench. Our extensive experiments (93,654 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=foA_SFQ9zo0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Minqi824/ADBench" target="_blank" rel="nofollow noreferrer">https://github.com/Minqi824/ADBench</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">To facilitate the reproducibility and fast experimental pipeline for the anomaly detection benchmark, we have made all the benchmark datasets and algorithms publicly available with BSD-2 License at https://github.com/Minqi824/ADBench, and welcome any customized algorithms to be evaluated via the plug-and-play testbed of ADBench.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD-2</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ApQTT2fsYRE" data-number="164">
        <h4>
          <a href="/forum?id=ApQTT2fsYRE">
              Benchmarking Robustness of 3D Point Cloud Recognition against Common Corruptions
          </a>


            <a href="/pdf?id=ApQTT2fsYRE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiachen_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiachen_Sun1">Jiachen Sun</a>, <a href="/profile?email=qzzhang%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="qzzhang@umich.edu">Qingzhao Zhang</a>, <a href="/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>, <a href="/profile?id=~Zhiding_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiding_Yu1">Zhiding Yu</a>, <a href="/profile?id=~Chaowei_Xiao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaowei_Xiao2">Chaowei Xiao</a>, <a href="/profile?id=~Zhuoqing_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuoqing_Mao1">Zhuoqing Mao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 11 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ApQTT2fsYRE-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ApQTT2fsYRE-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Point Cloud Recognition, Corruption Robustness, Data Augmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we present ModelNet40-C, a comprehensive dataset on 3D point cloud corruption robustness and we exhaustively benchmark 5,700 different experimental configurations. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep neural networks on 3D point cloud data have been widely used in the real world, especially in safety-critical applications. However, their robustness against corruptions is less studied. In this paper, we present ModelNet40-C, a comprehensive benchmark on 3D point cloud corruption robustness, consisting of 15 common and realistic corruptions. Our evaluation shows a significant gap between the performances on ModelNet40 and ModelNet40-C for state-of-the-art models. To reduce the gap, we propose a simple but effective method by combining PointCutMix-R and TENT after evaluating a wide range of augmentation and test-time adaptation strategies. We identify a number of critical insights for future studies on corruption robustness in point cloud recognition. For instance, we unveil that Transformer-based architectures with proper training recipes achieve the strongest robustness. We hope our in-depth analysis will motivate the development of robust training strategies or architecture designs in the 3D point cloud domain. Our codebase and dataset are included in https://github.com/jiachens/ModelNet40-C.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ApQTT2fsYRE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/jiachens/ModelNet40-C" target="_blank" rel="nofollow noreferrer">https://github.com/jiachens/ModelNet40-C</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/umich.edu/modelnet40c" target="_blank" rel="nofollow noreferrer">https://sites.google.com/umich.edu/modelnet40c</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD 3-Clause License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="QeuwINa96C" data-number="163">
        <h4>
          <a href="/forum?id=QeuwINa96C">
              USB: A Unified Semi-supervised Learning Benchmark
          </a>


            <a href="/pdf?id=QeuwINa96C" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yidong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yidong_Wang1">Yidong Wang</a>, <a href="/profile?id=~Hao_Chen15" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Chen15">Hao Chen</a>, <a href="/profile?id=~Yue_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Fan1">Yue Fan</a>, <a href="/profile?id=~Wang_SUN1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wang_SUN1">Wang SUN</a>, <a href="/profile?id=~Ran_Tao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ran_Tao2">Ran Tao</a>, <a href="/profile?id=~Wenxin_Hou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenxin_Hou1">Wenxin Hou</a>, <a href="/profile?id=~Renjie_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renjie_Wang1">Renjie Wang</a>, <a href="/profile?id=~Linyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linyi_Yang1">Linyi Yang</a>, <a href="/profile?id=~Zhi_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhi_Zhou2">Zhi Zhou</a>, <a href="/profile?id=~Lan-Zhe_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lan-Zhe_Guo2">Lan-Zhe Guo</a>, <a href="/profile?id=~Heli_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heli_Qi1">Heli Qi</a>, <a href="/profile?id=~Zhen_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhen_Wu2">Zhen Wu</a>, <a href="/profile?id=~Yu-Feng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu-Feng_Li1">Yu-Feng Li</a>, <a href="/profile?email=s-nakamura%40is.naist.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="s-nakamura@is.naist.jp">Satoshi Nakamura</a>, <a href="/profile?id=~Wei_Ye2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Ye2">Wei Ye</a>, <a href="/profile?id=~Marios_Savvides1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marios_Savvides1">Marios Savvides</a>, <a href="/profile?id=~Bhiksha_Raj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bhiksha_Raj1">Bhiksha Raj</a>, <a href="/profile?id=~Takahiro_Shinozaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Takahiro_Shinozaki1">Takahiro Shinozaki</a>, <a href="/profile?id=~Bernt_Schiele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernt_Schiele1">Bernt Schiele</a>, <a href="/profile?id=~Jindong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jindong_Wang1">Jindong Wang</a>, <a href="/profile?id=~Xing_Xie3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xing_Xie3">Xing Xie</a>, <a href="/profile?id=~Yue_Zhang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Zhang7">Yue Zhang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#QeuwINa96C-details-596" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QeuwINa96C-details-596"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Semi-supervised Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation on these SSL methods. We further provide pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 37 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with the typical protocol.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=QeuwINa96C&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/microsoft/Semi-supervised-learning" target="_blank" rel="nofollow noreferrer">https://github.com/microsoft/Semi-supervised-learning</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/microsoft/Semi-supervised-learning" target="_blank" rel="nofollow noreferrer">https://github.com/microsoft/Semi-supervised-learning</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FhqzyGoTSH" data-number="162">
        <h4>
          <a href="/forum?id=FhqzyGoTSH">
              CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks
          </a>


            <a href="/pdf?id=FhqzyGoTSH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tejas_Srinivasan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tejas_Srinivasan1">Tejas Srinivasan</a>, <a href="/profile?id=~Ting-Yun_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ting-Yun_Chang1">Ting-Yun Chang</a>, <a href="/profile?id=~Leticia_Leonor_Pinto_Alva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leticia_Leonor_Pinto_Alva1">Leticia Leonor Pinto Alva</a>, <a href="/profile?id=~Georgios_Chochlakis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgios_Chochlakis1">Georgios Chochlakis</a>, <a href="/profile?id=~Mohammad_Rostami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Rostami1">Mohammad Rostami</a>, <a href="/profile?id=~Jesse_Thomason1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Thomason1">Jesse Thomason</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">04 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#FhqzyGoTSH-details-970" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FhqzyGoTSH-details-970"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision-and-language, continual learning, multimodal, lifelong learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents CLiMB, a benchmark to study the challenge of learning vision-language tasks in a continual learning setting, and to systematically evaluate how upstream continual learning can rapidly transfer to new multi- and unimodal tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating "catastrophic forgetting", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FhqzyGoTSH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/GLAMOR-USC/CLiMB" target="_blank" rel="nofollow noreferrer">https://github.com/GLAMOR-USC/CLiMB</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>