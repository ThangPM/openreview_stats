<ul class="list-unstyled submissions-list">
    <li class="note " data-id="9-qUAxF8q9l" data-number="334">
        <h4>
          <a href="/forum?id=9-qUAxF8q9l">
              Simulating Environments for Evaluating Scarce Resource Allocation Policies
          </a>


            <a href="/pdf?id=9-qUAxF8q9l" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jeroen_Berrevoets1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeroen_Berrevoets1">Jeroen Berrevoets</a>, <a href="/profile?id=~Alex_Chan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Chan2">Alex Chan</a>, <a href="/profile?id=~Daniel_Jarrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jarrett1">Daniel Jarrett</a>, <a href="/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#9-qUAxF8q9l-details-521" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9-qUAxF8q9l-details-521"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">simulation, policy evaluation, counterfactual inference</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Scarce resource allocation policies require thorough testing before deployed, we provide a data-based simulation framework (with code) to do this.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Consider the sequential decision problem of allocating a limited supply of resources to a pool of potential recipients: This scarce resource allocation problem arises in a variety of settings characterized by “hard-to-make” tradeoffs—such as assigning organs to transplant patients, or rationing ventilators in overstretched ICUs. Assisting human judgement in these choices are dynamic allocation policies that prescribe how to match available assets to an evolving pool of beneficiaries—such as clinical guidelines that stipulate selection criteria on the basis of patient and organ attributes. However, while such policies have received increasing attention in recent years, a key challenge lies in pre-deployment evaluation: How might allocation policies behave in the real world? In particular, in addition to conventional backtesting, it is crucial that policies be evaluated on a variety of possible scenarios and sensitivities— such as distributions of patients and organs that may diverge from historic patterns. In this work, we present AllSim, an open-source framework for performing data-driven simulation of scarce resource allocation policies for pre-deployment evaluation. Simulation environments are modular (i.e. parameterized componentwise), learnable (i.e. on historical data), and customizable (i.e. to unseen conditions), and— upon interaction with a policy —outputs a dataset of simulated outcomes for analysis and benchmarking. Compared to existing work, we believe this approach takes a step towards more methodical evaluation of scarce resource allocation policies.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9-qUAxF8q9l&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We have included our evaluation package in the supplemental material.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="4FNkIKC3fsE" data-number="333">
        <h4>
          <a href="/forum?id=4FNkIKC3fsE">
              SIMBA: Split Inference - Mechanisms, Benchmarks and Attacks
          </a>


            <a href="/pdf?id=4FNkIKC3fsE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Abhishek_Singh5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Singh5">Abhishek Singh</a>, <a href="/profile?id=~Justin_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Yu1">Justin Yu</a>, <a href="/profile?id=~John_Mose1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Mose1">John Mose</a>, <a href="/profile?id=~Rohan_Sukumaran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rohan_Sukumaran1">Rohan Sukumaran</a>, <a href="/profile?id=~Jeffrey_Chiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeffrey_Chiu1">Jeffrey Chiu</a>, <a href="/profile?id=~Emily_T_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_T_Zhang1">Emily T Zhang</a>, <a href="/profile?id=~Vivek_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vivek_Sharma1">Vivek Sharma</a>, <a href="/profile?id=~Ramesh_Raskar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ramesh_Raskar2">Ramesh Raskar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#4FNkIKC3fsE-details-546" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4FNkIKC3fsE-details-546"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">privacy, adversarial obfuscation, empirical defense, empirical attacks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a benchmark and a framework for comparing defense and attack techniques in private DNN representations.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we tackle the question of how to benchmark the reconstruction of inputs from deep neural networks~(DNN) representations. This inverse problem is of great importance in the privacy community where obfuscation of features has been proposed as a technique for privacy-preserving machine learning(ML) inference. In this benchmark, we characterize different obfuscation techniques and design different attack models. We propose multiple reconstruction techniques based upon distinct background knowledge of the adversary. We develop a modular platform that integrates different obfuscation techniques, reconstruction algorithms, and evaluation metrics under a common framework. Using our platform, we benchmark various obfuscation and reconstruction techniques for evaluating their privacy-utility trade-off. Finally, we release a dataset of obfuscated representations to foster research in this area. We have open-sourced code, dataset, hyper-parameters, and trained models that can be found at https://tiny.cc/simba</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=4FNkIKC3fsE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/splitlearning/InferenceBenchmark/" target="_blank" rel="nofollow noreferrer">https://github.com/splitlearning/InferenceBenchmark/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1zUM2s2qCcSFFfI0FW9zkah6VUGD0rwS2" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1zUM2s2qCcSFFfI0FW9zkah6VUGD0rwS2</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="IBg_xblL3F2" data-number="332">
        <h4>
          <a href="/forum?id=IBg_xblL3F2">
              SDWPF: A Dataset for Spatial Dynamic Wind Power Forecasting Challenge at KDD Cup 2022
          </a>


            <a href="/pdf?id=IBg_xblL3F2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jingbo_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingbo_Zhou1">Jingbo Zhou</a>, <a href="/profile?id=~Xinjiang_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinjiang_Lu2">Xinjiang Lu</a>, <a href="/profile?email=xiaoyixiong%40baidu.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="xiaoyixiong@baidu.com">Yixiong Xiao</a>, <a href="/profile?id=~Yu_Li14" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Li14">Yu Li</a>, <a href="/profile?id=~Ji_Liu8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ji_Liu8">Ji Liu</a>, <a href="/profile?email=12091329%40chnenergy.com.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="12091329@chnenergy.com.cn">Jiantao Su</a>, <a href="/profile?email=lvjf%40mail.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="lvjf@mail.tsinghua.edu.cn">Junfu Lyu</a>, <a href="/profile?email=mayanjun02%40baidu.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mayanjun02@baidu.com">Yanjun Ma</a>, <a href="/profile?id=~Dejing_Dou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dejing_Dou3">Dejing Dou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#IBg_xblL3F2-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IBg_xblL3F2-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While wind power serves as a clean and safe source of renewable energy, the variability of wind power supply generally incurs substantial challenges to integrating wind power into the grid system. Thus, Wind Power Forecasting (WPF) has been widely recognized as one of the most critical issues in wind power integration and operation. Though there are a few publicly available WPF datasets, most of which have only a small number of wind turbines and/or are without knowing the locations and dynamic context information of each turbine at a fine-grained time scale. To promote and facilitate the advancements of WPF research, we present a unique Spatial Dynamic Wind Power Forecasting dataset, i.e., SDWPF, which consists of the data with 134 wind turbines from a wind farm for more than half a year. SDWPF includes the spatial distribution of wind turbines, as well as the dynamic context factors for each turbine. Moreover, we exploit this dataset to launch the ACM KDD Cup 2022 which has attracted more than 2, 200 registration teams over the world. The SDWPF dataset and all the challenge information about the KDD Cup are released at https://aistudio.baidu.com/aistudio/competition/detail/152/0/. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">

        The dataset can be downloaded from the following link: https://bj.bcebos.com/v1/ai-studio-online/85b5cb4eea5a4f259766f42a448e2c04a7499c43e1ae4cc28fbdee8e087e2385?responseContentDisposition=attachment%3B%20filename%3Dwtbdata_245days.csv&amp;authorization=bce-auth-v1%2F0ef6765c1e494918bc0d4c3ca3e5c6d1%2F2022-05-05T14%3A17%3A03Z%2F-1%2F%2F5932bfb6aa3af1bcfb467bf2a4a6877f8823fe96c6f4fd0d4a3caa722354e3ac


        The data for the relative position of all wind turbines can be downloaded here:https://bj.bcebos.com/v1/ai-studio-online/e927ce742c884955bf2a667929d36b2ef41c572cd6e245fa86257ecc2f7be7bc?responseContentDisposition=attachment%3B%20filename%3Dsdwpf_baidukddcup2022_turb_location.CSV&amp;authorization=bce-auth-v1%2F0ef6765c1e494918bc0d4c3ca3e5c6d1%2F2022-04-11T08%3A27%3A09Z%2F-1%2F%2Fcf377452dbd186873680f2f0fe39200b3de86083a036da220ab8a02abc5a8032



        Currently, the dataset can be downloaded from our website after registration: https://aistudio.baidu.com/aistudio/competition/detail/152/0/introduction



        </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We use this dataset to launch the ACM KDD Cup 2022, therefore it is still required to register to obtain the dataset for the public. The dataset will be publicly available and archived after the KDD Cup (which should be no later than August 15, 2022). </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The SDWPF dataset and related code are released under Apache 2.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KqVMeThbhYB" data-number="331">
        <h4>
          <a href="/forum?id=KqVMeThbhYB">
              A Dataset on Malicious Paper Bidding in Peer Review
          </a>


            <a href="/pdf?id=KqVMeThbhYB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Steven_Jecmen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Jecmen1">Steven Jecmen</a>, <a href="/profile?id=~Minji_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minji_Yoon1">Minji Yoon</a>, <a href="/profile?id=~Vincent_Conitzer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vincent_Conitzer2">Vincent Conitzer</a>, <a href="/profile?id=~Nihar_B_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nihar_B_Shah1">Nihar B Shah</a>, <a href="/profile?id=~Fei_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Fang1">Fei Fang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#KqVMeThbhYB-details-750" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KqVMeThbhYB-details-750"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">peer review, paper bidding, manipulation, assignment</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a dataset on malicious paper bidding in peer review along with analysis.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality paper assignment. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment (e.g., aiming to be assigned to a friend's paper), crucially undermining the peer review process. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, including our categorization of different strategies employed by participants. Finally, we evaluate the ability of each strategy to manipulate the assignment, and also evaluate the performance of some simple algorithms meant to detect malicious bidding. The performance of these detection algorithms can be taken as a baseline for future research on detecting malicious bidding. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KqVMeThbhYB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sjecmen/malicious_bidding_dataset" target="_blank" rel="nofollow noreferrer">https://github.com/sjecmen/malicious_bidding_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/sjecmen/malicious_bidding_dataset" target="_blank" rel="nofollow noreferrer">https://github.com/sjecmen/malicious_bidding_dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This dataset is licensed under a CC BY 4.0 license (https://creativecommons.org/licenses/by/4.0/).</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="EONuSdDjJrp" data-number="330">
        <h4>
          <a href="/forum?id=EONuSdDjJrp">
              MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwrting Verification
          </a>


            <a href="/pdf?id=EONuSdDjJrp" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peirong_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peirong_Zhang1">Peirong Zhang</a>, <a href="/profile?id=~Jiajia_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajia_Jiang1">Jiajia Jiang</a>, <a href="/profile?id=~Yuliang_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuliang_Liu2">Yuliang Liu</a>, <a href="/profile?id=~Lianwen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lianwen_Jin1">Lianwen Jin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#EONuSdDjJrp-details-129" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EONuSdDjJrp-details-129"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Handwriting Verification, Datasets, Handwritten Token Digit String, Handwritten Chinese Signature, Multimodal</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Although online handwriting verification has made great progress recently, the verification performances are still far behind the real usage owing to the small scale of the datasets as well as the limited biometric mediums. Therefore, this paper proposes a new handwriting verification benchmark dataset named Multimodal Signature and Digit String (MSDS), which consists of two subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings), contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to the best of our knowledge, is the largest publicly available Chinese signature dataset for handwriting verification, at least eight times larger than existing ones. Meanwhile, MSDS-TDS consists of handwritten Token Digit Strings, i.e, the actual phone numbers of users, which have not been explored yet. Extensive experiments with different baselines are respectively conducted for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of state-of-the-art methods on MSDS-TDS are generally better than those on MSDS-ChS, which indicates that the handwritten Token Digit String could be a more effective biometric than handwritten Chinese signature. This is a promising discovery that could inspire us to explore new biometric traits.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=EONuSdDjJrp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Our dataset is publicly available at https://github.com/sincert/MSDS.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) Public License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="4nAe0PS7D-l" data-number="329">
        <h4>
          <a href="/forum?id=4nAe0PS7D-l">
              PROSPECT: Labeled Tandem Mass Spectrometry Dataset for Machine Learning in Proteomics
          </a>


            <a href="/pdf?id=4nAe0PS7D-l" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Omar_Shouman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Omar_Shouman1">Omar Shouman</a>, <a href="/profile?id=~Wassim_Gabriel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wassim_Gabriel1">Wassim Gabriel</a>, <a href="/profile?id=~Victor-George_Giurcoiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Victor-George_Giurcoiu1">Victor-George Giurcoiu</a>, <a href="/profile?email=vitor.sternlicht%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="vitor.sternlicht@tum.de">Vitor Sternlicht</a>, <a href="/profile?email=mathias.wilhelm%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="mathias.wilhelm@tum.de">Mathias Wilhelm</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#4nAe0PS7D-l-details-170" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4nAe0PS7D-l-details-170"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Proteomics, Deep Learning, Machine Learning, Dataset, Mass Spectrometry, Retention Time, Annotated Spectra, Neutral Losses, ProteomeTools, Fragment Ions, Intensity</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The paper introduces a labeled tandem Mass Spectrometry dataset for machine learning in proteomics and recommends evaluation metrics.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Proteomics is the interdisciplinary field focusing on the large-scale study of proteins. Proteins essentially organize and execute all functions within organisms. Today, the bottom-up analysis approach is the most commonly used workflow, where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). MS-based proteomics has transformed various fields in life sciences, such as drug discovery and biomarker identification. Today, proteomics is entering a phase where it is helpful for clinical decision-making. Computational methods are vital in turning large amounts of acquired raw MS data into information and, ultimately, knowledge. Deep learning has proved its success in multiple domains as a robust framework for supervised and unsupervised machine learning problems. In proteomics, scientists are increasingly leveraging the potential of deep learning to predict the properties of peptides based on their sequence to improve their confident identification. However, a reference dataset is missing, covering several proteomics tasks, enabling performance comparison, and evaluating reproducibility and generalization. Here, we present a large labeled proteomics dataset spanning several tasks in the domain to address this challenge. We focus on two common applications: peptide retention time and MS/MS spectrum prediction. We review existing methods and task formulations from a machine learning perspective and recommend suitable evaluation metrics and visualizations. With an accessible dataset, we aim to lower the entry barrier and enable faster development in machine learning for proteomics.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=4nAe0PS7D-l&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.5281/zenodo.6602020" target="_blank" rel="nofollow noreferrer">https://doi.org/10.5281/zenodo.6602020</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://doi.org/10.5281/zenodo.6602020
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset license: Creative Commons Attribution 4.0 International

        Supplementary code license: MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FUMEGI1DyCF" data-number="328">
        <h4>
          <a href="/forum?id=FUMEGI1DyCF">
              WILD-SCAV: Benchmarking Deep Reinforcement Learning Algorithms in 3D Open-World Games
          </a>


            <a href="/pdf?id=FUMEGI1DyCF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xi_CHEN31" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_CHEN31">Xi CHEN</a>, <a href="/profile?id=~TIANYU_SHI2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~TIANYU_SHI2">TIANYU SHI</a>, <a href="/profile?id=~Qingpeng_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qingpeng_Zhao1">Qingpeng Zhao</a>, <a href="/profile?id=~Yuchen_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuchen_Sun1">Yuchen Sun</a>, <a href="/profile?id=~yunfei_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~yunfei_Gao1">yunfei Gao</a>, <a href="/profile?id=~Xiangjun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangjun_Wang1">Xiangjun Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#FUMEGI1DyCF-details-857" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FUMEGI1DyCF-details-857"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, deep reinforcement learning, multi-agent reinforcement learning, game AI</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances in deep reinforcement learning (RL) have demonstrated complex decision-making capabilities in simulation environments such as Arcade Learning Environment\cite{bellemare2013arcade}, MuJoCo\cite{todorov2012mujoco} and ViZDoom~\cite{kempka2016vizdoom} etc. However, they are hardly extensible to more complicated real-world problems, mainly due to the lack of complexity and variations in the environments they are trained and evaluated. To learn intelligent agents with more general task-solving capabilities that resemble real-world problems, it is imperative to develop environments with greater diversity and complexity. To bridge the gap, we developed WILD-SCAV, a powerful and extensible environment based on a 3D open-world FPS game. It provides realistic 3D environments of variable complexity, various tasks, and multiple modes of interaction, where agents can learn to perceive 3D environments, navigate and plan, compete and cooperate, similar to humans in the real world environment. WILD-SCAV supports customized complexities such as configurable maps with different terrains, building structures, and distributions, and also multi-agent settings with cooperative and competitive tasks. The experimental results on configurable complexity, multi-tasking, and multi-agent scenarios demonstrate the effectiveness of WILD-SCAV in benchmarking various RL algorithms as well as its potential in giving rise to intelligent agents with generalized task-solving abilities.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FUMEGI1DyCF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/inspirai/wilderness-scavenger" target="_blank" rel="nofollow noreferrer">https://github.com/inspirai/wilderness-scavenger</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fJZEqKer6HN" data-number="327">
        <h4>
          <a href="/forum?id=fJZEqKer6HN">
              FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization
          </a>


            <a href="/pdf?id=fJZEqKer6HN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zhen_WANG2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhen_WANG2">Zhen WANG</a>, <a href="/profile?id=~Weirui_Kuang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weirui_Kuang2">Weirui Kuang</a>, <a href="/profile?id=~Ce_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ce_Zhang1">Ce Zhang</a>, <a href="/profile?id=~Bolin_Ding3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bolin_Ding3">Bolin Ding</a>, <a href="/profile?id=~Yaliang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaliang_Li1">Yaliang Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#fJZEqKer6HN-details-584" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fJZEqKer6HN-details-584"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, hyperparameter optimization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a comprehensive, efficient, and extensible benchmark suite for federated hyperparameter optimization. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Hyperparameter optimization (HPO) is crucial for machine learning algorithms to achieve satisfactory performance, whose progress has been boosted by related benchmarks. Nonetheless, existing efforts in benchmarking all focus on HPO for traditional centralized learning while ignoring federated learning (FL), a promising paradigm for collaboratively learning models from dispersed data. In this paper, we first identify some uniqueness of HPO for FL algorithms from various aspects. Due to this uniqueness, comparing HPO methods on non-FL tasks cannot reflect their performance on FL tasks, and existing benchmarks are inapplicable for HPO methods deliberately designed for the FL setting. To facilitate the research of HPO in the FL scenarios, we propose and implement a benchmark suite FedHPO-B that incorporates comprehensive FL tasks, enables efficient function evaluations, and eases continuing extensions. We also conduct extensive experiments based on FedHPO-B to benchmark a few HPO methods. We open-source FedHPO-B at https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOB.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fJZEqKer6HN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOB" target="_blank" rel="nofollow noreferrer">https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOB</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The proposed FedHPO-B is built upon the open-sourced Federated Learning package, FederatedScope (https://github.com/alibaba/FederatedScope). Both the proposed benchmark and the leveraged package are released with an Apache-2.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Cs2hwgBmPTL" data-number="326">
        <h4>
          <a href="/forum?id=Cs2hwgBmPTL">
              SurvBoard: Standardised Benchmarking for Multi-omics Cancer Survival Models
          </a>


            <a href="/pdf?id=Cs2hwgBmPTL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~David_Wissel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Wissel1">David Wissel</a>, <a href="/profile?id=~Nikita_Janakarajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikita_Janakarajan1">Nikita Janakarajan</a>, <a href="/profile?id=~Aayush_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aayush_Grover1">Aayush Grover</a>, <a href="/profile?id=~Enrico_Toniato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enrico_Toniato1">Enrico Toniato</a>, <a href="/profile?id=~Maria_Rodriguez_Martinez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Rodriguez_Martinez1">Maria Rodriguez Martinez</a>, <a href="/profile?email=valentina.boeva%40inf.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="valentina.boeva@inf.ethz.ch">Valentina Boeva</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Cs2hwgBmPTL-details-654" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Cs2hwgBmPTL-details-654"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">survival analysis, multi-omics, cancer, censoring, TCGA, ICGC, benchmark, leaderboard</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We proposed a benchmark for multi-omics survival prediction that standardises the validation process. Also, we performed an empirical study, showing that statistical models outperformed neural methods, especially in terms of calibration.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, there has been a drastic increase in the generation and usage of high-throughput ``omics'' data, such as genomic, transcriptomic, and epigenetic data. Researchers are now using multi-modal omics data to predict the survival of cancer patients, often in addition to clinical data. Despite these advances, there is a need for standardization of several factors. These include preprocessing, the choice of cancer types, and other degrees of freedom such as validation methods for models that were trained on multiple cancer types. We propose a novel benchmark, SurvBoard, which standardises various experimental design choices to enable comparability between cancer survival methods incorporating multi-omics data. We provide a web service allowing simple evaluation of models on our benchmark. We also highlight several potential pitfalls relating to both preprocessing and validation of multi-omics cancer survival models which SurvBoard aims to remedy. Lastly, we benchmark neural and statistical models revealing that statistical models tend to outperform, especially in terms of calibration. All code and other resources are available on \href{https://github.com/BoevaLab/survboard/}{Github}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Cs2hwgBmPTL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/BoevaLab/survboard" target="_blank" rel="nofollow noreferrer">https://github.com/BoevaLab/survboard</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="vfUNn6ypVG7" data-number="325">
        <h4>
          <a href="/forum?id=vfUNn6ypVG7">
              ATVM: New Benchmarks for Accountable Text-based Visual Manipulation
          </a>


            <a href="/pdf?id=vfUNn6ypVG7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuliang_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuliang_Liu2">Yuliang Liu</a>, <a href="/profile?id=~Zhiwei_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwei_Zhang3">Zhiwei Zhang</a>, <a href="/profile?id=~Hao_Lu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Lu4">Hao Lu</a>, <a href="/profile?id=~Dahua_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dahua_Lin1">Dahua Lin</a>, <a href="/profile?id=~Hongsheng_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongsheng_Li3">Hongsheng Li</a>, <a href="/profile?id=~Lianwen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lianwen_Jin1">Lianwen Jin</a>, <a href="/profile?id=~Xiang_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Bai1">Xiang Bai</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#vfUNn6ypVG7-details-5" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vfUNn6ypVG7-details-5"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">text-based image manipulation, visual, lanugage</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct datasets for a novel task, in which the algorithm is required to both execute manipulation and provide feedback.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Given a command, humans can execute a task with both actions and feedback, which is crucial in many situations where uncertain behaviors are required to be controlled. However, existing benchmarks do not integrate text-based image manipulation and visual reasoning tasks into a unified dataset, limiting the development of human-like intelligence. In this paper, we construct datasets for such a novel task, termed Accountable Text-based Visual Manipulation (ATVM), in which the objects in an image will be manipulated conditioned on the user queries, and the machine is responsible for simultaneously generating the feedback to tell whether the actions can be done. To explore the feasibility of our proposal, three new datasets, including two synthetic datasets, CLEVR-ATVM-S (93k), CLEVR-ATVM-M (620k), and one manually pictured dataset, FruitATVM (50k), are created to analyze the quality of image generation, accuracy of answer, and uncertainty behavior under imperfect user queries.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=vfUNn6ypVG7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Yuliang-Liu/ATVM-Datasets" target="_blank" rel="nofollow noreferrer">https://github.com/Yuliang-Liu/ATVM-Datasets</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Yuliang-Liu/ATVM-Datasets" target="_blank" rel="nofollow noreferrer">https://github.com/Yuliang-Liu/ATVM-Datasets</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">30/06/2022</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="PfuW84q25y9" data-number="324">
        <h4>
          <a href="/forum?id=PfuW84q25y9">
              Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems
          </a>


            <a href="/pdf?id=PfuW84q25y9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Guanghu_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guanghu_Yuan1">Guanghu Yuan</a>, <a href="/profile?id=~Fajie_Yuan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fajie_Yuan2">Fajie Yuan</a>, <a href="/profile?id=~Beibei_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beibei_Kong1">Beibei Kong</a>, <a href="/profile?id=~Lei_Chen16" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Chen16">Lei Chen</a>, <a href="/profile?id=~Min_Yang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Min_Yang6">Min Yang</a>, <a href="/profile?id=~Chenyun_YU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenyun_YU1">Chenyun YU</a>, <a href="/profile?id=~Bo_Hu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Hu7">Bo Hu</a>, <a href="/profile?id=~Zang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zang_Li2">Zang Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#PfuW84q25y9-details-931" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PfuW84q25y9-details-931"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Recommendation, Large-scale, Multipurpose, Dataset, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing benchmark datasets for recommender systems (RS)  either are created  at a small scale or involve very limited forms of user feedback. RS models evaluated on such datasets often lack practical values for large-scale real-world applications. In this paper, we describe Tenrec, a novel and publicly available data collection for RS that records various user feedback from four different recommendation scenarios. To be specific, Tenrec has the following five characteristics: (1) it is large-scale, containing around 5 million users and 140 million interactions; (2) it has not only positive user feedback, but also true  negative feedback (vs. one-class recommendation); (3) it contains overlapped users and items across four different scenarios; (4) it contains various types of  user positive feedback, in forms of clicking, liking, sharing, and following, etc; (5) it contains additional features beyond the user IDs and item IDs. We verify Tenrec on ten diverse  recommendation  tasks by running several classical baseline models per task. Tenrec has the potential to become a  useful benchmark dataset for a majority of popular recommendation tasks.  Our source codes and datasets will be included  in supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=PfuW84q25y9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "> https://drive.google.com/file/d/1R1JhdT9CHzT3qBJODz09pVpHMzShcQ7a/view?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "> https://drive.google.com/file/d/1R1JhdT9CHzT3qBJODz09pVpHMzShcQ7a/view?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset License: CC BY NC 4.0
        Code License: Apache License 2.0  </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="GP1Ncd8nTgn" data-number="323">
        <h4>
          <a href="/forum?id=GP1Ncd8nTgn">
              METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19 Related Tweets
          </a>


            <a href="/pdf?id=GP1Ncd8nTgn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peilin_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peilin_Zhou1">Peilin Zhou</a>, <a href="/profile?id=~Zeqiang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeqiang_Wang1">Zeqiang Wang</a>, <a href="/profile?id=~Dading_Chong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dading_Chong1">Dading Chong</a>, <a href="/profile?id=~Zhijiang_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhijiang_Guo2">Zhijiang Guo</a>, <a href="/profile?id=~Yining_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yining_Hua1">Yining Hua</a>, <a href="/profile?email=suzc%40zju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="suzc@zju.edu.cn">Zichang Su</a>, <a href="/profile?id=~Zhiyang_Teng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyang_Teng1">Zhiyang Teng</a>, <a href="/profile?id=~Jiageng_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiageng_Wu1">Jiageng Wu</a>, <a href="/profile?id=~Jie_Yang13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Yang13">Jie Yang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#GP1Ncd8nTgn-details-301" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GP1Ncd8nTgn-details-301"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">medical, named entity recognition, sentiment, covid-19</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The COVID-19 pandemic continues to bring up various topics discussed or debated on social media. In order to explore the impact of pandemics on people's lives, it is crucial to understand the public's concerns and attitudes towards pandemic-related entities (e.g., drugs, vaccines) on social media. However, models trained on existing named entity recognition (NER) or targeted sentiment analysis (TSA) datasets have limited ability to understand COVID-19-related social media texts because these datasets are not designed or annotated from a medical perspective. In this paper, we release METS-CoV, a dataset containing medical entities and targeted sentiments from COVID-19 related tweets. METS-CoV contains 10,000 tweets with 7 types of entities, including 4 medical entity types (Disease, Drug, Symptom, and Vaccine) and 3 general entity types (Person, Location, and Organization). To further investigate tweet users' attitudes toward specific entities, 4 types of entities (Person, Organization, Drug, and Vaccine) are selected and annotated with user sentiments, resulting in a targeted sentiment dataset with 9,089 entities (in 5,278 tweets). To the best of our knowledge, METS-CoV is the first dataset to collect medical entities and corresponding sentiments of COVID-19 related tweets. We benchmark the performance of classical machine learning models and state-of-the-art deep learning models on NER and TSA tasks with extensive experiments. Results show that this dataset has vast room for improvement for both NER and TSA tasks. With rich annotations and comprehensive benchmark results, we believe METS-CoV is a fundamental resource for building better medical social media understanding tools and facilitating computational social science research, especially on epidemiological topics. Our data, annotation guidelines, benchmark models, and source code are publicly available (\url{https://github.com/YLab-Open/METS-CoV}) to ensure reproducibility. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=GP1Ncd8nTgn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/YLab-Open/METS-CoV" target="_blank" rel="nofollow noreferrer">https://github.com/YLab-Open/METS-CoV</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/YLab-Open/METS-CoV" target="_blank" rel="nofollow noreferrer">https://github.com/YLab-Open/METS-CoV</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0. (https://github.com/YLab-Open/METS-CoV/blob/main/LICENSE)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="gvaqa_WcIR6" data-number="322">
        <h4>
          <a href="/forum?id=gvaqa_WcIR6">
              VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web
          </a>


            <a href="/pdf?id=gvaqa_WcIR6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andrei_Manolache1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrei_Manolache1">Andrei Manolache</a>, <a href="/profile?id=~Florin_Brad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florin_Brad1">Florin Brad</a>, <a href="/profile?id=~Antonio_Barbalau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonio_Barbalau1">Antonio Barbalau</a>, <a href="/profile?id=~Radu_Tudor_Ionescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Radu_Tudor_Ionescu1">Radu Tudor Ionescu</a>, <a href="/profile?id=~Marius_Popescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Popescu1">Marius Popescu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#gvaqa_WcIR6-details-936" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gvaqa_WcIR6-details-936"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">authorship verification, dark web, authorship identification, authorship analysis, forensics, cybersecurity, natural language processing</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release datasets for authorship verification and identification collected from two DarkWeb forums and a DarkWeb-related subreddit</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Dark Web represents a hotbed for illicit activity, where users communicate on different market forums in order to exchange goods and services. Law enforcement agencies benefit from forensic tools that perform authorship analysis, in order to identify and profile users based on their textual content. However, authorship analysis has been traditionally studied using corpora featuring literary texts such as fragments from novels or fan fiction, which may not be suitable in a cybercrime context. Moreover, the few works that employ authorship analysis tools for cybercrime prevention usually employ ad-hoc experimental setups and datasets. To address these issues, we release VeriDark: a benchmark comprised of three large scale authorship verification datasets and one authorship identification dataset obtained from user activity from either Dark Web related Reddit communities or popular illicit Dark Web market forums. We evaluate competitive NLP baselines on the three datasets and perform an analysis of the predictions to better understand the limitations of such approaches. We make the datasets and baselines publicly available at https://github.com/bit-ml/VeriDark .</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=gvaqa_WcIR6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/bit-ml/VeriDark/tree/master/datasets" target="_blank" rel="nofollow noreferrer">https://github.com/bit-ml/VeriDark/tree/master/datasets</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The datasets are listed at this github page https://github.com/bit-ml/VeriDark/tree/master/datasets. The Google Drive links are provided for each of the four datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons 0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="x_kBZYiUrxR" data-number="320">
        <h4>
          <a href="/forum?id=x_kBZYiUrxR">
              PulseImpute: A Novel Benchmark Task for Physiological Signal Imputation
          </a>


            <a href="/pdf?id=x_kBZYiUrxR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Maxwell_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxwell_Xu1">Maxwell Xu</a>, <a href="/profile?id=~Alexander_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Moreno1">Alexander Moreno</a>, <a href="/profile?id=~Supriya_Nagesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Supriya_Nagesh1">Supriya Nagesh</a>, <a href="/profile?email=v.burakaydemir%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="v.burakaydemir@gatech.edu">Varol Burak Aydemir</a>, <a href="/profile?email=david.wetter%40hci.utah.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="david.wetter@hci.utah.edu">David Wetter</a>, <a href="/profile?id=~Santosh_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santosh_Kumar1">Santosh Kumar</a>, <a href="/profile?id=~James_Matthew_Rehg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Matthew_Rehg1">James Matthew Rehg</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#x_kBZYiUrxR-details-204" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x_kBZYiUrxR-details-204"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">missingness, imputation, mHealth, sensors, time-series, self-attention, pulsative, physiological, dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value "> We introduce PulseImpute, the first mHealth signal imputation challenge which includes realistic missingness models, an extensive set of baselines, and clinically-relevant downstream tasks that describe signal structures. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The promise of Mobile health (mHealth) is the ability to use wearable sensors to monitor a person's physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications. Moreover, the lack of large-scale datasets with labeled examples of missingness have prevented the ML community from tackling this important problem. We address this gap with PulseImpute, the first mHealth signal imputation challenge which includes realistic missingness models, an extensive set of baselines, and clinically-relevant downstream tasks that describe signal structures. The well-defined signal structure brings an additional emphasis on accurate shape reconstruction, and we demonstrate that existing state-of-the-art methods fail, adversely affecting their downstream clinical applications. We hypothesize that these models are unable to effectively exploit the quasi-periodic signal property for imputation, and thus, as a proof-of-concept, we introduce an augmented self-attention mechanism that attends on quasi-periodic features and achieves strong performance. We hope that PulseImpute will enable the ML community to tackle this significant and challenging task.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=x_kBZYiUrxR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/rehg-lab/pulseimpute" target="_blank" rel="nofollow noreferrer">https://github.com/rehg-lab/pulseimpute</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Code Repository with clear info on how to access our curated dataset: https://github.com/rehg-lab/pulseimpute
        Curated dataset direct link: https://www.dropbox.com/sh/6bygnzzx5t970yx/AAAHsVu9WeVXdQ_c1uBy_WkAa?dl=0
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License for Code Repository
        Open Data Commons Open Database License v1.0 for Data</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="2rQPxsmjKF" data-number="319">
        <h4>
          <a href="/forum?id=2rQPxsmjKF">
              DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection
          </a>


            <a href="/pdf?id=2rQPxsmjKF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xuanwen_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanwen_Huang1">Xuanwen Huang</a>, <a href="/profile?id=~Yang_Yang35" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Yang35">Yang Yang</a>, <a href="/profile?email=wangyang09%40xinye.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangyang09@xinye.com">Yang Wang</a>, <a href="/profile?id=~Chunping_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chunping_Wang1">Chunping Wang</a>, <a href="/profile?email=zhangzhsh6%40zju.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhangzhsh6@zju.edu.cn">Zhisheng Zhang</a>, <a href="/profile?id=~Jiarong_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiarong_Xu2">Jiarong Xu</a>, <a href="/profile?email=chenlei04%40xinye.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chenlei04@xinye.com">Lei Chen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#2rQPxsmjKF-details-322" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2rQPxsmjKF-details-322"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Anomaly Detection, Dynamic Graph, Finance fraudsters detection.</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper present DGraph, a real-world dynamic graph in the finance domain.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is a fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that those unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=2rQPxsmjKF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://dgraph.xinye.com" target="_blank" rel="nofollow noreferrer">https://dgraph.xinye.com</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Datasets: https://dgraph.xinye.com/
        Registering an account on https://dgraph.xinye.com are required for downloading datasets</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset in this paper is licensed under a Custom (non-commercial) license.  See official instructions https://dgraph.xinye.com/clause.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="x72kLyW1OG" data-number="318">
        <h4>
          <a href="/forum?id=x72kLyW1OG">
              WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series
          </a>


            <a href="/pdf?id=x72kLyW1OG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jean-Christophe_Gagnon-Audet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Christophe_Gagnon-Audet1">Jean-Christophe Gagnon-Audet</a>, <a href="/profile?id=~Kartik_Ahuja1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kartik_Ahuja1">Kartik Ahuja</a>, <a href="/profile?id=~Mohammad_Javad_Darvishi_Bayazi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Javad_Darvishi_Bayazi1">Mohammad Javad Darvishi Bayazi</a>, <a href="/profile?id=~Pooneh_Mousavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pooneh_Mousavi1">Pooneh Mousavi</a>, <a href="/profile?email=deep.introspection%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="deep.introspection@gmail.com">Guillaume Dumas</a>, <a href="/profile?id=~Irina_Rish1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irina_Rish1">Irina Rish</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#x72kLyW1OG-details-806" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="x72kLyW1OG-details-806"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Out-of-Distribution generalization, Time series, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present WOODS: eight challenging time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and sensor signals.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">    Deep learning models often fail to generalize well under distribution shifts. Understanding and overcoming these failures have led to a new research field on Out-of-Distribution (OOD) generalization. Despite being extensively studied for static computer vision tasks, OOD generalization has been severely underexplored for time series tasks. To shine a light on this gap, we present WOODS: ten challenging time series benchmarks covering a diverse range of data modalities, such as videos, brain recordings, and smart device sensory signals. We revise the existing OOD generalization algorithms for time series tasks and evaluate them using our systematic framework. Our experiments show a large room for improvement for empirical risk minimization and OOD generalization algorithms on our datasets, thus underscoring the new challenges posed by time series tasks. Code and documentation are available at https://woods-benchmarks.github.io/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=x72kLyW1OG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://woods-benchmarks.github.io/" target="_blank" rel="nofollow noreferrer">https://woods-benchmarks.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">All WOODS dataset are openly available via the project website and github repository, except for the IEMOCAP dataset which requires a license agreement via their website: https://sail.usc.edu/iemocap/.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">All datasets can be accessed through our website (https://woods-benchmarks.github.io/).</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The WOODS repository code is licensed under the MIT license.
        All dataset licenses are stated in the supplementary material and on the project website.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wtY5R198DP8" data-number="317">
        <h4>
          <a href="/forum?id=wtY5R198DP8">
              smallSSD: A semi-supervised detection dataset for agriculture
          </a>


            <a href="/pdf?id=wtY5R198DP8" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Krisztina_Sinkovics1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krisztina_Sinkovics1">Krisztina Sinkovics</a>, <a href="/profile?id=~Gabriel_Tseng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Tseng1">Gabriel Tseng</a>, <a href="/profile?email=tom.watsham%40smallrobotcompany.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tom.watsham@smallrobotcompany.com">Tom Watsham</a>, <a href="/profile?id=~Thomas_C._Walters1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_C._Walters1">Thomas C. Walters</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#wtY5R198DP8-details-757" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wtY5R198DP8-details-757"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, agriculture, object detection, computer vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">smallSSD: a dataset of labelled and unlabelled wheat and weed images for semi supervised object detection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning is critical to the fine-grained monitoring and management of crop populations. Such monitoring has ramifications for farm management, and food security more generally. For example, early stage crop counting unlocks estimates of future yield and gives room for yield-improving actions to be taken (such as targeted addition of nutrients). Robotic technology allows for the collection of massive amounts of data for crop detection, but annotating these data is costly and requires specialised expertise for high quality labels. With semi-supervised learning, we can leverage this unlabelled data to improve crop counting algorithms. To this end, we introduce a semi-supervised object detection dataset for crop detection. The dataset consists of two parts: 960 labelled top-down images of a winter wheat crop with per-plant annotations of both the crop plants and broad-leafed weeds and more than 100,032 unlabelled images of the same crop. We release this dataset alongside an evaluation set, so that researchers can evaluate semi-supervised object detection methods against a challenging real world problem. We also release baseline models to benchmark performance on the dataset. The dataset is designed to be extensible, allowing it to be augmented with a wider variety of field conditions and crop growth stages over time.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wtY5R198DP8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/smallrobotcompany/smallssd" target="_blank" rel="nofollow noreferrer">https://github.com/smallrobotcompany/smallssd</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6627697" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6627697</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="M3Y74vmsMcY" data-number="316">
        <h4>
          <a href="/forum?id=M3Y74vmsMcY">
              LAION-5B: An open large-scale dataset for training next generation image-text models
          </a>


            <a href="/pdf?id=M3Y74vmsMcY" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Christoph_Schuhmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christoph_Schuhmann1">Christoph Schuhmann</a>, <a href="/profile?id=~Romain_Beaumont1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Romain_Beaumont1">Romain Beaumont</a>, <a href="/profile?id=~Cade_W_Gordon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cade_W_Gordon1">Cade W Gordon</a>, <a href="/profile?id=~Ross_Wightman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ross_Wightman1">Ross Wightman</a>, <a href="/profile?id=~mehdi_cherti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~mehdi_cherti1">mehdi cherti</a>, <a href="/profile?id=~Theo_Coombes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Theo_Coombes1">Theo Coombes</a>, <a href="/profile?email=arksealplays%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="arksealplays@gmail.com">Aarush Katta</a>, <a href="/profile?email=claymullis%40fastmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="claymullis@fastmail.com">Clayton Mullis</a>, <a href="/profile?id=~Patrick_Schramowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Schramowski1">Patrick Schramowski</a>, <a href="/profile?id=~Srivatsa_R_Kundurthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Srivatsa_R_Kundurthy1">Srivatsa R Kundurthy</a>, <a href="/profile?id=~Katherine_Crowson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katherine_Crowson1">Katherine Crowson</a>, <a href="/profile?id=~Richard_Vencu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Vencu1">Richard Vencu</a>, <a href="/profile?id=~Ludwig_Schmidt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ludwig_Schmidt1">Ludwig Schmidt</a>, <a href="/profile?id=~Robert_Kaczmarczyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Kaczmarczyk1">Robert Kaczmarczyk</a>, <a href="/profile?id=~Jenia_Jitsev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jenia_Jitsev1">Jenia Jitsev</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#M3Y74vmsMcY-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="M3Y74vmsMcY-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-modal learning, large-scale datasets, reproducibility, open source, CLIP</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present LAION-5B, an open, publically available dataset of 5.8B image-text pairs and validate it by reproducing results of training state-of-the-art CLIP models of different scale.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong out-of-distribution sample generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, further large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP and GLIDE using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=M3Y74vmsMcY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/" target="_blank" rel="nofollow noreferrer">https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/" target="_blank" rel="nofollow noreferrer">https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Common CC-BY 4.0 license
        https://creativecommons.org/licenses/by/4.0/</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="eSTDtFMPjv7" data-number="314">
        <h4>
          <a href="/forum?id=eSTDtFMPjv7">
              CIC: Character Image Combination for Scene Text Recognition with Fewer Labels
          </a>


            <a href="/pdf?id=eSTDtFMPjv7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jeonghun_Baek2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeonghun_Baek2">Jeonghun Baek</a>, <a href="/profile?id=~Yusuke_Matsui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusuke_Matsui1">Yusuke Matsui</a>, <a href="/profile?id=~Kiyoharu_Aizawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiyoharu_Aizawa1">Kiyoharu Aizawa</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#eSTDtFMPjv7-details-207" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eSTDtFMPjv7-details-207"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Scene Text Recognition, Character Image Combination, Pseudo Word Image Generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose character image combination (CIC) as a pseudo word image for scene text recognition with fewer labels.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Scene text recognition (STR) with fewer labels aims to exploit fewer real data efficiently.
        In this work, to exploit real character images, we propose character image combination (CIC), a pseudo word image made by combining character images.
        Generating CIC is simple and, in our experiment, 70.9 times faster than the existing synthetic word generator.
        CIC looks unrealistic word image but is effective to train STR models.
        Through extensive experiments, we validate the effectiveness of CIC.
        We find that training the model on only CIC causes partial predictions: the model predicts only one or two characters instead of the whole word.
        We mitigate partial predictions with a simple solution, including 5K word images in the training set, and improve accuracy by 32.2%.
        Furthermore, we find that CIC generated by only 62 character images is worth about 25K word images.
        To the best of our knowledge, this is the first study that shows and analyzes the characteristic of CIC.
        Our code and data will be publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=eSTDtFMPjv7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We uploaded 600 examples of our data (CIC) to the Dropbox https://bit.ly/CIC-examples
        The description of examples is in the supplementary material.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We will release our code under the MIT license and our data (CIC) under the CC BY 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-6s-5cWw_Sn" data-number="313">
        <h4>
          <a href="/forum?id=-6s-5cWw_Sn">
              BOSS: A Benchmark for Human Belief Prediction in Object-context Scenarios
          </a>


            <a href="/pdf?id=-6s-5cWw_Sn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiafei_Duan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiafei_Duan1">Jiafei Duan</a>, <a href="/profile?id=~Samson_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samson_Yu1">Samson Yu</a>, <a href="/profile?id=~Nicholas_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Tan1">Nicholas Tan</a>, <a href="/profile?id=~Li_Yi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Yi2">Li Yi</a>, <a href="/profile?id=~Cheston_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheston_Tan1">Cheston Tan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#-6s-5cWw_Sn-details-418" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-6s-5cWw_Sn-details-418"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">machine theory of mind, scene understanding and analysis, video activity recognition</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A New 3D Video Dataset for Detecting Belief States of Humans in an Object-context Scenario with Nonverbal Communication. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Humans with an average level of social cognition can infer the beliefs of others based solely on the nonverbal communication signals (e.g. gaze, gesture, pose and contextual information) exhibited during social interactions. This social cognitive ability to predict human beliefs and intentions is more important than ever for ensuring safe human-robot interaction and collaboration. This paper uses the combined knowledge of Theory of Mind (ToM) and Object-Context Relations to investigate methods for enhancing collaboration between humans and autonomous systems in environments where verbal communication is prohibited. We propose a novel and challenging multimodal video dataset for assessing the capability of artificial intelligence (AI) systems in predicting human belief states in an object-context scenario. The proposed dataset consists of precise labelling of human belief state ground-truth and multimodal inputs replicating all nonverbal communication inputs captured by human perception. We further evaluate our dataset with existing deep learning models and provide new insights into the effects of the various input modalities and object-context relations on the performance of the baseline models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-6s-5cWw_Sn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/bossbelief/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/bossbelief/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1b8FdpyoWx9gUps-BX6qbE9Kea3C2Uyua" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1b8FdpyoWx9gUps-BX6qbE9Kea3C2Uyua</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="0OVN8fE2NPq" data-number="312">
        <h4>
          <a href="/forum?id=0OVN8fE2NPq">
              UltraMNIST Classification: A Benchmark to Train CNNs for Very Large Images
          </a>


            <a href="/pdf?id=0OVN8fE2NPq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Deepak_Gupta2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepak_Gupta2">Deepak Gupta</a>, <a href="/profile?id=~Udbhav_Bamba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Udbhav_Bamba1">Udbhav Bamba</a>, <a href="/profile?id=~Abhishek_Thakur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Thakur1">Abhishek Thakur</a>, <a href="/profile?id=~Akash_Gupta3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akash_Gupta3">Akash Gupta</a>, <a href="/profile?id=~Suraj_Sharan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suraj_Sharan1">Suraj Sharan</a>, <a href="/profile?id=~Ertugrul_Demir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ertugrul_Demir1">Ertugrul Demir</a>, <a href="/profile?id=~Dilip_K._Prasad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dilip_K._Prasad1">Dilip K. Prasad</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#0OVN8fE2NPq-details-49" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0OVN8fE2NPq-details-49"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">large-scale images, efficient training, generalization, memory efficiency</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Benchmark for training CNN for very large images</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Convolutional neural network (CNN) approaches available in the current literature are designed to work primarily with low-resolution images. When applied on very large images, challenges related to GPU memory, smaller receptive field than needed for semantic correspondence and the need to incorporate multi-scale features arise. The resolution of input images can be reduced, however, with significant loss of critical information. Based on the outlined issues, we introduce a novel research problem of training CNN models for very large images, and present ‘UltraMNIST dataset’, a simple yet representative benchmark dataset for this task. UltraMNIST has been designed using the popular MNIST digits with additional levels of complexity added to replicate well the challenges of real-world problems. We present two variants of the problem: ‘UltraMNIST classification’ and ‘Budget-aware UltraMNIST classification’. The standard UltraMNIST classification benchmark is intended to facilitate the development of novel CNN training methods that make the effective use of the best available GPU resources. The budget-aware variant is intended to promote development of methods that work under constrained GPU memory. For the development of competitive solutions, we present several baseline models for the standard benchmark and its budget-aware variant. We study the effect of reducing resolution on the performance and present results for baseline models involving pretrained backbones from among the popular state-of-the-art models. Finally, with the presented benchmark dataset and the baselines, we hope to pave the ground for a new generation of CNN methods suitable for handling large images in an efficient and resource-light manner. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=0OVN8fE2NPq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://www.kaggle.com/competitions/ultra-mnist, https://github.com/transmuteAI/ultramnist</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.kaggle.com/competitions/ultra-mnist/data" target="_blank" rel="nofollow noreferrer">https://www.kaggle.com/competitions/ultra-mnist/data</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="r2DdJQ9AJvI" data-number="311">
        <h4>
          <a href="/forum?id=r2DdJQ9AJvI">
              TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models
          </a>


            <a href="/pdf?id=r2DdJQ9AJvI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Huibin_Ge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huibin_Ge1">Huibin Ge</a>, <a href="/profile?id=~Xiaohu_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohu_Zhao1">Xiaohu Zhao</a>, <a href="/profile?id=~Chuang_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Liu3">Chuang Liu</a>, <a href="/profile?id=~Yulong_Zeng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulong_Zeng2">Yulong Zeng</a>, <a href="/profile?id=~Qun_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qun_Liu1">Qun Liu</a>, <a href="/profile?id=~Deyi_Xiong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deyi_Xiong2">Deyi Xiong</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 13 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#r2DdJQ9AJvI-details-935" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r2DdJQ9AJvI-details-935"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Text Generation, Pretrained Language Model, Data Curation, Text Generation Error</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A large dataset and benchmark tasks used to diagnostically analyze and improve the capability of pretrained language models in text generation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In order to diagnostically analyze and improve the capability of pretrained language models (PLMs) in text generation, we propose TGEA 2.0, to date the largest dataset built on machine-authored texts by PLMs with fine-grained semantic annotations on a wide variety of pathological generation errors. We collect 170K nominal, phrasal and sentential prompts from 6M natural sentences in 3 domains. These prompts are fed into 4 generative PLMs with their best decoding strategy to generate paragraphs. 195,629 sentences are extracted from these generated paragraphs for manual annotation, where 36K erroneous sentences are detected, 42K erroneous spans are located and categorized into an error type defined in a two-level error taxonomy. We define a \textbf{Mi}nimal \textbf{S}et of \textbf{E}rror-related \textbf{W}ords (MiSEW) for each erroneous span, which not only provides error-associated words but also rationalizes the reasoning behind the error. Quality control with a pre-annotation and feedback loop is performed before and during the entire annotation process. With the diagnostically annotated dataset, we propose 5 diagnosis benchmark tasks (i.e., erroneous text detection, MiSEW extraction, erroneous span location and correction together with error type classification) and 2 pathology mitigation benchmark tasks (pairwise comparison and word prediction). Experiment results on these benchmark tasks demonstrate that TGEA 2.0 is a challenging dataset that could facilitate further research on automatic diagnosis and pathology mitigation over machine texts. The dataset will be publicly available at https://download.mindspore.cn/dataset/TGEA/ and https://github.com/tjunlp-lab/TGEA/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=r2DdJQ9AJvI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://download.mindspore.cn/dataset/TGEA/ and https://github.com/tjunlp-lab/TGEA/ </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset will be released at https://download.mindspore.cn/dataset/TGEA/ and https://github.com/tjunlp-lab/TGEA/.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The full training dataset and a small dev/test dataset will be released at the project websites by July, 2022. The full dev/test datasets will be available when we organize the shared task with the dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="0OvRYQF0FwG" data-number="310">
        <h4>
          <a href="/forum?id=0OvRYQF0FwG">
              D-LORD: DYSL-AI Database for Low-Resolution Disguised Face Recognition
          </a>


            <a href="/pdf?id=0OvRYQF0FwG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sunny_Manchanda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sunny_Manchanda1">Sunny Manchanda</a>, <a href="/profile?id=~Kavita_Balutia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kavita_Balutia1">Kavita Balutia</a>, <a href="/profile?id=~Kaushik_Bhagwatkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaushik_Bhagwatkar1">Kaushik Bhagwatkar</a>, <a href="/profile?id=~Shivang_Agarwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shivang_Agarwal1">Shivang Agarwal</a>, <a href="/profile?id=~Jyoti_Chaudhary1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jyoti_Chaudhary1">Jyoti Chaudhary</a>, <a href="/profile?id=~Muskan_Dosi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muskan_Dosi1">Muskan Dosi</a>, <a href="/profile?id=~Chiranjeev_Arya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chiranjeev_Arya1">Chiranjeev Arya</a>, <a href="/profile?id=~Richa_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richa_Singh1">Richa Singh</a>, <a href="/profile?id=~Mayank_Vatsa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mayank_Vatsa1">Mayank Vatsa</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#0OvRYQF0FwG-details-119" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0OvRYQF0FwG-details-119"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Face recognition in a low-resolution video stream captured from a surveillance camera is a challenging problem. The problem becomes even more complicated when the subjects appearing in the video wear disguise artifacts to hide their identity or try to impersonate someone. The lack of labeled datasets restricts the current research on low-resolution face recognition systems under disguise. With this paper, we propose a large-scale database, D-LORD, that will facilitate the research on face recognition. The proposed D-LORD consists of high-resolution mugshot images of $2,100$ subjects and $14,098$ low-resolution surveillance videos with over $1.2$M frames. In the videos, the faces of subjects are occluded by various disguise artifacts, such as face masks, sunglasses, wigs, hats, and monkey caps. To the best of our knowledge, D-LORD is the first database to address the complex problem of low-resolution face recognition with disguise variations. Secondly, we establish the benchmark results of several state-of-the-art face detectors, frame selection algorithms, face restoration, and face verification algorithms on pre-defined experimental protocols. The results demonstrate GAR@$1$\%FAR values in the range of $86.44\%$ to $49.45\%$ for varying disguises at varying distances.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=0OvRYQF0FwG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Dataset : http://dyslai.org/datasets/D-LORD, Benchmark : https://github.com/aryachiranjeev/D-LORD</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://dyslai.org/datasets/D-LORD" target="_blank" rel="nofollow noreferrer">http://dyslai.org/datasets/D-LORD</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ZUK81igNmui" data-number="308">
        <h4>
          <a href="/forum?id=ZUK81igNmui">
              Multimodal Lecture Presentations Dataset: Understanding Multimodality in Educational Slides
          </a>


            <a href="/pdf?id=ZUK81igNmui" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Dong_Won_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong_Won_Lee1">Dong Won Lee</a>, <a href="/profile?id=~Chaitanya_Ahuja1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaitanya_Ahuja1">Chaitanya Ahuja</a>, <a href="/profile?id=~Paul_Pu_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Pu_Liang1">Paul Pu Liang</a>, <a href="/profile?email=snatu%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="snatu@andrew.cmu.edu">Sanika Natu</a>, <a href="/profile?id=~Louis-Philippe_Morency1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis-Philippe_Morency1">Louis-Philippe Morency</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ZUK81igNmui-details-731" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZUK81igNmui-details-731"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multimodal learning, crossmodal retrieval, education, vision and language</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Multimodal Lecture Presentations is a benchmark testing the capabilities of machine learning models in multimodal understanding of educational presentation content.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Lecture slide presentations, a sequence of pages that contain text and figures accompanied by speech, are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing AI to aid in student learning as intelligent teacher assistants, we introduce the Multimodal Lecture Presentations dataset as a large-scale benchmark testing the capabilities of machine learning models in multimodal understanding of educational content. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce two research tasks which are designed as stepping stones towards AI agents that can explain (automatically captioning a lecture presentation) and illustrate (synthesizing visual figures to accompany spoken explanations) educational content. We provide manual annotations to help implement these two research tasks and evaluate state-of-the-art models on them. Comparing baselines and human student performances, we find that current models struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. Towards addressing this issue, we also introduce PolyViLT, a multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentations.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ZUK81igNmui&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/dondongwon/MLPDataset" target="_blank" rel="nofollow noreferrer">https://github.com/dondongwon/MLPDataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/dondongwon/MLPDataset" target="_blank" rel="nofollow noreferrer">https://github.com/dondongwon/MLPDataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="05z9id-kJkG" data-number="306">
        <h4>
          <a href="/forum?id=05z9id-kJkG">
              Measuring Novel API Use in Program Synthesis
          </a>


            <a href="/pdf?id=05z9id-kJkG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ethan_A_Chi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_A_Chi1">Ethan A Chi</a>, <a href="/profile?id=~Yuhuai_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhuai_Wu1">Yuhuai Wu</a>, <a href="/profile?id=~Aitor_Lewkowycz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aitor_Lewkowycz2">Aitor Lewkowycz</a>, <a href="/profile?id=~Vedant_Misra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vedant_Misra1">Vedant Misra</a>, <a href="/profile?id=~Anders_Johan_Andreassen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anders_Johan_Andreassen1">Anders Johan Andreassen</a>, <a href="/profile?email=aslone%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="aslone@google.com">Ambrose Slone</a>, <a href="/profile?id=~Vinay_Venkatesh_Ramasesh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vinay_Venkatesh_Ramasesh2">Vinay Venkatesh Ramasesh</a>, <a href="/profile?id=~Guy_Gur-Ari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guy_Gur-Ari1">Guy Gur-Ari</a>, <a href="/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>, <a href="/profile?id=~Ethan_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Dyer1">Ethan Dyer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#05z9id-kJkG-details-558" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="05z9id-kJkG-details-558"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Large Language Models, Program Synthesis, API Use, Reasoning, Compositional Generalization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose synthetically generating API use programming problems, which require reading and working with novel APIs and measure LLMs performance on this benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large language models (LLMs) have demonstrated impressive program synthe-sis abilities, with high performance on benchmarks of real-world programming problems suggesting a strong fundamental ability to reason. However, a systematic analysis of the extent of these abilities can be challenging, due to performance being confounded by memorization. In this work, we propose synthetically generating API use programming problems, which require reading and working with novel APIs. Abstracting away complex, memorized operations through these APIs—which are defined solely within the scope of the problem—allows us to examine the ability of LLMs to perform tool use in a controlled setting while avoiding spuriously high performance from memorization. Our problem generation process is highly configurable, allowing us to examine the relative influence of components of problem structure (e.g. compositionality) on LLM reasoning performance in a controlled way. We create benchmarks examining some of the most relevant such components, against which we evaluate standard program synthesis models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=05z9id-kJkG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ethanachi/api_use/" target="_blank" rel="nofollow noreferrer">https://github.com/ethanachi/api_use/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="3rDujJTM0nq" data-number="304">
        <h4>
          <a href="/forum?id=3rDujJTM0nq">
              Vote'n'Rank: Revision of Benchmarking with Social Choice Theory
          </a>


            <a href="/pdf?id=3rDujJTM0nq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mark_Rofin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Rofin1">Mark Rofin</a>, <a href="/profile?id=~Mikhail_Florinsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Florinsky1">Mikhail Florinsky</a>, <a href="/profile?id=~Vladislav_Mikhailov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladislav_Mikhailov1">Vladislav Mikhailov</a>, <a href="/profile?id=~Andrey_Kravchenko3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Kravchenko3">Andrey Kravchenko</a>, <a href="/profile?id=~Elena_Tutubalina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elena_Tutubalina1">Elena Tutubalina</a>, <a href="/profile?id=~Tatiana_Shavrina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatiana_Shavrina1">Tatiana Shavrina</a>, <a href="/profile?id=~Daniel_Karabekyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Karabekyan1">Daniel Karabekyan</a>, <a href="/profile?id=~Katya_Artemova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katya_Artemova1">Katya Artemova</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#3rDujJTM0nq-details-914" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3rDujJTM0nq-details-914"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmarking, language models, social choice theory, ranking</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper proposes Vote'n'Rank, a framework for ranking systems in multi-task benchmarks under the principles of the social choice theory.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The development of state-of-the-art systems in different applied areas of artificial intelligence (AI) is driven by benchmarks, which have played a crucial role in shaping the paradigm of evaluating generalisation capabilities from multiple perspectives. Although the paradigm is shifting towards more fine-grained evaluation across diverse complex tasks, the delicate question of how to aggregate the performances has received particular interest in the community. The benchmarks generally follow the unspoken utilitarian principles, where the systems are ranked based on their mean average score over task-specific metrics. Such aggregation procedure has been viewed as a sub-optimal evaluation protocol, which may have created the illusion of progress in the field. This paper proposes Vote'n'Rank, a framework for ranking systems in multi-task benchmarks under the principles of the social choice theory. We demonstrate that our approach can be efficiently utilised to draw new insights on benchmarking in several AI sub-fields and identify the best-performing systems in simulated practical scenarios that meet user needs. The Vote'n'Rank's procedures are empirically shown to be more robust than the mean average while being able to handle missing performance scores and determine conditions under which the system becomes the winner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=3rDujJTM0nq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/PragmaticsLab/vote_and_rank" target="_blank" rel="nofollow noreferrer">https://github.com/PragmaticsLab/vote_and_rank</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9DnhAkyYo0" data-number="303">
        <h4>
          <a href="/forum?id=9DnhAkyYo0">
              DongTing: A Large-scale Dataset for Anomaly Detection of the Linux Kernel
          </a>


            <a href="/pdf?id=9DnhAkyYo0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~GuoYun_Duan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~GuoYun_Duan1">GuoYun Duan</a>, <a href="/profile?id=~YuanZhi_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~YuanZhi_Fu1">YuanZhi Fu</a>, <a href="/profile?id=~Minjie_Cai3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjie_Cai3">Minjie Cai</a>, <a href="/profile?id=~Hao_Chen28" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Chen28">Hao Chen</a>, <a href="/profile?id=~Jianhua_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhua_Sun2">Jianhua Sun</a>, <a href="/profile?id=~Yongheng_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongheng_Wang1">Yongheng Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#9DnhAkyYo0-details-941" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9DnhAkyYo0-details-941"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Host-based intrusion detection systems (HIDS) can automatically identify adversarial applications by learning models from system events that represent normal system behaviors. The system call is the only way for applications to interact with the operating system (OS). Thus, system call sequences are traditionally used in HIDS to train models to detect novel attacks, and a wide range of datasets has been proposed for this task. However, existing datasets are either built for user-level applications (not for OS kernels), or completely outdated (proposed more than 20 years ago). To address this issue, this paper presents the first large-scale dataset specifically assembled for anomaly detection of the Linux kernel. The task of creating such a dataset is challenging due to the difficulty both in collecting a diversified set of programs that can trigger bugs in the kernel and in tracing events that may crash the kernel at runtime. In this paper, we describe in detail how we collect the data through an automated and efficient framework. The raw dataset is 85 GB in size, and contains 18,966 system call sequences that are labeled with normal and abnormal attributes. Our dataset covers more than 200 kernel versions (including major/minor releases and revisions) and 3,600 bug-triggering programs in the past five years. In addition, we conduct cross-dataset evaluation to demonstrate that training on our dataset enables superior generalization ability than other related datasets, and provide benchmark results for anomaly detection of Linux kernel on our dataset. Our extensive dataset is both useful for machine learning researchers focusing on algorithmic optimizations and practitioners in kernel development who are interested in deploying deep learning models in OS kernels.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9DnhAkyYo0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9K-8l0WgSK3" data-number="302">
        <h4>
          <a href="/forum?id=9K-8l0WgSK3">
              CEDe: A collection of expert-curated datasets with atom-level entity annotations for Optical Chemical Structure Recognition
          </a>


            <a href="/pdf?id=9K-8l0WgSK3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rodrigo_Hormazabal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rodrigo_Hormazabal1">Rodrigo Hormazabal</a>, <a href="/profile?id=~Changyoung_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changyoung_Park1">Changyoung Park</a>, <a href="/profile?id=~Soonyoung_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soonyoung_Lee1">Soonyoung Lee</a>, <a href="/profile?id=~Sehui_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sehui_Han1">Sehui Han</a>, <a href="/profile?id=~Yeonsik_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yeonsik_Jo1">Yeonsik Jo</a>, <a href="/profile?id=~Jaewan_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaewan_Lee1">Jaewan Lee</a>, <a href="/profile?id=~Ahra_Jo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahra_Jo1">Ahra Jo</a>, <a href="/profile?id=~Seung_Hwan_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seung_Hwan_Kim1">Seung Hwan Kim</a>, <a href="/profile?id=~Jaegul_Choo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaegul_Choo1">Jaegul Choo</a>, <a href="/profile?id=~Moontae_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moontae_Lee1">Moontae Lee</a>, <a href="/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#9K-8l0WgSK3-details-640" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9K-8l0WgSK3-details-640"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Chemical structure recognition, Chemical image-to-structure translation, Molecular images atom-level instance annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A collection of datasets containing more than 700,000 atom-level entity annotations and their corresponding bounding boxes. This labels constitute all the necessary information for complete chemical graph reconstruction.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Optical Chemical Structure Recognition (OCSR) deals with the translation from chemical images to molecular structures, which is the main way chemical compounds are depicted in scientific documents. Traditional rule-based methods follow a framework based on the detection of atoms and bonds, followed by the reconstruction of the compound structure. Recently, neural architectures analog to image captioning have been explored to solve this task, yet they still show to be data inefficient, using millions of examples just to show performance comparable with traditional methods. Looking to motivate and benchmark new approaches based on atomic-level entities detection and graph reconstruction, we present CEDe, a unique collection of chemical entity bounding boxes manually curated by experts for scientific literature datasets. These annotations combine to more than 700,000 chemical entity bounding boxes with the necessary information for structure reconstruction. Also, a large synthetic dataset containing 1 million molecular images and annotations is released in order to explore transfer-learning techniques that could help these architectures perform better under low-data regimes. Benchmarks show that detection-reconstruction based models can achieve performances on par with or better than image captioning-like models, even with 100x fewer training examples.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9K-8l0WgSK3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://s3.us-east-2.amazonaws.com/lg.cede/CEDe-annotations-prev_0.tar.gz" target="_blank" rel="nofollow noreferrer">https://s3.us-east-2.amazonaws.com/lg.cede/CEDe-annotations-prev_0.tar.gz</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://s3.us-east-2.amazonaws.com/lg.cede/CEDe-annotations-prev_0.tar.gz

        This is the main endpoint that will be used for the dataset while on review.
        Also, details on how to access the data will be in the supplementary material.

        Also, a website for data exploration will be made available throughout the review process.
        However, since the site is not deployed yet, we do not have an extra URL to provide.
        We will provide the necessary information as soon as the website is online.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)
        https://creativecommons.org/licenses/by-nc/2.0/legalcode</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Kyswf8Kj83" data-number="301">
        <h4>
          <a href="/forum?id=Kyswf8Kj83">
              TwiBot-22: Towards Graph-Based Twitter Bot Detection
          </a>


            <a href="/pdf?id=Kyswf8Kj83" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shangbin_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shangbin_Feng1">Shangbin Feng</a>, <a href="/profile?id=~Zhaoxuan_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaoxuan_Tan1">Zhaoxuan Tan</a>, <a href="/profile?id=~Herun_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Herun_Wan1">Herun Wan</a>, <a href="/profile?id=~Ningnan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ningnan_Wang1">Ningnan Wang</a>, <a href="/profile?id=~Zilong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zilong_Chen1">Zilong Chen</a>, <a href="/profile?id=~Binchi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binchi_Zhang1">Binchi Zhang</a>, <a href="/profile?id=~Qinghua_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qinghua_Zheng1">Qinghua Zheng</a>, <a href="/profile?id=~Wenqian_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenqian_Zhang1">Wenqian Zhang</a>, <a href="/profile?id=~Zhenyu_Lei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyu_Lei1">Zhenyu Lei</a>, <a href="/profile?id=~Shujie_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shujie_Yang1">Shujie Yang</a>, <a href="/profile?id=~Xinshun_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinshun_Feng1">Xinshun Feng</a>, <a href="/profile?id=~Qingyue_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qingyue_Zhang1">Qingyue Zhang</a>, <a href="/profile?id=~Hongrui_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongrui_Wang1">Hongrui Wang</a>, <a href="/profile?id=~Yuhan_Liu9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhan_Liu9">Yuhan Liu</a>, <a href="/profile?id=~Yuyang_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuyang_Bai1">Yuyang Bai</a>, <a href="/profile?id=~Heng_Wang10" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heng_Wang10">Heng Wang</a>, <a href="/profile?id=~Zijian_Cai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zijian_Cai2">Zijian Cai</a>, <a href="/profile?id=~Yanbo_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanbo_Wang2">Yanbo Wang</a>, <a href="/profile?id=~Lijing_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lijing_Zheng1">Lijing Zheng</a>, <a href="/profile?id=~Zihan_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zihan_Ma1">Zihan Ma</a>, <a href="/profile?id=~Jundong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jundong_Li2">Jundong Li</a>, <a href="/profile?id=~Minnan_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minnan_Luo1">Minnan Luo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 10 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Kyswf8Kj83-details-777" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Kyswf8Kj83-details-777"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Twitter bot detection, social network analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at https://twibot22.github.io/</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Kyswf8Kj83&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://twibot22.github.io/" target="_blank" rel="nofollow noreferrer">https://twibot22.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://twibot22.github.io/" target="_blank" rel="nofollow noreferrer">https://twibot22.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The TwiBot-22 dataset uses the CC BY-NC-ND 4.0 license. Implemented code in the TwiBot-22 evaluation framework uses the MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="vrnqr3PG4yB" data-number="300">
        <h4>
          <a href="/forum?id=vrnqr3PG4yB">
              TempEL: Linking Dynamically Evolving and Newly Emerging Entities
          </a>


            <a href="/pdf?id=vrnqr3PG4yB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Klim_Zaporojets1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Klim_Zaporojets1">Klim Zaporojets</a>, <a href="/profile?id=~Lucie-Aim%C3%A9e_Kaffee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucie-Aimée_Kaffee1">Lucie-Aimée Kaffee</a>, <a href="/profile?id=~Johannes_Deleu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Deleu1">Johannes Deleu</a>, <a href="/profile?id=~Thomas_Demeester1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Demeester1">Thomas Demeester</a>, <a href="/profile?id=~Chris_Develder1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Develder1">Chris Develder</a>, <a href="/profile?id=~Isabelle_Augenstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Isabelle_Augenstein1">Isabelle Augenstein</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#vrnqr3PG4yB-details-111" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vrnqr3PG4yB-details-111"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Entity Linking, Entity Disambiguation, Information Extraction, Temporal Data Evolution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In our continuously evolving world, entities change over time and new, previously non-existing or unknown, entities appear. We study how this evolutionary scenario impacts the performance on a well established entity linking (EL) task. For that study, we introduce TempEL, an entity linking dataset that consists of time-stratified English Wikipedia snapshots from 2013 to 2022, from which we collect both anchor mentions of entities, and these target entities’ descriptions. By capturing such temporal aspects, our newly introduced TempEL resource contrasts with currently existing entity linking datasets, which are composed of fixed mentions linked to a single static version of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA). Indeed, for each of our collected temporal snapshots, TempEL contains links to entities that are continual, i.e., occur in all of the years, as well as completely new entities that appear for the first time at some point. Thus, we enable to quantify the performance of current state-of-the-art EL models for: (i) entities that are subject to changes over time in their Knowledge Base descriptions as well as their mentions’ contexts, and (ii) newly created entities that were previously non-existing (e.g., at the time the EL model was trained). Our experimental results show that in terms of temporal performance degradation, (i) continual entities suffer a decrease of up to 4.6% EL accuracy, while (ii) for new entities this accuracy drop is up to 15.4%. This highlights the challenge of the introduced TempEL dataset and opens new research prospects in the area of time-evolving entity disambiguation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=vrnqr3PG4yB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://cloud.ilabt.imec.be/index.php/s/dsqHLKFSq4zJZNk" target="_blank" rel="nofollow noreferrer">https://cloud.ilabt.imec.be/index.php/s/dsqHLKFSq4zJZNk</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution-ShareAlike 4.0 International license (CC BY-SA 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mV26hJxtSyb" data-number="299">
        <h4>
          <a href="/forum?id=mV26hJxtSyb">
              RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability
          </a>


            <a href="/pdf?id=mV26hJxtSyb" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Petr_Vytovtov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Petr_Vytovtov2">Petr Vytovtov</a>, <a href="/profile?id=~Eugenia_Elistratova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugenia_Elistratova1">Eugenia Elistratova</a>, <a href="/profile?id=~Evgenii_Tsymbalov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evgenii_Tsymbalov2">Evgenii Tsymbalov</a>, <a href="/profile?id=~Alexander_Ganshin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Ganshin1">Alexander Ganshin</a>, <a href="/profile?id=~Yuri_Pavlyukov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuri_Pavlyukov1">Yuri Pavlyukov</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#mV26hJxtSyb-details-905" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mV26hJxtSyb-details-905"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">precipitation nowcasting, weather forecasting, weather radar, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Weather radar dataset with benchmarks for nowcasting (next frame prediction) tasks with seasonal and geographical dependencies</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Precipitation nowcasting, a short-term (up to six hours) rain prediction, is arguably one of the most demanding weather forecasting tasks.
        To achieve accurate predictions, a forecasting model should consider miscellaneous meteorological and geographical data sources.
        Currently available datasets provide information only about precipitation intensity, vertically integrated liquid (VIL), or maximum reflectivity on the vertical section.
        Such single-level or aggregated data lacks description of the reflectivity change in vertical dimension, simplifying or distorting the corresponding models.

        To fill this gap, we introduce an additional dimension of the precipitation measurements in the RuDar dataset that incorporates 3D radar echo observations.
        Measurements are collected from 30 weather radars located mostly in the European part of Russia, covering multiple climate zones.
        Radar product updates every 10 minutes with a 2 km spatial resolution.
        The measurements include precipitation intensity (mm/h) at an altitude of 600 m, reflectivity (dBZ) and radial velocity (m/s) at 10 altitude levels from 1 km to 10 km with 1 km step.
        We also add the orography information as it affects the intensity and distribution of precipitation.
        The dataset includes over 50 000 timestamps over a two-year period from 2019 to 2021, totalling in roughly 100 GB of data.

        We evaluate several baselines, including optical flow and neural network models, for precipitation nowcasting on the proposed data. We also evaluate the uncertainty quantification for the ensemble scenario and show that the corresponding estimates do correlate with the ensemble errors on different sections of data.
        We believe that RuDar dataset will become a reliable benchmark for precipitation nowcasting models and also will be used in other machine learning tasks, e.g., in data shift studying, anomaly detection, or uncertainty estimation.
        Both dataset and code for data processing and model preparation are publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mV26hJxtSyb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Data: CC BY NC SA 4.0
        Code: Apache License 2.0  </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="CNJQKM5cV2o" data-number="298">
        <h4>
          <a href="/forum?id=CNJQKM5cV2o">
              ConfLab: A Rich Multimodal Multisensor Dataset of Free-Standing Social Interactions In-the-Wild
          </a>


            <a href="/pdf?id=CNJQKM5cV2o" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chirag_Raman2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chirag_Raman2">Chirag Raman</a>, <a href="/profile?id=~Jose_Vargas_Quiros1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jose_Vargas_Quiros1">Jose Vargas Quiros</a>, <a href="/profile?id=~Stephanie_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephanie_Tan1">Stephanie Tan</a>, <a href="/profile?id=~Ashraful_Islam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashraful_Islam1">Ashraful Islam</a>, <a href="/profile?id=~Ekin_Gedik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekin_Gedik1">Ekin Gedik</a>, <a href="/profile?id=~Hayley_Hung2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hayley_Hung2">Hayley Hung</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#CNJQKM5cV2o-details-242" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CNJQKM5cV2o-details-242"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Social Human Behavior, In-the-Wild Dataset, Free-standing Conversations</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose ConfLab (Conference Living Lab) as a new concept for in-the-wild recording of real-life social human behavior, and provide a dataset from the first edition of ConfLab at a major international conference.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recording the dynamics of unscripted human interactions in-the-wild is challenging due to the delicate trade-offs between several factors: participant privacy, ecological validity, data fidelity, and logistical overheads. To address these, following a 'datasets for the community by the community' ethos, we propose the Conference Living Lab (ConfLab): a new concept for multimodal multisensor data collection of in-the-wild free-standing social conversations. For the first instantiation of ConfLab described here, we organized a real-life professional networking event at a major international conference. Involving 48 conference attendees, the dataset captures a diverse mix of status, acquaintance, and networking motivations. Our capture setup improves upon the data fidelity of prior in-the-wild datasets while retaining privacy sensitivity: 8 videos (1920x1080, 60fps) from a non-invasive overhead view, and custom wearable sensors with onboard recording of body motion (full 9-axis IMU), privacy-preserving low-frequency audio (1250Hz), and Bluetooth-based proximity. Additionally, we developed custom solutions for distributed hardware synchronization at acquisition, and time-efficient continuous annotation of body keypoints and actions at high sampling rates. Our benchmarks showcase some of the open research tasks related to in-the-wild privacy-preserving social data analysis: keypoints detection from overhead camera views, skeleton-based no-audio speaker detection, and F-formation detection. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=CNJQKM5cV2o&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.4121/c.6034313" target="_blank" rel="nofollow noreferrer">https://doi.org/10.4121/c.6034313</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">Our data involves human participants and requires the signing of an End User License Agreement (available at https://doi.org/10.4121/20016194). A signed copy of the EULA needs to be sent to "SPCLabDatasets-insy@tudelft.nl". Once the request is approved by a member of the TUDelft Human Research Ethics Committee or a member of staff, private access links to download the parts of the dataset under embargo will be emailed to the requester. The process is also described on the main landing page of the dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset is available at: https://doi.org/10.4121/c.6034313
        The End User License Agreement (EULA) and Datasheet are public, while the components involving data from the human participants are under restricted access. To access these, a requester must send a signed copy of the EULA (available at https://doi.org/10.4121/20016194) to "SPCLabDatasets-insy@tudelft.nl". Once the request is approved by a member of the TUDelft Human Research Ethics Committee or a member of the administrative staff, private access links to download these parts of the dataset under embargo will be emailed to the requester. The process is also described on the main landing page of the dataset.
        For the review process, these requests (or the EULAs) are not sent to us authors. So the identities of the reviewers and the single-blinded nature of the review process are protected.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset itself is available under restricted access defined by an End-User License Agreement (EULA). The EULA itself is available under a CC0 license. The code (https://github.com/TUDelft-SPC-Lab/conflab) for the benchmark baseline tasks, and the schematics and data associated with the design of our custom wearable sensor called the Midge (https://github.com/TUDelft-SPC-Lab/spcl_midge_hardware) are available under the MIT License. The Licenses are all specified in their respective repositories. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="obfgpwBU_r_" data-number="297">
        <h4>
          <a href="/forum?id=obfgpwBU_r_">
              SAIC-IS: A Benchmark for High-Resolution Interactive Image Segmentation
          </a>


            <a href="/pdf?id=obfgpwBU_r_" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Vlad_Shakhuro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vlad_Shakhuro1">Vlad Shakhuro</a>, <a href="/profile?id=~Polina_Popenova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Polina_Popenova1">Polina Popenova</a>, <a href="/profile?id=~Anna_Vorontsova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Vorontsova1">Anna Vorontsova</a>, <a href="/profile?id=~Anton_Konushin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anton_Konushin1">Anton Konushin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#obfgpwBU_r_-details-316" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="obfgpwBU_r_-details-316"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">image segmentation benchmark, interactive segmentation, user input simulation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present SAIC-IS interactive segmentation benchmark — 1000 high resolution images and masks along with a novel evaluation procedure</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Interactive image segmentation is the core task in image editing. Existing interactive segmentation datasets contain images at a much lower resolution than the resolution of modern smartphone cameras. Hence, evaluation on such benchmarks may not represent the quality of interactive segmentation in real applications. Moreover, the standard evalution protocols shadow some crucial aspects of interactive segmentation. First, they use an IoU metric which we consider inadequate for high-resolution images, since it only measures general segmentation quality yet does not reflect the quality of segmented object boundaries. Second, evaluation protocols are deterministic; accordingly, they do not measure the robustness of interactive segmentation methods. In this paper, we propose SAIC-IS, a novel interactive segmentation benchmark containing 1000 high-resolution images and introduce a randomized evaluation protocol. We describe a procedure of simulating user input in form of clicks, strokes, and contours, and formulate two metrics measuring the quality, including the boundary quality, and the robustness of high-resolution interactive segmentation methods w.r.t. different interaction types. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=obfgpwBU_r_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://samsunglabs.github.io/saic-is/" target="_blank" rel="nofollow noreferrer">https://samsunglabs.github.io/saic-is/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT for code, Unsplash (https://unsplash.com/license) for images, CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/) for image labels</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="cWyacYBrVY" data-number="296">
        <h4>
          <a href="/forum?id=cWyacYBrVY">
              TextileNet: A Material Taxonomy-based Fashion Textile Dataset
          </a>


            <a href="/pdf?id=cWyacYBrVY" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shu_Zhong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shu_Zhong1">Shu Zhong</a>, <a href="/profile?email=miriam.ribul%40rca.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="miriam.ribul@rca.ac.uk">Miriam Ribul</a>, <a href="/profile?id=~Youngjun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngjun_Cho1">Youngjun Cho</a>, <a href="/profile?id=~Marianna_Obrist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marianna_Obrist1">Marianna Obrist</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#cWyacYBrVY-details-761" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cWyacYBrVY-details-761"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Textiles, Fibre, Fabric, Fashion, Circular Economy, Textiles Circularity, Material Science, Taxonomy-based Dataset, Image-based Retrieval</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Material Taxonomy-based Fashion Textile Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The rise of Machine Learning (ML) is gradually digitalizing and reshaping the fashion industry. Recent years have witnessed a number of fashion AI applications, for example, virtual try-ons. Textile material identification and categorization play a crucial role in the fashion textile sector, including fashion design, retails, and recycling. At the same time, Net Zero is a global goal and the fashion industry is undergoing a significant change so that textile materials can be reused, repaired and recycled in a sustainable manner. There is still a challenge in identifying textile materials automatically for garments, as we lack a low-cost and effective technique for identifying them. In light of this, we build the first fashion textile dataset, TextileNet, based on textile material taxonomies - a fibre taxonomy and a fabric taxonomy generated in collaboration with material scientists. TextileNet can be used to train and evaluate the state-of-the-art Deep Learning models for textile materials. We hope to standardize textile related datasets through the use of taxonomies. TextileNet contains 33 fibres labels and 27 fabrics labels, and has in total 760,949 images. We use standard Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to establish baselines for this dataset. Future applications for this dataset range from textile classification to optimization of the textile supply chain and interactive design for consumers. We envision that this can contribute to the development of a new AI-based fashion platform.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=cWyacYBrVY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hahashu/TextileNet" target="_blank" rel="nofollow noreferrer">https://github.com/hahashu/TextileNet</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/hahashu/TextileNet" target="_blank" rel="nofollow noreferrer">https://github.com/hahashu/TextileNet</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising. The data collection code is released with an MIT license. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ddPXQt-gM--" data-number="295">
        <h4>
          <a href="/forum?id=ddPXQt-gM--">
              Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability
          </a>


            <a href="/pdf?id=ddPXQt-gM--" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jonathan_Crabb%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Crabbé1">Jonathan Crabbé</a>, <a href="/profile?id=~Alicia_Curth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alicia_Curth1">Alicia Curth</a>, <a href="/profile?id=~Ioana_Bica1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioana_Bica1">Ioana Bica</a>, <a href="/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ddPXQt-gM---details-984" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ddPXQt-gM---details-984"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">heterogeneous treatment effect, CATE, ITE, causal inference, interpretability, feature importance, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct a benchmarking environment that allows us to empirically investigate the ability of personalized treatment effect models to identify predictive covariates.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Estimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial -- in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates -- covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation -- e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.  </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ddPXQt-gM--&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="HCnb1TByvx7" data-number="293">
        <h4>
          <a href="/forum?id=HCnb1TByvx7">
              Multilingual Abusive Comment Detection at Scale for Indic Languages
          </a>


            <a href="/pdf?id=HCnb1TByvx7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Vikram_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikram_Gupta1">Vikram Gupta</a>, <a href="/profile?id=~Sumegh_Roychowdhury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sumegh_Roychowdhury1">Sumegh Roychowdhury</a>, <a href="/profile?id=~Mithun_Das1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mithun_Das1">Mithun Das</a>, <a href="/profile?id=~Somnath_Banerjee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Somnath_Banerjee2">Somnath Banerjee</a>, <a href="/profile?email=punyajoysaha1998%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="punyajoysaha1998@gmail.com">Punyajoy Saha</a>, <a href="/profile?id=~Binny_Mathew1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binny_Mathew1">Binny Mathew</a>, <a href="/profile?email=hasta%40sharechat.co" class="profile-link" data-toggle="tooltip" data-placement="top" title="hasta@sharechat.co">Hastagiri P Vanchinathan</a>, <a href="/profile?id=~Animesh_Mukherjee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Mukherjee2">Animesh Mukherjee</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#HCnb1TByvx7-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HCnb1TByvx7-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Abusive Text Detection, Indic Languages, Social Media</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Social media platforms were conceived to act as online `town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into `mosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.
        To facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=HCnb1TByvx7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ShareChatAI/MACD" target="_blank" rel="nofollow noreferrer">https://github.com/ShareChatAI/MACD</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code and dataset is available for only research purposes and any commercial usage is strictly prohibited. The dataset MACD and model AbuseXLMR is distributed under CC BY-NC-SA license. CC BY-NC-SA allows reusers to distribute, remix, adapt, and build upon the material in any medium or format for noncommercial purposes only, and only so long as attribution is given to the creator. If you remix, adapt, or build upon the material, you must license the modified material under identical terms.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="UoEw6KigkUn" data-number="292">
        <h4>
          <a href="/forum?id=UoEw6KigkUn">
              The BigScience Corpus A 1.6TB Composite Multilingual Dataset
          </a>


            <a href="/pdf?id=UoEw6KigkUn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hugo_Lauren%C3%A7on1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hugo_Laurençon1">Hugo Laurençon</a>, <a href="/profile?id=~Lucile_Saulnier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucile_Saulnier1">Lucile Saulnier</a>, <a href="/profile?id=~Thomas_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Wang1">Thomas Wang</a>, <a href="/profile?id=~Christopher_Akiki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Akiki1">Christopher Akiki</a>, <a href="/profile?id=~Albert_Villanova_del_Moral1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Villanova_del_Moral1">Albert Villanova del Moral</a>, <a href="/profile?id=~Teven_Le_Scao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Teven_Le_Scao1">Teven Le Scao</a>, <a href="/profile?id=~Leandro_Von_Werra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leandro_Von_Werra1">Leandro Von Werra</a>, <a href="/profile?email=mouchenghao%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mouchenghao@gmail.com">Chenghao Mou</a>, <a href="/profile?id=~Eduardo_Gonz%C3%A1lez_Ponferrada1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eduardo_González_Ponferrada1">Eduardo González Ponferrada</a>, <a href="/profile?email=hiep256%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hiep256@gmail.com">Huu Nguyen</a>, <a href="/profile?id=~J%C3%B6rg_Frohberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jörg_Frohberg1">Jörg Frohberg</a>, <a href="/profile?email=mario%40huggingface.co" class="profile-link" data-toggle="tooltip" data-placement="top" title="mario@huggingface.co">Mario Šaško</a>, <a href="/profile?id=~Quentin_Lhoest1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quentin_Lhoest1">Quentin Lhoest</a>, <a href="/profile?id=~Angelina_McMillan-Major1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angelina_McMillan-Major1">Angelina McMillan-Major</a>, <a href="/profile?id=~G%C3%A9rard_Dupont1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gérard_Dupont1">Gérard Dupont</a>, <a href="/profile?id=~Stella_Biderman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stella_Biderman1">Stella Biderman</a>, <a href="/profile?id=~Anna_Rogers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Rogers1">Anna Rogers</a>, <a href="/profile?id=~Loubna_Ben_allal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Loubna_Ben_allal1">Loubna Ben allal</a>, <a href="/profile?id=~Francesco_De_Toni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_De_Toni1">Francesco De Toni</a>, <a href="/profile?id=~Giada_Pistilli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giada_Pistilli1">Giada Pistilli</a>, <a href="/profile?id=~Olivier_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Nguyen1">Olivier Nguyen</a>, <a href="/profile?id=~Somaieh_Nikpoor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Somaieh_Nikpoor1">Somaieh Nikpoor</a>, <a href="/profile?id=~Maraim_Masoud1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maraim_Masoud1">Maraim Masoud</a>, <a href="/profile?id=~Pierre_Colombo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre_Colombo2">Pierre Colombo</a>, <a href="/profile?id=~Javier_de_la_Rosa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_de_la_Rosa1">Javier de la Rosa</a>, <a href="/profile?id=~Paulo_Villegas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paulo_Villegas1">Paulo Villegas</a>, <a href="/profile?id=~Tristan_Thrush1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tristan_Thrush1">Tristan Thrush</a>, <a href="/profile?id=~Shayne_Longpre1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shayne_Longpre1">Shayne Longpre</a>, <a href="/profile?email=sebastian%40commoncrawl.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="sebastian@commoncrawl.org">Sebastian Nagel</a>, <a href="/profile?id=~Leon_Weber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leon_Weber1">Leon Weber</a>, <a href="/profile?email=mrm8488%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mrm8488@gmail.com">Manuel Romero Muñoz</a>, <a href="/profile?id=~Jian_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Zhu2">Jian Zhu</a>, <a href="/profile?id=~Daniel_Van_Strien1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Van_Strien1">Daniel Van Strien</a>, <a href="/profile?id=~Zaid_Alyafeai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zaid_Alyafeai1">Zaid Alyafeai</a>, <a href="/profile?email=k.almubarak%40psau.edu.sa" class="profile-link" data-toggle="tooltip" data-placement="top" title="k.almubarak@psau.edu.sa">Khalid Almubarak</a>, <a href="/profile?email=vumichien1692%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vumichien1692@gmail.com">Vu Minh Chien</a>, <a href="/profile?id=~Itziar_Gonzalez-Dios1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Itziar_Gonzalez-Dios1">Itziar Gonzalez-Dios</a>, <a href="/profile?id=~Aitor_Soroa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aitor_Soroa1">Aitor Soroa</a>, <a href="/profile?id=~Kyle_Lo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_Lo1">Kyle Lo</a>, <a href="/profile?id=~Manan_Dey2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manan_Dey2">Manan Dey</a>, <a href="/profile?id=~Pedro_Ortiz_Suarez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Ortiz_Suarez1">Pedro Ortiz Suarez</a>, <a href="/profile?id=~Aaron_Gokaslan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Gokaslan1">Aaron Gokaslan</a>, <a href="/profile?id=~Shamik_Bose1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shamik_Bose1">Shamik Bose</a>, <a href="/profile?id=~David_Ifeoluwa_Adelani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Ifeoluwa_Adelani1">David Ifeoluwa Adelani</a>, <a href="/profile?id=~Long_Phan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Phan1">Long Phan</a>, <a href="/profile?id=~Ian_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Yu1">Ian Yu</a>, <a href="/profile?id=~Suhas_Pai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suhas_Pai1">Suhas Pai</a>, <a href="/profile?email=violette%40huggingface.co" class="profile-link" data-toggle="tooltip" data-placement="top" title="violette@huggingface.co">Violette Lepercq</a>, <a href="/profile?id=~Suzana_Ilic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suzana_Ilic1">Suzana Ilic</a>, <a href="/profile?id=~Margaret_Mitchell3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Margaret_Mitchell3">Margaret Mitchell</a>, <a href="/profile?id=~Sasha_Luccioni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sasha_Luccioni1">Sasha Luccioni</a>, <a href="/profile?id=~Yacine_Jernite1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yacine_Jernite1">Yacine Jernite</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#UoEw6KigkUn-details-833" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UoEw6KigkUn-details-833"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">BigScience, Dataset, Multilingual, Language Modeling</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">1.6TB multilingual dataset created collaboratively within BigScience to train language models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience LargeOpen-science Open-access Multilingual language model (BLOOM). We further release a large initial subset of the corpus and analyses thereof, and hope to empower further large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research into studying this large multilingual corpus.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=UoEw6KigkUn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://huggingface.co/bigscience-catalogue-lm-data (reviewer access to be sent in an official note)</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://huggingface.co/bigscience-catalogue-lm-data
        https://github.com/bigscience-workshop/data-preparation</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">- Each constituent subset of the dataset will be released under the license that applies to it.
        - Code to be released under Apache 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KCYYqvgQUHZ" data-number="291">
        <h4>
          <a href="/forum?id=KCYYqvgQUHZ">
              SCERL: A Text-based Safety Benchmark for Reinforcement Learning Problems
          </a>


            <a href="/pdf?id=KCYYqvgQUHZ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lan_Hoang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lan_Hoang2">Lan Hoang</a>, <a href="/profile?id=~Nicolas_Galichet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Galichet1">Nicolas Galichet</a>, <a href="/profile?id=~Akifumi_Wachi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akifumi_Wachi2">Akifumi Wachi</a>, <a href="/profile?id=~Shivam_Ratnakar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shivam_Ratnakar1">Shivam Ratnakar</a>, <a href="/profile?id=~Keerthiram_Murugesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keerthiram_Murugesan1">Keerthiram Murugesan</a>, <a href="/profile?id=~Songtao_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songtao_Lu1">Songtao Lu</a>, <a href="/profile?id=~Mattia_Atzeni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mattia_Atzeni1">Mattia Atzeni</a>, <a href="/profile?email=declan.millar%40ibm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="declan.millar@ibm.com">Declan Millar</a>, <a href="/profile?id=~Michael_Katz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Katz1">Michael Katz</a>, <a href="/profile?id=~Subhajit_Chaudhury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subhajit_Chaudhury1">Subhajit Chaudhury</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#KCYYqvgQUHZ-details-447" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KCYYqvgQUHZ-details-447"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">safe reinforcement learning, text-based environment, safety constraints, safety monitoring</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents SCERL, a new text-based reinforcement learning environment with safety constraints and different monitoring of safety violations in agent observation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In recent years, the issue of safety and robustness has become a critical focus for AI research. Several works have focused on two lines of research that have so far been distinct, namely (i) safe reinforcement learning, where an agent needs to interact with the world under safety constraints, and (ii) textual reinforcement learning, where agents need to perform robust reasoning and modeling of the state of the environment. In this paper, we propose to bridge the gap between these two research directions through a benchmark of text-based environments for safety-critical problems. The contribution of this benchmark is safety-augmented environments for text games with added monitoring of safety violations. The benchmark provides a sample set of 10 games along with options to further generate a more diverse set of games with safety constraints and their corresponding metrics of safety types and difficulties. The source of safety constraints and goals are annotated from practical examples and can be adapted to more open problems. Overall, our benchmark, Safety-Constrained Environments for Reinforcement Learning (SCERL), is a flexible framework to provide a set of tasks to demonstrate safety challenges for reinforcement learning agents and aims to help the research community in exploring safety applications in a text-based domain.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KCYYqvgQUHZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">SCERL is licensed under the Apache License 2.0
        A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications, and larger works may be distributed under different terms and without source code.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LmeIEe7XC-" data-number="290">
        <h4>
          <a href="/forum?id=LmeIEe7XC-">
              CARLA-GeAR: A Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models
          </a>


            <a href="/pdf?id=LmeIEe7XC-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Federico_Nesti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Federico_Nesti1">Federico Nesti</a>, <a href="/profile?email=giulio.rossolini%40santannapisa.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="giulio.rossolini@santannapisa.it">Giulio Rossolini</a>, <a href="/profile?email=gianluca.damico%40santannapisa.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="gianluca.damico@santannapisa.it">Gianluca D'Amico</a>, <a href="/profile?email=alessandro.biondi%40santannapisa.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="alessandro.biondi@santannapisa.it">Alessandro Biondi</a>, <a href="/profile?email=giorgio.buttazzo%40santannapisa.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="giorgio.buttazzo@santannapisa.it">Giorgio Buttazzo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#LmeIEe7XC--details-430" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LmeIEe7XC--details-430"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial attack, defense, robustness, patch, autonomous driving, carla, simulator</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">CARLA-GeAR is a CARLA-based dataset generator that can be used for an extensive evaluation of the adversarial robustness of computer vision models in the context of autonomous driving.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Adversarial examples represent a serious threat for deep neural networks in several application domains and a huge amount of work has been produced to investigate them and mitigate their effects. Nevertheless, no much work has been devoted to the generation of datasets specifically designed to evaluate the adversarial robustness of neural models.
        This paper presents CARLA-GeAR, a tool for the automatic generation of photo-realistic synthetic datasets that can be used for a systematic evaluation of the adversarial robustness of neural models against physical adversarial patches, as well as for comparing the performance of different adversarial defense/detection methods. The tool is built on the CARLA simulator, using its Python API, and allows the generation of datasets for several vision tasks in the context of autonomous driving.
        The adversarial patches included in the generated datasets are attached to billboards or the back of a truck and are crafted by using state-of-the-art white-box attack strategies to maximize the prediction error of the model under test. Finally, the paper presents an experimental study to evaluate the performance of some defense methods against such attacks, showing how the datasets generated with CARLA-GeAR might be used in future work as a benchmark for adversarial defense in the real world. All the code and datasets used in this paper are available at http://carlagear.retis.santannapisa.it.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LmeIEe7XC-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://carlagear.retis.santannapisa.it" target="_blank" rel="nofollow noreferrer">http://carlagear.retis.santannapisa.it</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://carlagear.retis.santannapisa.it" target="_blank" rel="nofollow noreferrer">http://carlagear.retis.santannapisa.it</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="IIbJ9m5G73t" data-number="289">
        <h4>
          <a href="/forum?id=IIbJ9m5G73t">
              BLOX: Macro Neural Architecture Search Benchmark and Algorithms
          </a>


            <a href="/pdf?id=IIbJ9m5G73t" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Thomas_Chun_Pong_Chau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Chun_Pong_Chau1">Thomas Chun Pong Chau</a>, <a href="/profile?id=~%C5%81ukasz_Dudziak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Łukasz_Dudziak1">Łukasz Dudziak</a>, <a href="/profile?id=~Hongkai_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongkai_Wen1">Hongkai Wen</a>, <a href="/profile?id=~Nicholas_Donald_Lane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Donald_Lane1">Nicholas Donald Lane</a>, <a href="/profile?id=~Mohamed_S_Abdelfattah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_S_Abdelfattah1">Mohamed S Abdelfattah</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#IIbJ9m5G73t-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IIbJ9m5G73t-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Neural architecture search, Search space, Benchmark, Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A benchmark for NAS on a macro search space that consists of 91k unique models with accuracy and latency measurements.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Neural architecture search (NAS) has been successfully used to design numerous high-performance neural networks. However, NAS is typically compute-intensive, so most existing approaches restrict the search to decide the operations and topological structure of a single block only, then the same block is stacked repeatedly to form an end-to-end model. Although such an approach reduces the size of search space, recent studies show that a macro search space, which allows blocks in a model to be different, can lead to better performance. To provide a systematic study of the performance of NAS algorithms on a macro search space, we release Blox – a benchmark that consists of 91k unique models trained on the CIFAR-100 dataset. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. We perform extensive experiments to compare existing algorithms that are well studied on cell-based search spaces, with the emerging blockwise approaches that aim to make NAS scalable to much larger macro search spaces. The Blox benchmark and code are available at https://anonymous.4open.science/r/blox-F765.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=IIbJ9m5G73t&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/blox-F765" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/blox-F765</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/blox-F765" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/blox-F765</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The dataset will be release after the acceptance of the paper. This is to comply with the organization's guideline regarding the release of dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_5wQjYFjidc" data-number="288">
        <h4>
          <a href="/forum?id=_5wQjYFjidc">
              Latent Density Models for Uncertainty Categorization
          </a>


            <a href="/pdf?id=_5wQjYFjidc" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hao_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Sun3">Hao Sun</a>, <a href="/profile?id=~Boris_van_Breugel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_van_Breugel2">Boris van Breugel</a>, <a href="/profile?id=~Jonathan_Crabb%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Crabbé1">Jonathan Crabbé</a>, <a href="/profile?id=~Nabeel_Seedat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nabeel_Seedat1">Nabeel Seedat</a>, <a href="/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#_5wQjYFjidc-details-967" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_5wQjYFjidc-details-967"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Uncertainty Categorization, Uncertainty Quantification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a latent density-based method for uncertain-example categorization, and introduce new benchmarks for the task. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Uncertainty quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods. We introduce the confusion density matrix---a kernel-based approximation of the misclassification density---and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_5wQjYFjidc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Y2oIfr6eN2U" data-number="286">
        <h4>
          <a href="/forum?id=Y2oIfr6eN2U">
              VISUALHANDICAPS: Systematic Handicaps for Text-based Games
          </a>


            <a href="/pdf?id=Y2oIfr6eN2U" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Subhajit_Chaudhury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subhajit_Chaudhury1">Subhajit Chaudhury</a>, <a href="/profile?id=~Keerthiram_Murugesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keerthiram_Murugesan1">Keerthiram Murugesan</a>, <a href="/profile?id=~Thomas_Carta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Carta1">Thomas Carta</a>, <a href="/profile?id=~Kartik_Talamadupula1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kartik_Talamadupula1">Kartik Talamadupula</a>, <a href="/profile?id=~Michiaki_Tatsubori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michiaki_Tatsubori1">Michiaki Tatsubori</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Y2oIfr6eN2U-details-393" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y2oIfr6eN2U-details-393"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Text-based games, Natural language reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Providing systematic visual clues in partially observable text-based environments to improve agent's generalization</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present VISUALHANDICAPS, a new environment for systematic analysis of text-based games (TBGs) to improve the performance of reinforcement learning (RL) agents by providing visual handicaps or {\em hints} related to the game instances. Taking inspiration from the biological learning process that leverages mental maps of the (game) world for robust planning, we seek to analyze the performance of RL agents on TBGs when such systematic mental maps of varying difficulties are provided as handicaps. Prior methods for solving TBGs have analyzed performance improvements using textual handicaps. However, these works designed handicaps that allow additional textual information to simply measure how effectively an RL agent can understand from sequential natural language information with varying amounts of details. In contrast, our visual handicap analyzes the generalization ability of the RL agent using varying details of maps along with textual information - a setting that is inspired by learning in animals akin to real-world RL. We enable automatically generated variations and difficulty levels in our environment to emulate various real-world constraints. We perform experiments on TextWorld (TW) Cooking games and Interactive Fiction (IF) games and find that a policy that uses our systematic visual handicaps generally outperforms the previous methods in terms of success rate and the number of steps required to reach the goal. We also provide a detailed analysis of each handicap, which we believe to be important findings for driving future improvements in RL agents on TBGs.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Y2oIfr6eN2U&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/IBM/visual-hints-textworld-rl" target="_blank" rel="nofollow noreferrer">https://github.com/IBM/visual-hints-textworld-rl</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/IBM/visual-hints-textworld-rl" target="_blank" rel="nofollow noreferrer">https://github.com/IBM/visual-hints-textworld-rl</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Licensed under the

        Apache License 2.0
        A permissive license whose main conditions require preservation of copyright and license notices. Contributors provide an express grant of patent rights. Licensed works, modifications and larger works may be distributed under different terms and without source code.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="OQuswrK5ZDd" data-number="283">
        <h4>
          <a href="/forum?id=OQuswrK5ZDd">
              AutoRL-Bench 1.0
          </a>


            <a href="/pdf?id=OQuswrK5ZDd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gresa_Shala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gresa_Shala1">Gresa Shala</a>, <a href="/profile?id=~Sebastian_Pineda_Arango1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Pineda_Arango1">Sebastian Pineda Arango</a>, <a href="/profile?id=~Andr%C3%A9_Biedenkapp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Biedenkapp1">André Biedenkapp</a>, <a href="/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>, <a href="/profile?id=~Josif_Grabocka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Josif_Grabocka1">Josif Grabocka</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#OQuswrK5ZDd-details-708" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OQuswrK5ZDd-details-708"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AutoML, AutoRL, Benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">AutoRL-Bench enables easy and cheap evaluation of (multi-fidelity) HPO methods for RL as well as dynamic HPO methods. We use it to provide the first comparison of multiple HPO methods and shed light on hyperparameter importance for RL.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is well established that Reinforcement Learning (RL), while having shown tremendous successes in diverse scenarios, is very brittle and sensitive to the choice of hyperparameters. This prevents RL methods from being usable out of the box.
        The field of automated RL (AutoRL) aims at automatically configuring the RL pipeline, to both make RL usable by a broader audience, as well as reveal its full potential. Still, there has been little progress towards this goal, as new AutoRL methods are often evaluated with incompatible experimental protocols, such as different algorithms and environments.
        Furthermore, the typically high cost of experimentation prevents a thorough and meaningful comparison of different AutoRL methods or established hyperparameter optimization (HPO) methods from the automated Machine Learning (AutoML) community.
        To alleviate these issues, we propose the first tabular AutoRL Benchmark for studying the hyperparameters of RL algorithms. We consider the hyperparameter search spaces of three well established RL methods (PPO, DDPG, A2C). For every hyperparameter configuration in the search space, we compute and provide the reward curves in dozens of environments. As a result, future HPO methods can simply query our benchmark as a lookup table, instead of actually training agents. Therefore, our benchmark offers a testbed for (i) very fast, (ii) fair, and (iii) reproducible experimental protocols for comparing future black-box, gray-box, and online HPO methods for RL.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=OQuswrK5ZDd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/releaunifreiburg/AutoRL-Bench.git" target="_blank" rel="nofollow noreferrer">https://github.com/releaunifreiburg/AutoRL-Bench.git</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://rewind.tf.uni-freiburg.de/index.php/s/R9FwPznPecJRqip/download/data_arl_bench.zip" target="_blank" rel="nofollow noreferrer">https://rewind.tf.uni-freiburg.de/index.php/s/R9FwPznPecJRqip/download/data_arl_bench.zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KYcoRmqdY-" data-number="282">
        <h4>
          <a href="/forum?id=KYcoRmqdY-">
              UniFed: A Benchmark for Federated Learning Frameworks
          </a>


            <a href="/pdf?id=KYcoRmqdY-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xiaoyuan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyuan_Liu1">Xiaoyuan Liu</a>, <a href="/profile?id=~Tianneng_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianneng_Shi1">Tianneng Shi</a>, <a href="/profile?id=~Chulin_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chulin_Xie1">Chulin Xie</a>, <a href="/profile?id=~Qinbin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qinbin_Li1">Qinbin Li</a>, <a href="/profile?id=~Kangping_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kangping_Hu1">Kangping Hu</a>, <a href="/profile?id=~Haoyu_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoyu_Kim1">Haoyu Kim</a>, <a href="/profile?id=~Xiaojun_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojun_Xu1">Xiaojun Xu</a>, <a href="/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li19">Bo Li</a>, <a href="/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#KYcoRmqdY--details-635" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KYcoRmqdY--details-635"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated Learning, Federated Learning Frameworks, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Qualitative and quantitative evaluation results of nine existing popular open-sourced FL frameworks with 15 various evaluation scenarios, which provides suggestions on how to select open-source frameworks for FL real-world use cases.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning (FL) has become a practical and popular paradigm in machine learning. However, currently, there is no systematic solution that covers diverse use cases. Practitioners often face the challenge of how to select a matching FL framework for their use case. In this work, we present UniFed, the first unified benchmark for standardized evaluation of the existing open-source FL frameworks. With 15 evaluation scenarios, we present both qualitative and quantitative evaluation results of nine existing popular open-sourced FL frameworks, from the perspectives of functionality, usability, and system performance. We also provide suggestions on framework selection based on the benchmark conclusions and point out future improvement directions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KYcoRmqdY-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/AI-secure/FLBenchmark-toolkit" target="_blank" rel="nofollow noreferrer">https://github.com/AI-secure/FLBenchmark-toolkit</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YyzOqCVXtS9" data-number="281">
        <h4>
          <a href="/forum?id=YyzOqCVXtS9">
              A Benchmark for Federated Hetero-Task Learning
          </a>


            <a href="/pdf?id=YyzOqCVXtS9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Liuyi_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liuyi_Yao1">Liuyi Yao</a>, <a href="/profile?id=~Dawei_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawei_Gao1">Dawei Gao</a>, <a href="/profile?id=~Zhen_WANG2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhen_WANG2">Zhen WANG</a>, <a href="/profile?id=~Yuexiang_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuexiang_Xie1">Yuexiang Xie</a>, <a href="/profile?id=~Weirui_Kuang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weirui_Kuang2">Weirui Kuang</a>, <a href="/profile?id=~Daoyuan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daoyuan_Chen1">Daoyuan Chen</a>, <a href="/profile?id=~Haohui_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haohui_Wang1">Haohui Wang</a>, <a href="/profile?id=~Chenhe_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenhe_Dong1">Chenhe Dong</a>, <a href="/profile?id=~Bolin_Ding3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bolin_Ding3">Bolin Ding</a>, <a href="/profile?id=~Yaliang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaliang_Li1">Yaliang Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YyzOqCVXtS9-details-172" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YyzOqCVXtS9-details-172"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">To investigate the heterogeneity in federated learning in real-world scenarios, we generalize the classic federated learning to federated hetero-task learning, which emphasizes the inconsistency across the participants in federated learning in terms of both data distribution and learning tasks. We also present B-FHTL, a federated hetero-task learning benchmark consisting of simulation dataset, FL protocols and a unified evaluation mechanism. B-FHTL dataset contains three well-designed federated learning tasks with increasing heterogeneity. Each task simulates the clients with different non-IID data and learning tasks. To ensure fair comparison among different FL algorithms, B-FHTL builds in a full suite of FL protocols by providing high-level APIs to avoid privacy leakage, and presets most common evaluation metrics spanning across different learning tasks, such as regression, classification, text generation and etc. Furthermore, we compare the FL algorithms in fields of federated multi-task learning, federated personalization and federated meta learning within B-FHTL, and highlight the influence of heterogeneity and difficulties of federated hetero-task learning. Our benchmark, including the federated dataset, protocols, the evaluation mechanism and the preliminary experiment, is open-sourced at https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YyzOqCVXtS9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL" target="_blank" rel="nofollow noreferrer">https://github.com/alibaba/FederatedScope/tree/master/benchmark/B-FHTL</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License: The proposed B-FHTL is built upon the open-sourced Federated Learning package, FederatedScope (https://github.com/alibaba/FederatedScope). Both the proposed benchmark and the leveraged package are released with an Apache-2.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YX_6gnBkprm" data-number="280">
        <h4>
          <a href="/forum?id=YX_6gnBkprm">
              Realistic Comparison of the Performance of DNN Classification Models when Evaluated on Multiple Similar Datasets
          </a>


            <a href="/pdf?id=YX_6gnBkprm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Esla_Timothy_Anzaku1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Esla_Timothy_Anzaku1">Esla Timothy Anzaku</a>, <a href="/profile?id=~Haohan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haohan_Wang1">Haohan Wang</a>, <a href="/profile?id=~Arnout_Van_Messem2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnout_Van_Messem2">Arnout Van Messem</a>, <a href="/profile?id=~Wesley_De_Neve2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wesley_De_Neve2">Wesley De Neve</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#YX_6gnBkprm-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YX_6gnBkprm-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">DNN model evaluation on multiple test datasets, evaluation under mild covariate shift, leveraging uncertainty to evaluate DNN model accuracy</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a principled evaluation protocol for the effective comparison of the performance of DNN models on multiple similar-but-non-identical test datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep Neural Network (DNN) models are increasingly being subjected to evaluations on new replication test datasets that are carefully created to be similar to older and popular benchmark datasets. However, contrary to our expectations, DNN classification models show significant, consistent,  and largely unexplained degradation in accuracy on these replication datasets. While the popular evaluation approach is to evaluate the accuracy of a model on all the data points in the respective test datasets, we argue that this evaluation approach hinders us from adequately capturing the behavior of DNN models and having realistic expectations about their accuracy. Therefore, we propose a principled evaluation protocol that is suitable for comparing the accuracy of a model on multiple test datasets. By leveraging the proposed evaluation protocol, we evaluated $564$ DNN models on the CIFAR-10 and ImageNet datasets and their replication datasets. Our results suggest that the accuracy degradation between established benchmark datasets and their replications is consistently lower than published results that use conventional evaluation approaches that do not utilize the uncertainty-related information that models generate.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uL0JX0IfrH" data-number="279">
        <h4>
          <a href="/forum?id=uL0JX0IfrH">
              Benchmark Dataset for Precipitation Forecasting by Post-Processing the Numerical Weather Prediction
          </a>


            <a href="/pdf?id=uL0JX0IfrH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Taehyeon_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taehyeon_Kim1">Taehyeon Kim</a>, <a href="/profile?id=~Namgyu_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Namgyu_Ho1">Namgyu Ho</a>, <a href="/profile?email=eaststar%40kaist.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="eaststar@kaist.ac.kr">Donggyu Kim</a>, <a href="/profile?id=~Se-Young_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Se-Young_Yun1">Se-Young Yun</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#uL0JX0IfrH-details-861" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uL0JX0IfrH-details-861"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Precipitation Forecasting, Numerical Weather Forecasting, Post-Processing</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">KoMet is the most comprehensive publicly available dataset for rainfall forecasting based on post-processing NWP outputs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Precipitation forecasting is an important scientific challenge that has wide-reaching impacts on society. Historically, this challenge has been tackled using numerical weather prediction (NWP) models, grounded on physics-based simulations. Recently, many works have proposed an alternative approach, using end-to-end deep learning (DL) models to replace physics-based NWP. While these DL methods show improved performance and computational efficiency, they exhibit limitations in long-term forecasting and lack the explainability of NWP models. In this work, we present a hybrid NWP-DL workflow to fill the gap between standalone NWP and DL approaches. Under this workflow, the NWP output is fed into a deep model, which post-processes the data to yield a refined precipitation forecast. The deep model is trained with supervision, using Automatic Weather Station (AWS) observations as ground-truth labels. This can achieve the best of both worlds, and can even benefit from future improvements in NWP technology. To facilitate study in this direction, we present a novel dataset focused on the Korean peninsula, termed KoMet, (Korea Meteorological Dataset), comprised of NWP predictions and AWS observations. For NWP, we use the Global Data Assimilation and Prediction Systems-Korea Integrated Model (GDAPS-KIM). We provide analysis on a comprehensive set of baseline methods aimed at addressing the challenges of KoMet, including the sparsity of AWS observations and class imbalance. To lower the barrier to entry and encourage further study, we also provide an extensive open-source Python package for data processing and model development. We provide our benchmark data and code at https://github.com/osilab-kaist/KoMet-Benchmark-Dataset.

        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=uL0JX0IfrH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/osilab-kaist/KoMet-Benchmark-Dataset" target="_blank" rel="nofollow noreferrer">https://github.com/osilab-kaist/KoMet-Benchmark-Dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.dropbox.com/s/qachyygl2ouuy1v/KoMet.v1.0.tar.gz?dl=0" target="_blank" rel="nofollow noreferrer">https://www.dropbox.com/s/qachyygl2ouuy1v/KoMet.v1.0.tar.gz?dl=0</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2022 osilab-kaist

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.

        *Note
        In addition to the MIT License, the scope of the license for data use is being negotiated with the organizations with the rights to meteorological data in South Korea. We plan to add this in the future (Revision period).</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="m65ir_8N6ta" data-number="278">
        <h4>
          <a href="/forum?id=m65ir_8N6ta">
              Model-Agnostic Label Quality Scoring to Detect Real-World Label Errors
          </a>


            <a href="/pdf?id=m65ir_8N6ta" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=jwsm303%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jwsm303@gmail.com">Johnson Kuan</a>, <a href="/profile?id=~Jonas_Mueller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonas_Mueller1">Jonas Mueller</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#m65ir_8N6ta-details-308" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="m65ir_8N6ta-details-308"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">classification, label noise, data-centric AI</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We evaluate the performance of model-agnostic methods for detecting label errors with real-world label errors rather than synthetically introduced ones</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We consider the task of algorithmically finding wrongly labeled data, which lurks in many real-world applications and hinders the training/evaluation of ML models.  We present the first empirical study of various methods for this task that evaluates on real datasets with occurring label errors (as opposed to synthetically introduced label errors).  The model-agnostic label quality scoring methods we consider here can be utilized with arbitrary models, making them widely applicable for cleaning up diverse classification datasets.  Our evaluation examines five popular models to comprehensively characterize how well particular methods can identify label errors in practice.  We also study ensemble label quality scores that leverage multiple arbitrary models for superior detection of label errors.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=m65ir_8N6ta&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/JohnsonKuan/label-error-detection-benchmarks/" target="_blank" rel="nofollow noreferrer">https://github.com/JohnsonKuan/label-error-detection-benchmarks/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU Affero General Public License v3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="D29JbExncTP" data-number="277">
        <h4>
          <a href="/forum?id=D29JbExncTP">
              Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning
          </a>


            <a href="/pdf?id=D29JbExncTP" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yuanpei_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanpei_Chen2">Yuanpei Chen</a>, <a href="/profile?id=~Yaodong_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaodong_Yang1">Yaodong Yang</a>, <a href="/profile?id=~Tianhao_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianhao_Wu2">Tianhao Wu</a>, <a href="/profile?id=~Shengjie_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengjie_Wang2">Shengjie Wang</a>, <a href="/profile?id=~Xidong_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xidong_Feng1">Xidong Feng</a>, <a href="/profile?id=~Jiechuan_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiechuan_Jiang1">Jiechuan Jiang</a>, <a href="/profile?id=~Zongqing_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zongqing_Lu2">Zongqing Lu</a>, <a href="/profile?id=~Stephen_Marcus_McAleer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Marcus_McAleer1">Stephen Marcus McAleer</a>, <a href="/profile?id=~Hao_Dong3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Dong3">Hao Dong</a>, <a href="/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#D29JbExncTP-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D29JbExncTP-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-Agent RL, Robotics, Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a bimanual dexterous manipulation benchmark according to literature from cognitive science for comprehensive reinforcement learning research.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Achieving human-level dexterity is an important open problem in robotics. However, tasks of dexterous hand manipulation even at the baby level are challenging to solve through reinforcement learning (RL). The difficulty lies in the high degrees of freedom and the required cooperation among heterogeneous agents (e.g., joints of fingers). In this study, we propose the Bimanual Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two dexterous hands with tens of bimanual manipulation tasks and thousands of target objects. Tasks in Bi-DexHands are first designed to match human-level motor skills according to literature in cognitive science, and then are built in Issac Gym; this enables highly efficient RL trainings, reaching 30,000+ FPS by only one single NVIDIA RTX 3090. We provide a comprehensive benchmark for popular RL algorithms under different settings; this includes multi-agent RL, offline RL, multi-task RL, and meta RL. Our results show that PPO type on-policy algorithms can learn to solve simple manipulation tasks that are equivalent up to 48-month human baby (e.g., catching a flying object, opening a bottle), while multi-agent RL can further help to learn manipulations that require skilled bimanual cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each individual task, when it comes to mastering multiple manipulation skills, existing RL algorithms fail to work in most of the multi-task and the few-shot learning tasks, which calls for more future development from the RL community. Our project is open-sourced at https://github.com/PKU-MARL/DexterousHands.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=D29JbExncTP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/PKU-MARL/DexterousHands" target="_blank" rel="nofollow noreferrer">https://github.com/PKU-MARL/DexterousHands</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/PKU-MARL/DexterousHands
        You can go to our github repository page to access our dataset and benchmark. For specific steps, please refer to the README.md in our repository.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All dataset or code are under a Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="tXEe-Ew_ikh" data-number="275">
        <h4>
          <a href="/forum?id=tXEe-Ew_ikh">
              Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems
          </a>


            <a href="/pdf?id=tXEe-Ew_ikh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Abishek_Thangamuthu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abishek_Thangamuthu1">Abishek Thangamuthu</a>, <a href="/profile?id=~Gunjan_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunjan_Kumar2">Gunjan Kumar</a>, <a href="/profile?id=~Suresh_Bishnoi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suresh_Bishnoi1">Suresh Bishnoi</a>, <a href="/profile?id=~Ravinder_Bhattoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ravinder_Bhattoo1">Ravinder Bhattoo</a>, <a href="/profile?id=~N_M_Anoop_Krishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~N_M_Anoop_Krishnan1">N M Anoop Krishnan</a>, <a href="/profile?id=~Sayan_Ranu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sayan_Ranu2">Sayan Ranu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#tXEe-Ew_ikh-details-749" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tXEe-Ew_ikh-details-749"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">physics-informed graph neural network, dynamical systems, benchmarking study, neural ODE, Lagrangian neural network, Hamiltonian neural network</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Benchmarking physics-informed graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of ten different physics-informed graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Finally, we evaluate them on the spring and pendulum systems with varying sizes to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases such as explicit constraints, and decoupling of kinetic and potential energies can significantly enhance the performance of GNNs. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=tXEe-Ew_ikh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/benchmarking_graph-20D2" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/benchmarking_graph-20D2</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://anonymous.4open.science/r/benchmarking_graph-20D2" target="_blank" rel="nofollow noreferrer">https://anonymous.4open.science/r/benchmarking_graph-20D2</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>