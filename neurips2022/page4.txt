<ul class="list-unstyled submissions-list">
    <li class="note " data-id="3HCT3xfNm9r" data-number="274">
        <h4>
          <a href="/forum?id=3HCT3xfNm9r">
              Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset
          </a>


            <a href="/pdf?id=3HCT3xfNm9r" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Peter_Henderson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Henderson1">Peter Henderson</a>, <a href="/profile?id=~Mark_Simon_Krass1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Simon_Krass1">Mark Simon Krass</a>, <a href="/profile?id=~Lucia_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucia_Zheng1">Lucia Zheng</a>, <a href="/profile?id=~Neel_Guha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neel_Guha1">Neel Guha</a>, <a href="/profile?id=~Christopher_D_Manning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_D_Manning1">Christopher D Manning</a>, <a href="/profile?id=~Dan_Jurafsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Jurafsky1">Dan Jurafsky</a>, <a href="/profile?id=~Daniel_E._Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_E._Ho1">Daniel E. Ho</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#3HCT3xfNm9r-details-58" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3HCT3xfNm9r-details-58"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">data curation, legal data, content filtering, ai and law</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this work we have examine how the law and legal data can inform data filtering practices and provide an extensive 256GB legal dataset (the Pile of Law) that can be used to learn these norms, and for pretraining.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take into account context. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a 256GB dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may potentially help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://huggingface.co/datasets/pile-of-law/pile-of-law" target="_blank" rel="nofollow noreferrer">https://huggingface.co/datasets/pile-of-law/pile-of-law</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://huggingface.co/datasets/pile-of-law/pile-of-law" target="_blank" rel="nofollow noreferrer">https://huggingface.co/datasets/pile-of-law/pile-of-law</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Pile of Law is released under a CreativeCommons Attribution-NonCommercial-ShareAlike 4.0 International license, but some data subsets come under different open license variations. See paper appendices for a detailed subset-by-subset breakdown of licenses.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="7ywelx5Yieb" data-number="273">
        <h4>
          <a href="/forum?id=7ywelx5Yieb">
              DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech
          </a>


            <a href="/pdf?id=7ywelx5Yieb" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Keon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keon_Lee1">Keon Lee</a>, <a href="/profile?id=~Kyumin_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyumin_Park1">Kyumin Park</a>, <a href="/profile?id=~Daeyoung_Kim3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daeyoung_Kim3">Daeyoung Kim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 06 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#7ywelx5Yieb-details-174" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7ywelx5Yieb-details-174"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Text-to-speech, conversational TTS, TTS dataset, spontaneous</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">DailyTalk is a dataset for Text-to-Speech which contains contextual information.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Majority of current TTS datasets, which are collections of individual utterances, contain little conversational aspects in terms of both style and metadata. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for Text-to-Speech. We sampled, modified and recorded 2,541 dialogues from open-domain dialogue dataset DailyDialog which are adequately long to represent context of each dialogue. During data construction step we manage to maintain attributes distribution originally annotated in DailyDialog to support diverse dialogue in DailyTalk.
        On top of our dataset, we extend a prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialog. We gather metadata so that a TTS model can learn historical dialog information, the key to generate context-aware speech.
        From the baseline experiment results, we show DailyTalk can be used to train neural text-to-speech models and our baseline can represent contextual information. The DailyTalk dataset and baseline code are freely available for academical use. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=7ywelx5Yieb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/keonlee9420/DailyTalk" target="_blank" rel="nofollow noreferrer">https://github.com/keonlee9420/DailyTalk</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/keonlee9420/DailyTalk

        Link to the dataset is in the instruction page in provided Github page. Anyone can download dataset in Google drive. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="CZeIOfCjMf" data-number="272">
        <h4>
          <a href="/forum?id=CZeIOfCjMf">
              HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions
          </a>


            <a href="/pdf?id=CZeIOfCjMf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lingjiao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lingjiao_Chen1">Lingjiao Chen</a>, <a href="/profile?id=~Zhihua_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhihua_Jin1">Zhihua Jin</a>, <a href="/profile?id=~Sabri_Eyuboglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sabri_Eyuboglu1">Sabri Eyuboglu</a>, <a href="/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>, <a href="/profile?id=~Matei_Zaharia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matei_Zaharia1">Matei Zaharia</a>, <a href="/profile?id=~James_Y._Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Y._Zou1">James Y. Zou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#CZeIOfCjMf-details-881" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CZeIOfCjMf-details-881"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">ML API, performance shift, model update</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct and analyze a large-scale longitudinal dataset of commercial ML API predictions.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoptions in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performances. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and an integral part of analytics, it is critical to systematically study and compare different APIs with each other and to characterize how individual APIs change over time. However, this practically important topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition, and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API’s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML  as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs’ performance changes substantially over time—several APIs’ accuracies dropped on specific benchmark datasets. Even when the API’s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs’ performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=CZeIOfCjMf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/lchen001/HAPI/" target="_blank" rel="nofollow noreferrer">https://github.com/lchen001/HAPI/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/lchen001/HAPI/" target="_blank" rel="nofollow noreferrer">https://github.com/lchen001/HAPI/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bUCgNT9jGK" data-number="270">
        <h4>
          <a href="/forum?id=bUCgNT9jGK">
              FedGraphNN: A Federated Learning Benchmark System for Graph Neural Networks
          </a>


            <a href="/pdf?id=bUCgNT9jGK" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chaoyang_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaoyang_He1">Chaoyang He</a>, <a href="/profile?id=~Keshav_Balasubramanian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keshav_Balasubramanian1">Keshav Balasubramanian</a>, <a href="/profile?id=~Emir_Ceyani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emir_Ceyani1">Emir Ceyani</a>, <a href="/profile?id=~Carl_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carl_Yang1">Carl Yang</a>, <a href="/profile?id=~Han_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Xie1">Han Xie</a>, <a href="/profile?id=~Lichao_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lichao_Sun1">Lichao Sun</a>, <a href="/profile?id=~Lifang_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lifang_He1">Lifang He</a>, <a href="/profile?id=~Liangwei_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liangwei_Yang1">Liangwei Yang</a>, <a href="/profile?id=~Philip_S._Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_S._Yu1">Philip S. Yu</a>, <a href="/profile?id=~Yu_Rong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Rong1">Yu Rong</a>, <a href="/profile?id=~Peilin_Zhao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peilin_Zhao2">Peilin Zhao</a>, <a href="/profile?id=~Junzhou_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junzhou_Huang1">Junzhou Huang</a>, <a href="/profile?id=~Murali_Annavaram1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Murali_Annavaram1">Murali Annavaram</a>, <a href="/profile?id=~Salman_Avestimehr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Salman_Avestimehr1">Salman Avestimehr</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#bUCgNT9jGK-details-674" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bUCgNT9jGK-details-674"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">federated learning, graph neural networks, recommender system, subgraphs, molecular property prediction, citation networks, social networks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Federated Learning Benchmark for training graph neural networks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Neural Network (GNN) research is rapidly growing thanks to the capacity of GNNs in learning representations from graph-structured data.
        However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to privacy regulations and regulatory restrictions. Federated learning (FL), a trending distributed learning paradigm, provides possibilities to solve this challenge while preserving data privacy. Despite recent advances in vision and language domains, there is no suitable platform for the FL of GNNs.
        To this end, we introduce \ours, an open FL benchmark system to foster federated GNN research. \ours is built on a unified formulation of graph FL and contains a wide range of datasets from different domains, popular GNN models, and FL algorithms, with industry-grade secure, efficient, and deployable system support. Particularly for the datasets, we collect, preprocess, and partition 36 datasets from 7 domains, including both publicly available ones and specifically obtained ones such as \texttt{hERG} and \texttt{Tencent}. Inspired from OGB, we propose \texttt{FedOGB} and \texttt{FedOGB-LSC} to foster federated GNN research. Our empirical analysis showcases the utility of our benchmark system, while exposing significant challenges in graph FL: federated GNNs perform worse in most datasets with a non-IID split than centralized GNNs; the GNN model that attains the best result in the centralized setting may not maintain its advantage in the FL setting. These results imply that more research efforts are needed to unravel the mystery behind federated GNNs. Moreover, our system performance analysis demonstrates that the \texttt{FedGraphNN} system is computationally efficient and secure to large-scale graphs datasets. We maintain the source code, dataset, and detailed tutorials at \url{https://github.com/FedML-AI/FedGraphNN}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bUCgNT9jGK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/FedML-AI/FedGraphNN" target="_blank" rel="nofollow noreferrer">https://github.com/FedML-AI/FedGraphNN</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Bs8iFQ7AM6" data-number="269">
        <h4>
          <a href="/forum?id=Bs8iFQ7AM6">
              DC-BENCH: Dataset Condensation Benchmark
          </a>


            <a href="/pdf?id=Bs8iFQ7AM6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Justin_Cui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Cui1">Justin Cui</a>, <a href="/profile?id=~Ruochen_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruochen_Wang2">Ruochen Wang</a>, <a href="/profile?id=~Si_Si1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Si_Si1">Si Si</a>, <a href="/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Bs8iFQ7AM6-details-747" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Bs8iFQ7AM6-details-747"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset compression, dataset condensation, neural architecture search</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue.
        The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Bs8iFQ7AM6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/justincui03/dc_benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/justincui03/dc_benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bWTLEhZXfJB" data-number="268">
        <h4>
          <a href="/forum?id=bWTLEhZXfJB">
              Towards benchmarking and developing meta-teaching agents that can learn to teach
          </a>


            <a href="/pdf?id=bWTLEhZXfJB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Paul_Bertens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Bertens1">Paul Bertens</a>, <a href="/profile?id=~Seong-Whan_Lee3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seong-Whan_Lee3">Seong-Whan Lee</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#bWTLEhZXfJB-details-67" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bWTLEhZXfJB-details-67"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, meta-learning, meta-teaching, curriculum learning, lifelong learning, AI safety, cooperative AI, AGI</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a different type of environmental setup that is focused on producing meta-teaching agents that not only have to learn how to solve new tasks, but that also have to learn how to teach other agents to solve those tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A wide variety of simulated environments have been developed in recent years that require reinforcement learning agents to solve some particular task. However, the end-goal of these environments is generally the same, producing agents that can directly solve a given task. Here we propose a different type of environmental setup that is focused on producing meta-teaching agents that not only have to learn to solve new tasks, but also have to learn how to teach other agents to solve those tasks. We design a curriculum based learning environment with the goal of evaluating such meta-teaching agents (i.e. agents that learn to teach). The proposed curriculum consists of different phases that alternate between a teacher and student agent, requiring the student to learn to solve new tasks purely through feedback from the teacher. This type of environmental design hopefully encourages work towards the development of a meta-teaching agent that can not only learn to solve increasingly complex tasks, but that can also teach others to solve those tasks. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bWTLEhZXfJB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU General Public License v3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="hGl8rsmNXzs" data-number="267">
        <h4>
          <a href="/forum?id=hGl8rsmNXzs">
              ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models
          </a>


            <a href="/pdf?id=hGl8rsmNXzs" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chunyuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chunyuan_Li1">Chunyuan Li</a>, <a href="/profile?id=~Haotian_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haotian_Liu1">Haotian Liu</a>, <a href="/profile?id=~Liunian_Harold_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liunian_Harold_Li1">Liunian Harold Li</a>, <a href="/profile?id=~Pengchuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengchuan_Zhang1">Pengchuan Zhang</a>, <a href="/profile?id=~Jyoti_Aneja2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jyoti_Aneja2">Jyoti Aneja</a>, <a href="/profile?id=~Jianwei_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianwei_Yang1">Jianwei Yang</a>, <a href="/profile?id=~Ping_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Jin1">Ping Jin</a>, <a href="/profile?id=~Yong_Jae_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Jae_Lee2">Yong Jae Lee</a>, <a href="/profile?id=~Houdong_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Houdong_Hu1">Houdong Hu</a>, <a href="/profile?id=~Zicheng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zicheng_Liu1">Zicheng Liu</a>, <a href="/profile?id=~Jianfeng_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Gao1">Jianfeng Gao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#hGl8rsmNXzs-details-2" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hGl8rsmNXzs-details-2"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">evaluation platform, task-level transfer, language-image pre-training, image classification, object detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">ELEVATER provides the first public platform and toolkit to evaluate vision foundation models in their large-scale task-level visual transfer in 20 image classification tasks and 35 object detection tasks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains challenging to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark to compare and evaluate pre-trained language-augmented visual models. Several highlights include: (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to ensure the fairness in model adaption. To leverage the full power of language-augmented visual models, novel language-aware initialization methods are proposed to significantly improve the adaption performance. (iii) Metrics. A variety of evaluation metrics are used, including sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). We will publicly release ELEVATER.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=hGl8rsmNXzs&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://computer-vision-in-the-wild.github.io/ELEVATER/" target="_blank" rel="nofollow noreferrer">https://computer-vision-in-the-wild.github.io/ELEVATER/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/Computer-Vision-in-the-Wild/DataDownload" target="_blank" rel="nofollow noreferrer">https://github.com/Computer-Vision-in-the-Wild/DataDownload</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is under MIT license.
        The data of our benchmark includes two parts: (1) A suite of public datasets. Please follow the original license of each dataset. (2) External knowledge. The knowledge extracted from WordNet/Wiktionary follows their own licenses. For the knowledge extracted from GPT3, we have the approval from OpenAI to use it for this research benchmark to encourage future studies.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="HYELrdRdJI" data-number="265">
        <h4>
          <a href="/forum?id=HYELrdRdJI">
              MVP-N: A Dataset and Benchmark for Real-World Multi-View Object Classification
          </a>


            <a href="/pdf?id=HYELrdRdJI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ren_Wang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ren_Wang7">Ren Wang</a>, <a href="/profile?id=~Jiayue_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayue_Wang1">Jiayue Wang</a>, <a href="/profile?id=~Tae_Sung_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tae_Sung_Kim1">Tae Sung Kim</a>, <a href="/profile?id=~JINSUNG_KIM1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~JINSUNG_KIM1">JINSUNG KIM</a>, <a href="/profile?id=~Hyuk-Jae_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyuk-Jae_Lee1">Hyuk-Jae Lee</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#HYELrdRdJI-details-593" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HYELrdRdJI-details-593"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">multi-view object classification, learning from noisy labels, dataset and benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper presents a dataset and benchmark for multi-view object classification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Combining information from multiple views is essential for discriminating similar objects. However, existing datasets for multi-view object classification have several limitations, such as synthetic and coarse-grained objects, no validation split for hyper-parameter tuning, and a lack of view-level information quantity annotations for analyzing multi-view-based methods. To address these issues, this paper proposes a new dataset, called MVP-N, containing 44 retail products, 16k real captured views with human-perceived information quantity annotations, and 9k multi-view sets. The fine-grained categorization of objects naturally generates multi-view label noise due to the inter-class view similarity, which also enables the study of learning from noisy labels. Moreover, this paper presents benchmarks for 4 feature aggregation and 12 soft label methods on MVP-N. Experimental results show that the proposed dataset is a useful resource to facilitate the development of feature aggregation and soft label methods for real-world multi-view object classification. The dataset and code are publicly available at https://github.com/SMNUResearch/MVP-N.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=HYELrdRdJI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/SMNUResearch/MVP-N" target="_blank" rel="nofollow noreferrer">https://github.com/SMNUResearch/MVP-N</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/SMNUResearch/MVP-N" target="_blank" rel="nofollow noreferrer">https://github.com/SMNUResearch/MVP-N</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-ND License for the dataset; MIT License for the code</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="2N8JzuiWZ25" data-number="264">
        <h4>
          <a href="/forum?id=2N8JzuiWZ25">
              OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology
          </a>


            <a href="/pdf?id=2N8JzuiWZ25" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cheng_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Jiang2">Cheng Jiang</a>, <a href="/profile?id=~Asadur_Zaman_Chowdury1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Asadur_Zaman_Chowdury1">Asadur Zaman Chowdury</a>, <a href="/profile?id=~Xinhai_Hou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinhai_Hou1">Xinhai Hou</a>, <a href="/profile?id=~Akhil_Kondepudi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akhil_Kondepudi1">Akhil Kondepudi</a>, <a href="/profile?id=~Christian_Freudiger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Freudiger1">Christian Freudiger</a>, <a href="/profile?id=~Kyle_Stephen_Conway1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_Stephen_Conway1">Kyle Stephen Conway</a>, <a href="/profile?email=sandraca%40med.umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sandraca@med.umich.edu">Sandra Camelo-Piragua</a>, <a href="/profile?email=daniel.orringer%40nyulangone.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="daniel.orringer@nyulangone.org">Daniel A Orringer</a>, <a href="/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="/profile?id=~Todd_Hollon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Todd_Hollon1">Todd Hollon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#2N8JzuiWZ25-details-609" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2N8JzuiWZ25-details-609"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Stimulated Raman Histology, Computer Vision, Convolutional Neural Network, Vision Transformer, Contrastive Learning, Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">OpenSRH is the first ever publicly available stimulated Raman histology (SRH) dataset and benchmark, which will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multi-class histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=2N8JzuiWZ25&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">opensrh.mlins.org</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">opensrh.mlins.org</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The data will be released under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0), and the companion source code will be released under MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ChWf1E43l4" data-number="263">
        <h4>
          <a href="/forum?id=ChWf1E43l4">
              DABS 2.0: Improved Datasets and Algorithms for Universal Self-Supervision
          </a>


            <a href="/pdf?id=ChWf1E43l4" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Alex_Tamkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Tamkin1">Alex Tamkin</a>, <a href="/profile?id=~Gaurab_Banerjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaurab_Banerjee1">Gaurab Banerjee</a>, <a href="/profile?id=~Mohamed_Owda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_Owda1">Mohamed Owda</a>, <a href="/profile?email=vliu15%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="vliu15@stanford.edu">Vincent Liu</a>, <a href="/profile?id=~Shashank_Rammoorthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shashank_Rammoorthy1">Shashank Rammoorthy</a>, <a href="/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#ChWf1E43l4-details-187" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ChWf1E43l4-details-187"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, domain agnostic</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We extend the DABS benchmark, presenting improved datasets and algorithms for universal self-supervision</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Universal self-supervised (SSL) algorithms hold enormous promise for making machine learning accessible to high-impact domains such as protein biology, manufacturing, and genomics. We present DABS 2.0: a set of improved datasets and algorithms for advancing research on universal SSL. We extend the recently-introduced DABS benchmark with the addition of five real-world science and engineering domains: protein biology, bacterial genomics, multispectral satellite imagery, semiconductor wafers, and particle physics, bringing the total number of domains in the benchmark to twelve. We also propose a new universal SSL algorithm, Capri, and a generalized version of masked autoencoding, and apply both on all twelve domains---the most wide-ranging exploration of SSL yet. We find that multiple algorithms show gains across domains, outperforming previous baselines. In addition, we demonstrate the usefulness of DABS for scientific study of SSL by investigating the optimal corruption rate for each algorithm, showing that the best setting varies based on the domain. Code will be released at http://github.com/alextamkin/dabs}{http://github.com/alextamkin/dabs</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ChWf1E43l4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">dabs.stanford.edu (will be updated with new domains)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9WqAXsbKDO" data-number="261">
        <h4>
          <a href="/forum?id=9WqAXsbKDO">
              The StarCraft Multi-Agent Challenges$^{+}$ : Learning Multi-Stage Tasks and Environmental Factors without Precise Reward Functions
          </a>


            <a href="/pdf?id=9WqAXsbKDO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mingyu_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyu_Kim2">Mingyu Kim</a>, <a href="/profile?id=~Jihwan_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihwan_Oh1">Jihwan Oh</a>, <a href="/profile?id=~Yongsik_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongsik_Lee1">Yongsik Lee</a>, <a href="/profile?id=~Joonkee_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joonkee_Kim1">Joonkee Kim</a>, <a href="/profile?id=~SeongHwan_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SeongHwan_Kim2">SeongHwan Kim</a>, <a href="/profile?id=~Song_Chong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song_Chong1">Song Chong</a>, <a href="/profile?id=~Se-Young_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Se-Young_Yun1">Se-Young Yun</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#9WqAXsbKDO-details-240" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9WqAXsbKDO-details-240"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent reinforcement learning, Benchmark, Exploration, StarCraft Challenge, Offensive Scenario, Multi-Stage tasks, Environmental Factors</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we propose a novel benchmark called the StarCraft Multi-Agent Challenges$^{+}$(SMAC$^{+}$), where agents learn to perform multi-stage tasks and to use environmental factors without precise reward functions. The previous challenges (SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning are mainly concerned with ensuring that all agents cooperatively eliminate approaching adversaries only through fine manipulation with obvious reward functions. SMAC$^{+}$, on the other hand, is interested in the exploration capability of MARL algorithms to efficiently learn implicit multi-stage tasks and environmental factors as well as micro-control. This study covers both offensive and defensive scenarios. In the offensive scenarios, agents must learn to first find opponents and then eliminate them. The defensive scenarios require agents to use topographic features. For example, agents need to position themselves behind protective structures to make it harder for enemies to attack. We investigate MARL algorithms under SMAC$^{+}$ and observe that recent approaches work well in similar settings to the previous challenges, but misbehave in offensive scenarios. Additionally, we observe that an enhanced exploration approach has a positive effect on performance but is not able to completely solve all scenarios and settings and propose directions for future research. Code for the benchmark and implemented baselines is available at https://github.com/osilab-kaist/smac_plus</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=9WqAXsbKDO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://osilab-kaist.github.io/smac_plus/" target="_blank" rel="nofollow noreferrer">https://osilab-kaist.github.io/smac_plus/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/osilab-kaist/smac_plus" target="_blank" rel="nofollow noreferrer">https://github.com/osilab-kaist/smac_plus</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="B2W8Vy0rarw" data-number="260">
        <h4>
          <a href="/forum?id=B2W8Vy0rarw">
              EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records
          </a>


            <a href="/pdf?id=B2W8Vy0rarw" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gyubok_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gyubok_Lee1">Gyubok Lee</a>, <a href="/profile?id=~Hyeonji_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyeonji_Hwang1">Hyeonji Hwang</a>, <a href="/profile?id=~Seongsu_Bae1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seongsu_Bae1">Seongsu Bae</a>, <a href="/profile?email=dustn1259%40swu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="dustn1259@swu.ac.kr">Yeonsu Kwon</a>, <a href="/profile?id=~Woncheol_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Woncheol_Shin1">Woncheol Shin</a>, <a href="/profile?id=~Seongjun_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seongjun_Yang1">Seongjun Yang</a>, <a href="/profile?id=~Minjoon_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjoon_Seo1">Minjoon Seo</a>, <a href="/profile?email=jykim%40kyuh.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="jykim@kyuh.ac.kr">Jong-Yeup Kim</a>, <a href="/profile?id=~Edward_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Choi1">Edward Choi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#B2W8Vy0rarw-details-825" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B2W8Vy0rarw-details-825"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">EHR, EHR QA, Text-to-SQL, semantic parsing</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A new text-to-SQL dataset for electronic health records (EHRs)</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present a new text-to-SQL dataset for electronic health records (EHRs), where the utterances are collected from 222 hospital staff—including physicians, nurses, and insurance review and health records teams through a poll conducted at a university hospital. Along with the utterances covering the actual needs of hospital staff, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, to construct a healthcare QA dataset. Our dataset poses three unique challenges: understanding the meaning and generating queries for the questions that 1) reflect the actual needs of different professionals in the hospital and 2) contain various types of time expressions, while 3)  assessing the trustworthiness of the QA model for system reliability. We believe that our dataset could serve as a benchmark to develop semantic parsing models in the hospital workplace and take a further step towards bridging the gap between academic and industrial settings in the healthcare domain.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=B2W8Vy0rarw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/glee4810/EHRSQL" target="_blank" rel="nofollow noreferrer">https://github.com/glee4810/EHRSQL</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="nUTemM6v9sv" data-number="259">
        <h4>
          <a href="/forum?id=nUTemM6v9sv">
              HandMeThat: Human-Robot Communication in Physical and Social Environments
          </a>


            <a href="/pdf?id=nUTemM6v9sv" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yanming_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanming_Wan1">Yanming Wan</a>, <a href="/profile?id=~Jiayuan_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayuan_Mao1">Jiayuan Mao</a>, <a href="/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#nUTemM6v9sv-details-39" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nUTemM6v9sv-details-39"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pragmatic Reasoning, Goal Inference, Instruction Following</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">HandMeThat is a benchmark for evaluating instruction understanding and following in physical and social contexts.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social contexts. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human intentions) contexts. HandMeThat contains a collection of 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction.
        In this paper, we present a textual interface for our benchmark, where the robot interact with a physically grounded virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting a significant room for future work on physical and social human-robot communications and interactions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=nUTemM6v9sv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/hand-me-that/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/hand-me-that/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/hand-me-that/" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/hand-me-that/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code for the benchmark is under the MIT license: https://opensource.org/licenses/MIT</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="sd1fv0g3UO1" data-number="257">
        <h4>
          <a href="/forum?id=sd1fv0g3UO1">
              CLEVRER-Humans: Describing Physical and Causal Events the Human Way
          </a>


            <a href="/pdf?id=sd1fv0g3UO1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xuelin_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuelin_Yang1">Xuelin Yang</a>, <a href="/profile?id=~Jiayuan_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayuan_Mao1">Jiayuan Mao</a>, <a href="/profile?id=~Xikun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xikun_Zhang1">Xikun Zhang</a>, <a href="/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>, <a href="/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#sd1fv0g3UO1-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sd1fv0g3UO1-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Physical Reasoning, Causal Reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The CLEVRER-Humans benchmark is a video reasoning dataset for causal judgment of physical events with human labels. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of the causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting great challenges set forth by our benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=sd1fv0g3UO1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/stanford.edu/clevrer-humans" target="_blank" rel="nofollow noreferrer">https://sites.google.com/stanford.edu/clevrer-humans</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/stanford.edu/clevrer-humans" target="_blank" rel="nofollow noreferrer">https://sites.google.com/stanford.edu/clevrer-humans</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset is under CC0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="z1d8fUiS8Cr" data-number="256">
        <h4>
          <a href="/forum?id=z1d8fUiS8Cr">
              Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities
          </a>


            <a href="/pdf?id=z1d8fUiS8Cr" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zejiang_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zejiang_Shen1">Zejiang Shen</a>, <a href="/profile?id=~Kyle_Lo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_Lo1">Kyle Lo</a>, <a href="/profile?email=laurenyu%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="laurenyu@umich.edu">Lauren Yu</a>, <a href="/profile?id=~Nathan_Dahlberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nathan_Dahlberg1">Nathan Dahlberg</a>, <a href="/profile?email=mschlan%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mschlan@umich.edu">Margo Schlanger</a>, <a href="/profile?id=~Doug_Downey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Doug_Downey1">Doug Downey</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#z1d8fUiS8Cr-details-341" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="z1d8fUiS8Cr-details-341"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Abstractive Summarization, Multi-Document Summarization, Legal Document Summarization, Controlled Summarization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Multi-LexSum is a multi-doc summarization dataset for civil rights litigations lawsuits with summaries of three granularities. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">With the advent of large language models, methods for abstractive summarization have made great strides, creating the potential for their use in applications aiding knowledge workers in processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC), which organizes documents and disseminates information about civil rights lawsuits for both legal scholars and the general public. The CRLC's real-world summarization efforts require extensive training of legal scholars who spend hours per case to process multiple relevant documents and produce high-quality summaries of key events and outcomes. Motivated by this ongoing annotation effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing writing efforts at the CRLC. Multi-LexSum presents a challenging multi-document summarization task given the impressive length of the input documents, often exceeding one hundred pages. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence "extreme" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the data (adherence to strict content and style guidelines, inter alia), state-of-the-art summarization models struggle to perform well on this task. We release Multi-LexSum to both further research in summarization methods as well as facilitate development of applications to assist in the CRLC's mission.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=z1d8fUiS8Cr&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">multilexsum.github.io</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://multilexsum.github.io, https://github.com/multilexsum/dataset</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The Multi-LexSum dataset is distributed under the Open Data Commons Attribution License (ODC-By). The case summaries and metadata are licensed under the Creative Commons Attribution License (CC BY-NC), and the source documents are already in the public domain. Commercial users who desire a license for summaries and metadata can contact info@clearinghouse.net, which will allow free use but limit summary reposting. The corresponding code for downloading and loading the dataset is licensed under the Apache License 2.0. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uduG0ufxbD" data-number="255">
        <h4>
          <a href="/forum?id=uduG0ufxbD">
              FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems
          </a>


            <a href="/pdf?id=uduG0ufxbD" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Md_Mahim_Anjum_Haque1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Md_Mahim_Anjum_Haque1">Md Mahim Anjum Haque</a>, <a href="/profile?id=~Wasi_Uddin_Ahmad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wasi_Uddin_Ahmad1">Wasi Uddin Ahmad</a>, <a href="/profile?id=~Ismini_Lourentzou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ismini_Lourentzou1">Ismini Lourentzou</a>, <a href="/profile?id=~Chris_Brown1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Brown1">Chris Brown</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#uduG0ufxbD-details-787" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uduG0ufxbD-details-787"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Source code repositories consist of large codebases, often containing error-prone programs. The increasing complexity of software has led to a drastic rise in time and costs for identifying and fixing these defects. Various methods exist to automatically generate fixes for buggy code. However, due to the large combinatorial space of possible solutions for a particular bug, there are not many tools and datasets available to evaluate generated code effectively. In this work, we introduce \textbf{\textsc{FixEval}}, a benchmark comprising buggy code submissions to competitive programming problems and their respective fixes.
        We introduce a rich test suite to evaluate and assess the correctness of model-generated program fixes. We consider two Transformer language models pretrained on programming languages as our baselines, and compare them using match-based and execution-based evaluation metrics. Our experiments show that match-based metrics do not reflect model-generated program fixes accurately, while execution-based methods evaluate programs through all cases and scenarios specifically designed for that solution.
        Therefore, we believe \textbf{\textsc{FixEval}} provides a step towards real-world automatic bug fixing and model-generated code evaluation.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mahimanzum/FixEval" target="_blank" rel="nofollow noreferrer">https://github.com/mahimanzum/FixEval</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1dzuHuouuWzlFCy1CMj9DYG9JGraEay27?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1dzuHuouuWzlFCy1CMj9DYG9JGraEay27?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">No License or Restrictions</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ehPXtrsEpuE" data-number="254">
        <h4>
          <a href="/forum?id=ehPXtrsEpuE">
              Game-Theoretic Machine Learning
          </a>


            <a href="/pdf?id=ehPXtrsEpuE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xinrun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinrun_Wang1">Xinrun Wang</a>, <a href="/profile?id=~Chongyang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongyang_Gao1">Chongyang Gao</a>, <a href="/profile?id=~Chang_Yang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chang_Yang3">Chang Yang</a>, <a href="/profile?id=~Shuxin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuxin_Li1">Shuxin Li</a>, <a href="/profile?id=~Youzhi_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youzhi_Zhang2">Youzhi Zhang</a>, <a href="/profile?id=~Bo_An2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_An2">Bo An</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ehPXtrsEpuE-details-615" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ehPXtrsEpuE-details-615"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Game Theory, Machine Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a comprehensive benchmark of game-theoretic methods to game-theoretic machine learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work, we propose Game-Theoretic Machine Learning (GTML), i.e., \emph{machine learning problems which can be viewed as games between multiple players}. The representative scenarios of GTML include generative adversarial networks (GAN), adversarial training (AT), distributional robust optimization (DRO), and robust adversarial reinforcement learning (RARL). Most works rely on gradient descent/ascent (GDA) and its variants to solve the game-theoretic formulations. Despite the empirical success, GDA does not have any guarantee of the convergence to the equilibrium. While in game theory, a simple yet effective framework, double oracle/policy space response oracle (DO/PSRO) is widely used, which can potentially bring better performance to GTML with sound theoretical properties but is largely ignored in machine learning. We also observe that GDA is a special case of DO/PSRO. Therefore, in this work, we provide a comprehensive benchmark of game-theoretic methods to GTML. Specifically, we implement the DO/PSRO method for GAN, AT, DRO and RARL, and then conduct experiments to investigate the following questions: i) Whether DO/PSRO is needed? and ii) Which meta-solver is the best? Results show that DO/PSRO is helpful in many experiments, e.g., AT and RARL, and Nash Equilibrium (NE) as meta-solver is consistently better than the uniform distribution. We expect that game-theoretic methods cannot only achieve better performance, but also provide a framework to deepen our understanding of intrinsic properties of GTML. We hope this work can bridge the research in machine learning and game theory. The code is released at \url{https://github.com/rainwangphy/gtml}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ehPXtrsEpuE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/rainwangphy/gtml" target="_blank" rel="nofollow noreferrer">https://github.com/rainwangphy/gtml</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="N7-AW47wXEs" data-number="253">
        <h4>
          <a href="/forum?id=N7-AW47wXEs">
              PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning
          </a>


            <a href="/pdf?id=N7-AW47wXEs" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eric_Wonhee_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Wonhee_Lee1">Eric Wonhee Lee</a>, <a href="/profile?id=~Joyce_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joyce_Ho1">Joyce Ho</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#N7-AW47wXEs-details-678" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N7-AW47wXEs-details-678"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Pubmed Graph Benchmark, Heterogeneous Information Network</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">PGB is a new heterogeneous graph benchmark dataset for biomedical literature.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been a rapid growth in biomedical literature, yet analyzing the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English biomedical articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. This benchmark can construct a huge bibliographic network which is a heterogeneous network that can be used in many tasks. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified source and make the benchmark to be publicly available for any future works.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=N7-AW47wXEs&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6406776#.YqtNCHbMKUl" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6406776#.YqtNCHbMKUl</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The PGB dataset is publicly available.
        You can log into zenodo and download the dataset.
        https://zenodo.org/record/6406776#.YqtNCHbMKUl</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">PGB dataset is released under the CC BY-NC 4.0 license and for non-commercial use.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="TzNuIdrHoU" data-number="252">
        <h4>
          <a href="/forum?id=TzNuIdrHoU">
              Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds
          </a>


            <a href="/pdf?id=TzNuIdrHoU" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Joshua_Albrecht1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_Albrecht1">Joshua Albrecht</a>, <a href="/profile?id=~Kanjun_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kanjun_Qiu1">Kanjun Qiu</a>, <a href="/profile?id=~Abraham_J_Fetterman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abraham_J_Fetterman1">Abraham J Fetterman</a>, <a href="/profile?email=bryden%40generallyintelligent.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="bryden@generallyintelligent.ai">Bryden Fogelman</a>, <a href="/profile?id=~Ellie_Kitanidis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ellie_Kitanidis1">Ellie Kitanidis</a>, <a href="/profile?email=bawr%40generallyintelligent.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="bawr@generallyintelligent.ai">Bartosz Wróblewski</a>, <a href="/profile?email=nicole%40generallyintelligent.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="nicole@generallyintelligent.ai">Nicole Seo</a>, <a href="/profile?email=mjr%40generallyintelligent.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="mjr@generallyintelligent.ai">Michael Rosenthal</a>, <a href="/profile?email=maksis%40generallyintelligent.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="maksis@generallyintelligent.ai">Maksis Knutins</a>, <a href="/profile?id=~Zachary_Polizzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Polizzi1">Zachary Polizzi</a>, <a href="/profile?id=~James_B_Simon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_B_Simon1">James B Simon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#TzNuIdrHoU-details-799" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TzNuIdrHoU-details-799"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, benchmark, generalization, simulator, embodied agents, virtual reality</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Avalon is a benchmark for generalization in RL where all individual tasks are constructed via finely controlled procedural generation of environments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Despite impressive successes, deep reinforcement learning (RL) systems still fall short of human performance on generalization to new tasks that differ from their training. As a benchmark tailored for studying RL generalization, we introduce Avalon, a set of tasks in which embodied agents in a highly diverse procedural 3D world must survive by navigating terrain, hunting food, and avoiding predators. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment in each world. Its twenty tasks, ranging in complexity from eat and throw to hunt and survive, each present worlds in which the agent must perform the stated skill in order to obtain food. This fact, in tandem with Avalon's high customizability, make possible a diversity of tests of agents' generalization to unseen tasks and environments. Avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. We find that RL baselines make progress on most tasks but are still far from human performance, suggesting Avalon is challenging enough to enable progress in the quest for generalizing RL.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=TzNuIdrHoU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">See official note for the code and dataset URLs.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">See official note for the code and dataset URLs.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">All of our data in our dataset, modifications to the Godot game engine, and other code, training scripts, and other assets will be released on or before Dec 1st (prior to the main NeurIPS conference.)

        The reason for delaying release (from the date of submission) is that, by the nature of being a benchmark built on top of a complex simulator, it is important to release a polished, easy-to-use main version from which people can build. This later release date will provide time to incorporate feedback and additional testing to create the most useful version of our benchmark environment.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our human playback dataset will be released under the CC BY-SA 4.0 license: https://creativecommons.org/licenses/by-sa/4.0/
        Our modifications to the Godoot game engine will be released under the MIT license: https://opensource.org/licenses/MIT
        All other code, training scripts, and other assets will be released under the GPLv3 license: https://opensource.org/licenses/GPL-3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fg8Z-Lj6aKh" data-number="251">
        <h4>
          <a href="/forum?id=fg8Z-Lj6aKh">
              Stance in Replies and Quotes (SRQ): A New Dataset For Learning Stance in Spanish Twitter Conversations
          </a>


            <a href="/pdf?id=fg8Z-Lj6aKh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ramon_Villa-Cox1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ramon_Villa-Cox1">Ramon Villa-Cox</a>, <a href="/profile?email=epelaez%40espol.edu.ec" class="profile-link" data-toggle="tooltip" data-placement="top" title="epelaez@espol.edu.ec">Enrique Pelaez</a>, <a href="/profile?email=rmena%40espol.edu.ec" class="profile-link" data-toggle="tooltip" data-placement="top" title="rmena@espol.edu.ec">Roberto Mena</a>, <a href="/profile?id=~Kathleen_M._Carley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kathleen_M._Carley1">Kathleen M. Carley</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#fg8Z-Lj6aKh-details-873" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fg8Z-Lj6aKh-details-873"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">conversation stance detection, rumor stance detection, target stance detection, Spanish Twitter conversations</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this work we construct an annotated dataset for the joint exploration of rumors and polarization during contentious events.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automated extraction of stances from conversations on social media is essential to advance opinion mining research. Recently, there is a renewed excitement in the field given its utility as an auxiliary subtask for rumor and controversy detection. However, most annotated datasets are built around defined rumors which limits their utility for controversy detection. Moreover, most of these resources are only available in English, which has limited the exploration of rumors and polarization in non-English speaking regions. In this work, we hand-label a dataset to begin to address these limitations. We focus on Spanish Twitter conversations around the riots that shocked the South American Region at the end of 2019. We adapted a sampling methodology that focuses on contentious conversations and annotated a sample of 7.4 thousand target-response pairs. This is one of the largest datasets for this task and the first available in Spanish. In addition, we separate the task of detecting stance in conversations in two parts: first we identify agreements, disagreements, or neutral responses; and then we identify whether non-neutral responses are supporting (denying) the veracity of their target. We believe that this unified treatment can help bridge the two related areas of rumor and controversy detection.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fg8Z-Lj6aKh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1VjQQtiscdM1scKXQdn7VfotRY--povcl?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1VjQQtiscdM1scKXQdn7VfotRY--povcl?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The data is currently held in a temporary repository made available through the following link during the review process: https://drive.google.com/drive/folders/1VjQQtiscdM1scKXQdn7VfotRY--povcl?usp=sharing
        It will be made publicly available through Zenodo before the camera-ready version of the paper is released. The data repository is comprised of 5 different tables:
          - "dataset\_schema.xlsx": Contains the schema of all tables, describes the different fields they include, and the values taken by each field.
          - "1-Final\_Labels.csv": Contains the final labels for all consolidated questions and for all annotated pairs.
          - "2-Anonymized\_Text.csv": Anonymized text for annotated pairs (both for targets and responses).
          - "3-Annotations\_long\_format.csv": Results for all annotations in long format (one row for each annotation and question)
          - "4-Conversation\_edgelists.csv": Tweet IDs for all collected tweets in the conversation trees of the annotated targets.
          - "Spanish_Annotation_Manual.pdf": Copy of the annotation manual provided to the annotators.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License: CC BY-NC.
        We opt for this type of license to adhere to Twitter's fair usage policy given the inclusion of anonymized text data with the annotated labels.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-ZcoZhU0CJ" data-number="250">
        <h4>
          <a href="/forum?id=-ZcoZhU0CJ">
              Offline Equilibrium Finding
          </a>


            <a href="/pdf?id=-ZcoZhU0CJ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shuxin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuxin_Li1">Shuxin Li</a>, <a href="/profile?id=~Xinrun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinrun_Wang1">Xinrun Wang</a>, <a href="/profile?id=~Jakub_Cerny1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakub_Cerny1">Jakub Cerny</a>, <a href="/profile?id=~Youzhi_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youzhi_Zhang2">Youzhi Zhang</a>, <a href="/profile?id=~Hau_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hau_Chan1">Hau Chan</a>, <a href="/profile?id=~Bo_An2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_An2">Bo An</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#-ZcoZhU0CJ-details-233" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-ZcoZhU0CJ-details-233"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Game Theory, Offline Reinforcement Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide mutiple datasets and a benchmark method for offline equilibrium finding.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Offline reinforcement learning (Offline RL) is an emerging field that has recently begun gaining attention across various application domains due to its ability to learn behavior from earlier collected datasets. Using logged data is imperative when further interaction with the environment is expensive (computationally or otherwise), unsafe, or entirely unfeasible. Offline RL proved very successful, paving a path to solving previously intractable real-world problems, and we aim to generalize this paradigm to a multi-agent or multiplayer-game setting. Very little research has been done in this area, as the progress is hindered by the lack of standardized datasets and meaningful benchmarks. In this work, we coin the term \textit{offline equilibrium finding} (OEF) to describe this area and construct multiple datasets consisting of strategies collected across a wide range of games using several established methods. We also propose a benchmark method -- an amalgamation of a behavior-cloning and a model-based algorithm. Our two model-based algorithms -- OEF-PSRO and OEF-CFR -- are adaptations of the widely-used equilibrium finding algorithms Deep CFR and PSRO in the context of offline learning. In the empirical part, we evaluate the performance of the benchmark algorithms on the constructed datasets. We hope that our efforts may help to accelerate research in large-scale equilibrium finding. Datasets and code are available at \url{https://github.com/SecurityGames/oef}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-ZcoZhU0CJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/SecurityGames/oef" target="_blank" rel="nofollow noreferrer">https://github.com/SecurityGames/oef</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/folders/1Y4hnkQ8hk2b81lbMaYWu26WTEF3L75FM?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/folders/1Y4hnkQ8hk2b81lbMaYWu26WTEF3L75FM?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8lQDn9zTQlW" data-number="249">
        <h4>
          <a href="/forum?id=8lQDn9zTQlW">
              BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing
          </a>


            <a href="/pdf?id=8lQDn9zTQlW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jason_Alan_Fries1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_Alan_Fries1">Jason Alan Fries</a>, <a href="/profile?id=~Leon_Weber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leon_Weber1">Leon Weber</a>, <a href="/profile?id=~Natasha_Seelam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Natasha_Seelam1">Natasha Seelam</a>, <a href="/profile?id=~Gabriel_Altay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Altay1">Gabriel Altay</a>, <a href="/profile?id=~Debajyoti_Datta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Debajyoti_Datta1">Debajyoti Datta</a>, <a href="/profile?id=~Samuele_Garda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuele_Garda1">Samuele Garda</a>, <a href="/profile?id=~Myungsun_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Myungsun_Kang1">Myungsun Kang</a>, <a href="/profile?id=~Ruisi_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruisi_Su1">Ruisi Su</a>, <a href="/profile?id=~Wojciech_Kusa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Kusa1">Wojciech Kusa</a>, <a href="/profile?id=~Samuel_Cahyawijaya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_Cahyawijaya1">Samuel Cahyawijaya</a>, <a href="/profile?id=~Fabio_Barth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabio_Barth1">Fabio Barth</a>, <a href="/profile?id=~Simon_Ott1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Ott1">Simon Ott</a>, <a href="/profile?id=~Matthias_Samwald1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Samwald1">Matthias Samwald</a>, <a href="/profile?id=~Stephen_Bach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Bach1">Stephen Bach</a>, <a href="/profile?id=~Stella_Biderman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stella_Biderman1">Stella Biderman</a>, <a href="/profile?id=~Mario_S%C3%A4nger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mario_Sänger1">Mario Sänger</a>, <a href="/profile?id=~Bo_Wang18" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Wang18">Bo Wang</a>, <a href="/profile?id=~Alison_Callahan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alison_Callahan1">Alison Callahan</a>, <a href="/profile?id=~Daniel_Le%C3%B3n_Peri%C3%B1%C3%A1n1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_León_Periñán1">Daniel León Periñán</a>, <a href="/profile?id=~Th%C3%A9o_Gigant1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Théo_Gigant1">Théo Gigant</a>, <a href="/profile?id=~Patrick_Haller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Haller2">Patrick Haller</a>, <a href="/profile?id=~Jenny_Chim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jenny_Chim1">Jenny Chim</a>, <a href="/profile?id=~Jose_David_Posada1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jose_David_Posada1">Jose David Posada</a>, <a href="/profile?id=~John_Michael_Giorgi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Michael_Giorgi1">John Michael Giorgi</a>, <a href="/profile?id=~Karthik_Rangasai_Sivaraman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karthik_Rangasai_Sivaraman1">Karthik Rangasai Sivaraman</a>, <a href="/profile?id=~Marc_P%C3%A0mies1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Pàmies1">Marc Pàmies</a>, <a href="/profile?id=~Marianna_Nezhurina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marianna_Nezhurina1">Marianna Nezhurina</a>, <a href="/profile?id=~Robert_Martin3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Martin3">Robert Martin</a>, <a href="/profile?id=~Michael_Cullan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Cullan1">Michael Cullan</a>, <a href="/profile?id=~Moritz_Freidank1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moritz_Freidank1">Moritz Freidank</a>, <a href="/profile?id=~Nathan_Dahlberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nathan_Dahlberg1">Nathan Dahlberg</a>, <a href="/profile?id=~Shubhanshu_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shubhanshu_Mishra1">Shubhanshu Mishra</a>, <a href="/profile?id=~Shamik_Bose1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shamik_Bose1">Shamik Bose</a>, <a href="/profile?id=~Nicholas_Michio_Broad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Michio_Broad1">Nicholas Michio Broad</a>, <a href="/profile?id=~Yanis_Labrak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanis_Labrak1">Yanis Labrak</a>, <a href="/profile?id=~Shlok_S_Deshmukh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shlok_S_Deshmukh1">Shlok S Deshmukh</a>, <a href="/profile?id=~Sid_Kiblawi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sid_Kiblawi1">Sid Kiblawi</a>, <a href="/profile?id=~Ayush_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ayush_Singh1">Ayush Singh</a>, <a href="/profile?email=vumichien1692%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vumichien1692@gmail.com">Minh Chien Vu</a>, <a href="/profile?id=~Trishala_Neeraj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trishala_Neeraj1">Trishala Neeraj</a>, <a href="/profile?id=~Jonas_Golde1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonas_Golde1">Jonas Golde</a>, <a href="/profile?id=~Albert_Villanova_del_Moral1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Villanova_del_Moral1">Albert Villanova del Moral</a>, <a href="/profile?id=~Benjamin_Beilharz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Beilharz1">Benjamin Beilharz</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#8lQDn9zTQlW-details-906" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8lQDn9zTQlW-details-906"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">biomedical, natural language processing, data-centric, language modeling</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">BigBIO is a community library of 130+ biomedical NLP datasets, covering 12 tasks and 10 languages. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Training and evaluating language models increasingly requires the construction of \emph{meta-datasets} -- diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a diversity of novel pretraining tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce \textsc{BigBIO} a community library of 130+ biomedical NLP datasets, currently covering 12 task categories and 10+ languages. \textsc{BigBIO} facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. \textsc{BigBIO} is an ongoing community effort and is available at \href{https://github.com/bigscience-workshop/biomedical}{this URL}. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=8lQDn9zTQlW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/bigscience-workshop/biomedical" target="_blank" rel="nofollow noreferrer">https://github.com/bigscience-workshop/biomedical</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Instructions for installing and downloading BigBIO are available on our project GitHub page: https://github.com/bigscience-workshop/biomedical</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="r6XAESGTioL" data-number="248">
        <h4>
          <a href="/forum?id=r6XAESGTioL">
              Sound-Dr Dataset and Baseline System for Detecting Respiratory Anomaly
          </a>


            <a href="/pdf?id=r6XAESGTioL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hoang_Van_Truong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hoang_Van_Truong1">Hoang Van Truong</a>, <a href="/profile?id=~Quang_Huu_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quang_Huu_Nguyen1">Quang Huu Nguyen</a>, <a href="/profile?email=cuongnq1%40fsoft.com.vn" class="profile-link" data-toggle="tooltip" data-placement="top" title="cuongnq1@fsoft.com.vn">Nguyen Quoc Cuong</a>, <a href="/profile?email=phongnx1%40fsoft.com.vn" class="profile-link" data-toggle="tooltip" data-placement="top" title="phongnx1@fsoft.com.vn">Nguyen Xuan Phong</a>, <a href="/profile?id=~Hoang_D._Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hoang_D._Nguyen1">Hoang D. Nguyen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#r6XAESGTioL-details-896" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r6XAESGTioL-details-896"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Healthcare, COVID-19, Abnormaly, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As the burden of respiratory diseases continues to fall on society worldwide, this paper proposes a high-quality and robust dataset of human sounds for studying respiratory anomalies, including pneumonia and COVID-19. It consists of coughing, mouth breathing, and nose breathing sounds together with valuable metadata on related clinical characteristics. We also develop a proof-of-concept system for establishing baselines and benchmarking against multiple datasets, such as Coswara and CoughVid. Our comprehensive experiments show that the Sound-Dr dataset has richer features, better performance, and is more robust to dataset shifts. It is promising for real-time trials on mobile devices. The proposed dataset and system will serve as effective tools to assist physicians in diagnosing respiratory disorders.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rLV2kJW2BTL" data-number="247">
        <h4>
          <a href="/forum?id=rLV2kJW2BTL">
              FGAVE: A Dataset for Fine-Grained Audio-Visual Event Localization
          </a>


            <a href="/pdf?id=rLV2kJW2BTL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=zhaoyaqian%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaoyaqian@inspur.com">Yaqian Zhao</a>, <a href="/profile?id=~Lu_Liu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Liu13">Lu Liu</a>, <a href="/profile?id=~Baoyu_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baoyu_Fan1">Baoyu Fan</a>, <a href="/profile?email=jinliang%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jinliang@inspur.com">Liang Jin</a>, <a href="/profile?id=~Li_Wang20" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Wang20">Li Wang</a>, <a href="/profile?id=~Xiaochuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaochuan_Li1">Xiaochuan Li</a>, <a href="/profile?email=guozhenhua%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guozhenhua@inspur.com">Zhenhua Guo</a>, <a href="/profile?email=zhaokunbj%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaokunbj@inspur.com">Kun Zhao</a>, <a href="/profile?email=lirengang.hsslab%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lirengang.hsslab@gmail.com">Rengang Li</a>, <a href="/profile?email=gongwf%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gongwf@inspur.com">Weifeng Gong</a>, <a href="/profile?email=wangendong.hsslab%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangendong.hsslab@gmail.com">Endong Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#rLV2kJW2BTL-details-492" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rLV2kJW2BTL-details-492"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We develop a dataset for fine-grained audio-visual event localization (AVEL), named FGAVE. It contains semantic information to promote the AVEL task of matching audios in visual frames with similar scenes.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Audio-visual event localization (AVEL) task aims to localize the sound events in corresponding visual segments temporally, better performance on an AVEL task could facilitate video understanding in cross-modality realm. In reality, the very minor change of continuous visual/auditory scenes with different labels makes the temporal boundaries indistinguishable. For example, it’s difficult to find flute sound from the video segments of the concert scene, as shown in Fig. 1(a), and vice versa in Fig. 1(b). The cause of this problem is lacking semantic information in two modalities. In this paper, we propose a fine-grained AVE dataset, FGAVE, to promote researches on AVEL task in the realistic realm. FGAVE consists of a series of autonomous sensory meridian response (ASMR) videos with a relatively still visual background. Through a systematic annotation and filter procedure, our FGAVE dataset consists of 14,464 videos with a fixed length of 10 seconds, covering a wide variety of sound sources triggered by different actions with different material. In benchmark tasks, we perform several experiments on the two sub-tasks to evaluate our fine-grained AVE dataset and demonstrate the challenges and usefulness of semantic annotations in AVEL problem. The code and data are released at https://github.com/inspur-hsslab/NeurIPS-Dataset-FGAVE.git under the license of CC BY-NC-SA 4.0 for academic and non-commercial usage.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rLV2kJW2BTL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/inspur-hsslab/NeurIPS-Dataset-FGAVE.git" target="_blank" rel="nofollow noreferrer">https://github.com/inspur-hsslab/NeurIPS-Dataset-FGAVE.git</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/inspur-hsslab/NeurIPS-Dataset-FGAVE.git" target="_blank" rel="nofollow noreferrer">https://github.com/inspur-hsslab/NeurIPS-Dataset-FGAVE.git</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">FGAVE: A Dataset for Fine-Grained Audio-Visual Event Localization
        I agree
        1) Use the videos and images for research purposes only;
        2) Do not redistribute videos and images to any third party;
        3) If videos and images are reproduced in electronic or print media, use only in scientific journals and include notice of copyright
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="7w-a8PYPlP" data-number="246">
        <h4>
          <a href="/forum?id=7w-a8PYPlP">
              OpenFWI: Large-scale Multi-structural Benchmark Datasets for Full Waveform Inversion
          </a>


            <a href="/pdf?id=7w-a8PYPlP" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chengyuan_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengyuan_Deng1">Chengyuan Deng</a>, <a href="/profile?id=~Shihang_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shihang_Feng1">Shihang Feng</a>, <a href="/profile?id=~Hanchen_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanchen_Wang3">Hanchen Wang</a>, <a href="/profile?id=~Xitong_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xitong_Zhang1">Xitong Zhang</a>, <a href="/profile?id=~Peng_Jin6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peng_Jin6">Peng Jin</a>, <a href="/profile?id=~Yinan_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinan_Feng1">Yinan Feng</a>, <a href="/profile?id=~Qili_Zeng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qili_Zeng1">Qili Zeng</a>, <a href="/profile?id=~Yinpeng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinpeng_Chen1">Yinpeng Chen</a>, <a href="/profile?id=~Youzuo_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youzuo_Lin1">Youzuo Lin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">9 Replies</span>


        </div>

          <a href="#7w-a8PYPlP-details-514" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7w-a8PYPlP-details-514"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Seismic Full Waveform Inversion, Data-driven Approach</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present an open-source platform for Full Waveform Inversion with twelve datasets and benchmarks on four deep learning methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of $12$ datasets ($2.1$TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO$_2$ reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contain various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. In addition to evaluations on a single dataset, OpenFWI enables the study of generalization across datasets. Our study uncovers that the deep learning methods generalize poorly across domains, and the degradation connects to the complexity of subsurface structures. We hope OpenFWI facilitates diversified, rigorous, and reproducible research in the geophysics and machine learning community. All datasets and related information can be accessed through our website at https://openfwi-lanl.github.io/</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=7w-a8PYPlP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://openfwi-lanl.github.io/" target="_blank" rel="nofollow noreferrer">https://openfwi-lanl.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://openfwi-lanl.github.io/docs/data.html" target="_blank" rel="nofollow noreferrer">https://openfwi-lanl.github.io/docs/data.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License for codes: GNU General Public License v3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NdKY43ng7mw" data-number="245">
        <h4>
          <a href="/forum?id=NdKY43ng7mw">
              XDailyDialog: A Multilingual Parallel Corpus for Chit-chat
          </a>


            <a href="/pdf?id=NdKY43ng7mw" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zeming_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeming_Liu1">Zeming Liu</a>, <a href="/profile?id=~Ping_Nie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Nie1">Ping Nie</a>, <a href="/profile?id=~Jie_Cai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Cai2">Jie Cai</a>, <a href="/profile?id=~PENG_ZHANG27" class="profile-link" data-toggle="tooltip" data-placement="top" title="~PENG_ZHANG27">PENG ZHANG</a>, <a href="/profile?id=~Yuyu_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuyu_Zhang1">Yuyu Zhang</a>, <a href="/profile?id=~Mrinmaya_Sachan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mrinmaya_Sachan3">Mrinmaya Sachan</a>, <a href="/profile?id=~KAIPING_PENG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~KAIPING_PENG1">KAIPING PENG</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#NdKY43ng7mw-details-441" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NdKY43ng7mw-details-441"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">monolingual chit-chat, multilingual chit-chat, cross-lingual chit-chat, XDailyDialog, kNN-Chat.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this paper, we provide a multilingual parallel chit-chat dataset, XDailyDialog, to enable researchers to explore the challenging task of multilingual and cross-lingual Chit-chat. The difference between XDailyDialog and existing chit-chat datasets is that the conversation in XDailyDialog is annotated in multilingual and is parallel, while other datasets are restricted to a single language or are not parallel. We collect 13K dialogs aligned across 4 languages (52K dialogues and 410K utterances in total). We then propose a conversation generation framework, kNN-Chat, with a novel kNN-search mechanism that can support unified response retrieval for monolingual, multilingual, and cross-lingual chit-chat generation. Experiment results show that the use of multilingual data can bring performance improvement for monolingual chit-chat, indicating the benefits of  XDailyDialog. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual chit-chat.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NdKY43ng7mw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/liuzeming01/XDailyDialog" target="_blank" rel="nofollow noreferrer">https://github.com/liuzeming01/XDailyDialog</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/liuzeming01/XDailyDialog" target="_blank" rel="nofollow noreferrer">https://github.com/liuzeming01/XDailyDialog</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="fJbeXt24oGO" data-number="244">
        <h4>
          <a href="/forum?id=fJbeXt24oGO">
              The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications
          </a>


            <a href="/pdf?id=fJbeXt24oGO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mirac_Suzgun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mirac_Suzgun1">Mirac Suzgun</a>, <a href="/profile?id=~Luke_Melas-Kyriazi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Melas-Kyriazi1">Luke Melas-Kyriazi</a>, <a href="/profile?id=~Suproteem_K_Sarkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suproteem_K_Sarkar1">Suproteem K Sarkar</a>, <a href="/profile?id=~Scott_Kominers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Kominers1">Scott Kominers</a>, <a href="/profile?id=~Stuart_Shieber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stuart_Shieber1">Stuart Shieber</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#fJbeXt24oGO-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fJbeXt24oGO-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">hupd, patents, innovation, dataset, uspto, intellectual-property, nlp, classification, summarization, language-modeling, patent-analysis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a corpus of English-language patent applications and accompanying structured filing metadata; we demonstrate how our dataset can be used to study patterns in innovation and technological advancement across different domains and over time. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Though the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, ML offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets in NLP, it contains the inventor-submitted versions of patent applications—not the final versions of granted patents—thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured metadata alongside the text of patent filings: By providing each application’s metadata along with all of its text fields, the dataset enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community—namely, binary classification of patent decisions. We additionally show the structured metadata provided in the dataset enables us to conduct explicit studies of concept shifts for this task. Finally, we demonstrate how our dataset can be used for three additional tasks: Multi-class classification of patent subject areas, language modeling, and summarization. Overall, HUPD is one of the largest multi-purpose NLP datasets containing domain-specific textual data, along with well-structured bibliographic metadata, and aims to advance research extending language and classification models to diverse and dynamic real-world data distributions.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=fJbeXt24oGO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://patentdataset.org/" target="_blank" rel="nofollow noreferrer">https://patentdataset.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">* Website: https://patentdataset.org/

        * GitHub: https://github.com/suzgunmirac/hupd

        * Hugging Face Datasets: https://huggingface.co/datasets/HUPD/hupd</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released under the Creative Commons Attribution 4.0 International License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="OxFoLTKDcNm" data-number="243">
        <h4>
          <a href="/forum?id=OxFoLTKDcNm">
              Communicating Natural Programs to Humans and Machines
          </a>


            <a href="/pdf?id=OxFoLTKDcNm" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sam_Acquaviva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sam_Acquaviva1">Sam Acquaviva</a>, <a href="/profile?id=~Yewen_Pu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yewen_Pu1">Yewen Pu</a>, <a href="/profile?id=~Marta_Kryven1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marta_Kryven1">Marta Kryven</a>, <a href="/profile?id=~Theodoros_Sechopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Theodoros_Sechopoulos1">Theodoros Sechopoulos</a>, <a href="/profile?id=~Catherine_Wong3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Catherine_Wong3">Catherine Wong</a>, <a href="/profile?id=~Gabrielle_Ecanow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabrielle_Ecanow1">Gabrielle Ecanow</a>, <a href="/profile?id=~Maxwell_Nye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxwell_Nye1">Maxwell Nye</a>, <a href="/profile?id=~Michael_Henry_Tessler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Henry_Tessler1">Michael Henry Tessler</a>, <a href="/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#OxFoLTKDcNm-details-880" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OxFoLTKDcNm-details-880"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We collect a dataset called LARC, consisting of natural language instructions, used by end-users to instruct each-other how to solve the ARC (a notoriously difficult dataset for AI and program synthesis) tasks</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of \emph{language}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the \textit{Language-complete ARC}: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=OxFoLTKDcNm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/samacqua/LARC" target="_blank" rel="nofollow noreferrer">https://github.com/samacqua/LARC</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/samacqua/LARC" target="_blank" rel="nofollow noreferrer">https://github.com/samacqua/LARC</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset https://github.com/samacqua/LARC/tree/main/dataset is licensed under the Creative Commons Attribution 4.0 International License
        view at : http://creativecommons.org/licenses/by/4.0/

        All supporting code follows the MIT License
        view at : https://opensource.org/licenses/MIT

        Copyright (c) 2021 Sam Acquaviva and other contributors

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NQ_yUkEHTlg" data-number="242">
        <h4>
          <a href="/forum?id=NQ_yUkEHTlg">
              Benchmarking Safe Policy Optimization for Constrained Reinforcement Learning
          </a>


            <a href="/pdf?id=NQ_yUkEHTlg" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jiamg_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiamg_Ji1">Jiamg Ji</a>, <a href="/profile?id=~Long_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Yang4">Long Yang</a>, <a href="/profile?id=~Shangding_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shangding_Gu1">Shangding Gu</a>, <a href="/profile?id=~Yuanpei_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanpei_Chen2">Yuanpei Chen</a>, <a href="/profile?id=~Zhouchen_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhouchen_Lin1">Zhouchen Lin</a>, <a href="/profile?id=~Yaodong_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaodong_Yang1">Yaodong Yang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#NQ_yUkEHTlg-details-208" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NQ_yUkEHTlg-details-208"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Safe Policy Optimization, Constrained Reinforcement Learning, Constrained MDP</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a solid safe RL benchmark along with new testing environments.  </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Safe reinforcement learning (safe RL) tackles decision making problems with safety constraints. Despite the influx of attention in this field, there is a lack of commonly recognized safe RL benchmark. This is partly because the code of many safe RL methods is unavailable and also because new methods often come with new testing tasks.  As a result, researchers suffer from incorrect implementations, unfair comparisons, and misleading conclusions.  In this study, we offer a solid safe RL benchmark---(\texttt{SafePO})---which benchmarks popular safe policy learning algorithms across a list of common environments. Specifically, we start by standardizing the problem of safe exploration via solving constrained Markov decision processes (CMDP). Then, we provide implementations for CMDP solutions, covering both constrained policy optimization type methods and Lagrangian type methods.  Our implementations in SafePO are highly efficient in the sense that learners can collect samples in parallel and synchronize their policy gradients on different physical CPU cores. We test them on four types of safety-aware robot learning tasks. Based on the benchmark results, we derive new insights by disclosing the interplay of different attributes on safety performance and illustrating the difficulty of safety learning on the sparse cost. Furthermore, to consider the safe RL problem in multi-agent settings, we introduce new tasks based on the DexterousHands environment and report comparison results for both single-agent and multi-agent safe RL algorithms. Our project is released at: \url{https://github.com/PKU-MARL/Safe-Policy-Optimization}.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NQ_yUkEHTlg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/PKU-MARL/Safe-Policy-Optimization" target="_blank" rel="nofollow noreferrer">https://github.com/PKU-MARL/Safe-Policy-Optimization</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/view/safepo-benchmark" target="_blank" rel="nofollow noreferrer">https://sites.google.com/view/safepo-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">This study is not the case.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="56gbQGXlnYv" data-number="241">
        <h4>
          <a href="/forum?id=56gbQGXlnYv">
              Ontologue: Declarative Benchmark Construction for Ontological Multi-Label Classification
          </a>


            <a href="/pdf?id=56gbQGXlnYv" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sean_T._Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sean_T._Yang1">Sean T. Yang</a>, <a href="/profile?id=~Bernease_Herman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernease_Herman1">Bernease Herman</a>, <a href="/profile?id=~Bill_Howe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bill_Howe1">Bill Howe</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#56gbQGXlnYv-details-557" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="56gbQGXlnYv-details-557"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Hierarchical MultiLabel Classification, Declarative Query Toolkit, Customized Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Ontologue is a toolkit for ontological multi-label classification dataset construction from DBPedia. This toolkit allows users to control contextual, distributional, and structured properties and create customized datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We describe a customizable benchmark for hierarchical and ontological multi-label classification, a task where labels are equipped with a graph structure and data items can be assigned multiple labels.  We find that current benchmarks do not adequately represent the problem space, casting doubt on the generalizability of current results. We consider three dimensions of the problem space: context (availability of rich features on the data and labels), distribution of labels over data, and graph structure. For context, the lack of complex features on the labels (and in some cases, the data) artificially prevent the use of modern representation learning techniques as an appropriate baseline.  For distribution, we find the long tail of labels over data constitute a few-shot learning problem that artificially confounds the results: for most common benchmarks, over 40% of the labels have fewer than 5 data points in the training set.  For structure, we find that the correlation between performance and the height of the tree can explain some of the variation in performance, informing practical utility. In this paper, we demonstrate how the lack of diversity in benchmarks can confound performance analysis, then present a declarative query system called Ontologue for generating custom benchmarks with specific properties, then use this system to design 4 new benchmarks extracted from DBPedia that better represent the problem space. We evaluate state-of-the-art algorithms on both existing and new benchmarks and show that the performance conclusions can vary significantly depending on the dimensions we consider.  We intend the system and derived benchmarks to improve the analysis of generalizability for these problems.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=56gbQGXlnYv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/seanyang38/Ontologue" target="_blank" rel="nofollow noreferrer">https://github.com/seanyang38/Ontologue</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The datasets and the code will be available on https://github.com/seanyang38/Ontologue</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="CP3HZztueqy" data-number="239">
        <h4>
          <a href="/forum?id=CP3HZztueqy">
              Towards an Ideal Methodological Data Repository: Lessons and Recommendations
          </a>


            <a href="/pdf?id=CP3HZztueqy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rachel_Longjohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rachel_Longjohn1">Rachel Longjohn</a>, <a href="/profile?id=~Markelle_Kelly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markelle_Kelly1">Markelle Kelly</a>, <a href="/profile?id=~Sameer_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sameer_Singh1">Sameer Singh</a>, <a href="/profile?id=~Padhraic_Smyth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Padhraic_Smyth1">Padhraic Smyth</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#CP3HZztueqy-details-525" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CP3HZztueqy-details-525"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Data Repository, Data Curation, Benchmarking, Reproducible Research</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We analyzed over 150 data repositories' metadata, quality assurance, citation/tracking, and model evaluation practices to better understand how they can better support ML research.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Online data repositories have become a staple of modern research, enabling collection of domain-specific knowledge, replication of published research results, and easy access to research data. Machine learning researchers, for example, often use online data repositories to find datasets for training models and evaluating new methods. Although prior work establishes guidelines for data repositories, relatively little is known about how widely these standards have been adopted in practice. Further, these various guidelines have not been surveyed holistically, and do not include many considerations important for machine learning, such as data validation and model evaluation. In this paper, we present a comprehensive survey of over 150 data repositories, characterizing their metadata documentation and standardization, data curation and validation, and tracking of dataset use in the literature. We synthesize our findings about the current state of data repositories and adherence to existing standards, suggesting best practices for data repositories in general and for machine learning data repositories in particular.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=CP3HZztueqy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Data is available here: https://doi.org/10.5281/zenodo.6645277. See Supplementary Material for more information.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All data are available under a Creative Commons Attribution 4.0 International Public License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mlJhMokkavL" data-number="238">
        <h4>
          <a href="/forum?id=mlJhMokkavL">
              MVIndEmo: A Dataset for Micro-video Public Induced Emotion Prediction on Social Media
          </a>


            <a href="/pdf?id=mlJhMokkavL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=guozhenhua%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guozhenhua@inspur.com">Zhenhua Guo</a>, <a href="/profile?id=~Qi_Jia4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Jia4">Qi Jia</a>, <a href="/profile?id=~Baoyu_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baoyu_Fan1">Baoyu Fan</a>, <a href="/profile?email=wangdi11%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangdi11@inspur.com">Di Wang</a>, <a href="/profile?id=~Cong_Xu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cong_Xu3">Cong Xu</a>, <a href="/profile?email=wang_libj%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wang_libj@inspur.com">Li Wang</a>, <a href="/profile?email=wangyanwei%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangyanwei@inspur.com">Yanwei Wang</a>, <a href="/profile?email=zhaoyaqian%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaoyaqian@inspur.com">Yaqian Zhao</a>, <a href="/profile?email=lirengang.hsslab%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lirengang.hsslab@gmail.com">Rengang Li</a>, <a href="/profile?email=gongwf%40inspur.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gongwf@inspur.com">Weifeng Gong</a>, <a href="/profile?email=wangendong.hsslab%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangendong.hsslab@gmail.com">Endong Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mlJhMokkavL-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mlJhMokkavL-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Induced emotion is evoked in the audiences for content and has widely range of applications especially in the field of social media. With the development of communication technology and the popularization of smart terminals, Micro-video has become a prevailing trend and given a unprecedented impact to social media.  People are willing to create micro videos to express concern about social topic and to influence others. Therefore, studying the induced emotion that micro video evoked the public is indispensable. We introduce a public induced emotion prediction dataset for micro videos - MVIndEmo. The dataset is collected from Tiktok-a trending social media platform for creating and sharing micro- videos. We select eight topics that evokes social discussion on Tiktok to construct the dataset. We design an automated label generation strategy utilizing video comments for two tasks: induced emotion polarity and induced emotion classification.  We provide both binary classification and probability distribution labels to accommodate flexible benchmark settings. There are 7153 micro videos with label in our dataset. We perform statistics on our dataset to show the overall situation. We hope this dataset will drive new research directions in sentiment analysis—especially for induced emotion, and support scholars researching social media and multimodal information understanding.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/inspur-hsslab/NeurIPS-Dataset-Induced-Emotion" target="_blank" rel="nofollow noreferrer">https://github.com/inspur-hsslab/NeurIPS-Dataset-Induced-Emotion</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/inspur-hsslab/NeurIPS-Dataset-Induced-Emotion" target="_blank" rel="nofollow noreferrer">https://github.com/inspur-hsslab/NeurIPS-Dataset-Induced-Emotion</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MVIndEmo: A Dataset for Micro-video Public Induced Emotion Prediction on Social Media
        I agree
        1) Use the videos and images for research purposes only;
        2) Do not redistribute videos and images to any third party;
        3) If videos and images are reproduced in electronic or print media, use only in scientific journals and include notice of copyright</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Hs8WnQteQWj" data-number="237">
        <h4>
          <a href="/forum?id=Hs8WnQteQWj">
              DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding
          </a>


            <a href="/pdf?id=Hs8WnQteQWj" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=kajay001%40odu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kajay001@odu.edu">Kehinde Ajayi</a>, <a href="/profile?email=xwei001%40odu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xwei001@odu.edu">Xin Wei</a>, <a href="/profile?email=mgryd001%40odu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mgryd001@odu.edu">Martin Gryder</a>, <a href="/profile?email=wshie002%40odu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wshie002@odu.edu">Winston Shields</a>, <a href="/profile?id=~Jian_Wu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Wu7">Jian Wu</a>, <a href="/profile?email=smjones%40lanl.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="smjones@lanl.gov">Shawn Morgan Jones</a>, <a href="/profile?id=~Michal_Kucer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michal_Kucer1">Michal Kucer</a>, <a href="/profile?id=~Diane_Oyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diane_Oyen1">Diane Oyen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Hs8WnQteQWj-details-490" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Hs8WnQteQWj-details-490"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">technical drawing, figure understanding, big data, natural language processing, natural language understanding, image segmentation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce DeepPatent2, a new large-scale auto-tagged and auto-segmented dataset for patent figure understanding. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances in computer vision (CV) and natural language processing (NLP) have been driven by exploiting big data to explore many practical applications. However, these research fields are still limited not only by the sheer volume but also by the versatility and diversity, of the available datasets. CV tasks, such as image captioning, have primarily been carried out on natural images. Despite the efficiency of the state-of-the-art (SOTA) captioning models on natural images, they still struggle to produce accurate and meaningful captions on sketched images. In this paper, we introduce DeepPatent2, a new large-scale dataset for patent figure understanding. This dataset provides more than 2 million sketched images with 123,806 object categories extracted from US design patent documents. We demonstrate the usefulness of DeepPatent2 to the CV and NLP communities with two use cases namely, image segmentation and image captioning. We trained a baseline image captioning model on a subset of our dataset to produce captions that focus on identifying the actual object and different viewpoints of patent figures. Our model achieves METEOR and ROUGE-l scores of 62.7\% and 63.4\% on the test data. For the image segmentation task, we used a transformer-based method to segment the technical drawings in DeepPatent2, and achieved a 99.5\% accuracy on the test data. The experiments showcased the potential usefulness of our dataset to facilitate future research such as image grounding on sketched images and technical drawings.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Hs8WnQteQWj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/lamps-lab/Patent-figure-segmentor" target="_blank" rel="nofollow noreferrer">https://github.com/lamps-lab/Patent-figure-segmentor</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/lamps-lab/Patent-figure-segmentor" target="_blank" rel="nofollow noreferrer">https://github.com/lamps-lab/Patent-figure-segmentor</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8doyL0kKlu1" data-number="236">
        <h4>
          <a href="/forum?id=8doyL0kKlu1">
              Developing a Series of AI Challenges for the United States Department of the Air Force
          </a>


            <a href="/pdf?id=8doyL0kKlu1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Vijay_Gadepally1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vijay_Gadepally1">Vijay Gadepally</a>, <a href="/profile?email=gregangelides%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="gregangelides@ll.mit.edu">Gregory Angelides</a>, <a href="/profile?id=~Andrei_Barbu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrei_Barbu3">Andrei Barbu</a>, <a href="/profile?email=andrew.bowne.2%40us.af.mil" class="profile-link" data-toggle="tooltip" data-placement="top" title="andrew.bowne.2@us.af.mil">Andrew Bowne</a>, <a href="/profile?id=~Laura_Brattain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laura_Brattain1">Laura Brattain</a>, <a href="/profile?id=~Tamara_Broderick2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tamara_Broderick2">Tamara Broderick</a>, <a href="/profile?email=cabreraa%40mit.ed" class="profile-link" data-toggle="tooltip" data-placement="top" title="cabreraa@mit.ed">Armando Cabrera</a>, <a href="/profile?email=glenn.carl%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="glenn.carl@ll.mit.edu">Glenn Carl</a>, <a href="/profile?email=ronisha%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ronisha@mit.edu">Ronisha Carter</a>, <a href="/profile?email=miriam.cha%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="miriam.cha@ll.mit.edu">Miriam Cha</a>, <a href="/profile?email=emilie.cowen%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="emilie.cowen@ll.mit.edu">Emilie Cowen</a>, <a href="/profile?id=~Jesse_Cummings1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Cummings1">Jesse Cummings</a>, <a href="/profile?id=~Bill_Freeman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bill_Freeman1">Bill Freeman</a>, <a href="/profile?id=~James_R._Glass1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_R._Glass1">James R. Glass</a>, <a href="/profile?email=sgoldberg%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sgoldberg@mit.edu">Sam Goldberg</a>, <a href="/profile?id=~Mark_Hamilton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Hamilton1">Mark Hamilton</a>, <a href="/profile?id=~Thomas_Heldt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Heldt1">Thomas Heldt</a>, <a href="/profile?id=~Kuan_Wei_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuan_Wei_Huang1">Kuan Wei Huang</a>, <a href="/profile?id=~Phillip_Isola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phillip_Isola1">Phillip Isola</a>, <a href="/profile?id=~Boris_Katz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Katz1">Boris Katz</a>, <a href="/profile?email=jkoerner%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jkoerner@mit.edu">Jamie Koerner</a>, <a href="/profile?id=~Yen-Chen_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yen-Chen_Lin1">Yen-Chen Lin</a>, <a href="/profile?id=~David_Mayo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Mayo1">David Mayo</a>, <a href="/profile?id=~Kyle_McAlpin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_McAlpin1">Kyle McAlpin</a>, <a href="/profile?id=~J._Taylor_Perron1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J._Taylor_Perron1">J. Taylor Perron</a>, <a href="/profile?id=~Jean_Eugene_Piou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean_Eugene_Piou1">Jean Eugene Piou</a>, <a href="/profile?id=~Hrishikesh_M_Rao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hrishikesh_M_Rao1">Hrishikesh M Rao</a>, <a href="/profile?email=hayley%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hayley@ll.mit.edu">Hayley Reynolds</a>, <a href="/profile?email=kmsamuel%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kmsamuel@mit.edu">Kaira Samuel</a>, <a href="/profile?id=~Siddharth_Samsi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_Samsi1">Siddharth Samsi</a>, <a href="/profile?email=morgansc%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="morgansc@mit.edu">Morgan Schmidt</a>, <a href="/profile?email=leslie.shing%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="leslie.shing@ll.mit.edu">Leslie Shing</a>, <a href="/profile?email=osimek%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="osimek@ll.mit.edu">Olga Simek</a>, <a href="/profile?id=~Brandon_Swenson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brandon_Swenson1">Brandon Swenson</a>, <a href="/profile?id=~Vivienne_Sze1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vivienne_Sze1">Vivienne Sze</a>, <a href="/profile?email=jonathan.taylor%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jonathan.taylor@ll.mit.edu">Jonathan Taylor</a>, <a href="/profile?id=~Paul_Tylkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Tylkin1">Paul Tylkin</a>, <a href="/profile?email=mark.veillette%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mark.veillette@ll.mit.edu">Mark veillette</a>, <a href="/profile?email=mlweiss%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mlweiss@ll.mit.edu">Matthew Weiss</a>, <a href="/profile?id=~Allan_Wollaber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Allan_Wollaber1">Allan Wollaber</a>, <a href="/profile?email=sophia.yuditskaya%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sophia.yuditskaya@ll.mit.edu">Sophia Yuditskaya</a>, <a href="/profile?email=kepner%40ll.mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kepner@ll.mit.edu">Jeremy Kepner</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#8doyL0kKlu1-details-102" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8doyL0kKlu1-details-102"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AI Challenges, Air Force, National Security</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This article describes the development and subsequent lessons learned in developing a number of AI challenges aligned with with U.S. Federal AI priorities. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Through a series of federal initiatives and orders, the U.S. Government has been making a concerted effort to ensure American leadership in AI. These broad strategy documents have influenced organizations such as the United States Department of the Air Force (DAF). The DAF-MIT AI Accelerator is an initiative between the DAF and MIT to bridge the gap between AI researchers and DAF mission requirements. Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities. These challenges target priorities by making large, AI-ready datasets publicly available, incentivizing open-source solutions, and creating a demand signal for dual use technologies that can stimulate further research. In this article, we describe these public challenges being developed and how their application contributes to scientific advances.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=8doyL0kKlu1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Please see supplementary material for links to various datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Links to individual datasets is provided in the article and supplementary material. These links contain information on how to access the datasets and associated documentation.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">Most datasets are already publicly available. Only the data from the CogPilot challenge is undergoing PhysioNet credentialing. We expect this to be accepted and made public soon.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License information for individual datasets is included in the Supplementary Material. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="2QrFr_U782Z" data-number="235">
        <h4>
          <a href="/forum?id=2QrFr_U782Z">
              A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking
          </a>


            <a href="/pdf?id=2QrFr_U782Z" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Keyu_Duan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keyu_Duan2">Keyu Duan</a>, <a href="/profile?id=~Zirui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zirui_Liu1">Zirui Liu</a>, <a href="/profile?id=~Peihao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peihao_Wang1">Peihao Wang</a>, <a href="/profile?id=~Wenqing_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenqing_Zheng1">Wenqing Zheng</a>, <a href="/profile?id=~Kaixiong_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaixiong_Zhou1">Kaixiong Zhou</a>, <a href="/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="/profile?id=~Xia_Hu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xia_Hu4">Xia Hu</a>, <a href="/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#2QrFr_U782Z-details-473" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2QrFr_U782Z-details-473"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Convolutional Networks, Scalability, Benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a comprehensive and fair benchmark study on large-scale graph training and further propose a new layer-wise training manner the achieves new SOTA performance on large-scale graph datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efficiency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-of-the-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=2QrFr_U782Z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking" target="_blank" rel="nofollow noreferrer">https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">All datasets can be accessed through the PyTorch Geometric (https://github.com/pyg-team/pytorch_geometric), a GCN library for PyTorch.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT LICENSE</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="qiDmAaG6mP" data-number="234">
        <h4>
          <a href="/forum?id=qiDmAaG6mP">
              M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus
          </a>


            <a href="/pdf?id=qiDmAaG6mP" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lichao_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lichao_Zhang2">Lichao Zhang</a>, <a href="/profile?id=~Ruiqi_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruiqi_Li2">Ruiqi Li</a>, <a href="/profile?id=~Shoutong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shoutong_Wang1">Shoutong Wang</a>, <a href="/profile?id=~Liqun_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liqun_Deng1">Liqun Deng</a>, <a href="/profile?id=~Jinglin_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinglin_Liu1">Jinglin Liu</a>, <a href="/profile?id=~Yi_Ren2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Ren2">Yi Ren</a>, <a href="/profile?id=~Jinzheng_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinzheng_He1">Jinzheng He</a>, <a href="/profile?id=~Rongjie_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rongjie_Huang1">Rongjie Huang</a>, <a href="/profile?id=~Jieming_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jieming_Zhu2">Jieming Zhu</a>, <a href="/profile?id=~Xiao_Chen7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Chen7">Xiao Chen</a>, <a href="/profile?id=~Zhou_Zhao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhou_Zhao3">Zhou Zhao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#qiDmAaG6mP-details-845" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qiDmAaG6mP-details-845"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">singing voice corpus, singing voice synthesis, singing voice conversion, automatic music transcription</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The lack of publicly available high-quality and accurately labeled datasets has long been a major bottleneck for singing voice synthesis (SVS). To tackle this, we present M4Singer, a free-to-use Multi-style, Multi-singer Mandarin singing collection with elaborately annotated Musical scores as well as its benchmarks. Specifically, 1) we construct and release a large high-quality Chinese singing voice corpus, which is recorded by 19 professional singers, covering 704 Chinese pop songs as well as all the four SATB types (i.e.,  soprano, alto, tenor, and bass); 2) we take extensive efforts to manually compose the musical scores for each record song, which are necessary to the study of the prosody modeling for SVS. 3) To facilitate the use and demonstrate the quality of M4Singer, we conduct four different benchmark experiments: score-based SVS, controllable singing voice (CSV), singing voice conversion (SVC) and automatic music transcription (AMT).</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=qiDmAaG6mP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://m4singer.github.io/" target="_blank" rel="nofollow noreferrer">https://m4singer.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/M4Singer/M4Singer" target="_blank" rel="nofollow noreferrer">https://github.com/M4Singer/M4Singer</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="aJtVdI251Vv" data-number="233">
        <h4>
          <a href="/forum?id=aJtVdI251Vv">
              WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models
          </a>


            <a href="/pdf?id=aJtVdI251Vv" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yonatan_Bitton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonatan_Bitton1">Yonatan Bitton</a>, <a href="/profile?id=~Nitzan_Bitton_Guetta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nitzan_Bitton_Guetta1">Nitzan Bitton Guetta</a>, <a href="/profile?email=ron.yosef%40mail.huji.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="ron.yosef@mail.huji.ac.il">Ron Yosef</a>, <a href="/profile?id=~Yuval_Elovici1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuval_Elovici1">Yuval Elovici</a>, <a href="/profile?id=~Mohit_Bansal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohit_Bansal2">Mohit Bansal</a>, <a href="/profile?id=~Gabriel_Stanovsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Stanovsky1">Gabriel Stanovsky</a>, <a href="/profile?id=~Roy_Schwartz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roy_Schwartz1">Roy Schwartz</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 13 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#aJtVdI251Vv-details-366" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aJtVdI251Vv-details-366"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">vision-and-language, dynamic-benchmark, visual-associations, visual-common-sense-reasoning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce WinoGAViL: an online game to collect vision-and-language associations, used as a dynamic benchmark to evaluate state-of-the-art models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game to collect vision-and-language associations, (e.g., werewolves to a full moon), used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player has to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, aiming to allow future data collection that can be used to develop models with better association abilities.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=aJtVdI251Vv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://winogavil.github.io/" target="_blank" rel="nofollow noreferrer">https://winogavil.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://winogavil.github.io/" target="_blank" rel="nofollow noreferrer">https://winogavil.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code (https://github.com/WinoGAViL/WinoGAViL-experiments) is licensed under the MIT license https://opensource.org/
        565 licenses/MIT.
        Dataset (https://winogavil.github.io/download) is licensed under CC-BY 4.0 license https://creativecommons.
        566 org/licenses/by/4.0/legalcode.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LDlwAkFNi4B" data-number="232">
        <h4>
          <a href="/forum?id=LDlwAkFNi4B">
              Benchmarking Machine Learning Robustness in Covid-19 Genome Sequence Classification
          </a>


            <a href="/pdf?id=LDlwAkFNi4B" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sarwan_Ali1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarwan_Ali1">Sarwan Ali</a>, <a href="/profile?id=~Bikram_Sahoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bikram_Sahoo1">Bikram Sahoo</a>, <a href="/profile?id=~Aleksandr_Zelikovskiy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksandr_Zelikovskiy1">Aleksandr Zelikovskiy</a>, <a href="/profile?id=~Pin-Yu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pin-Yu_Chen1">Pin-Yu Chen</a>, <a href="/profile?id=~Murray_Patterson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Murray_Patterson1">Murray Patterson</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#LDlwAkFNi4B-details-983" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LDlwAkFNi4B-details-983"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">COVID-19, Sequence Classification, Genome Sequences, k-mers, Robustness</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce several ways to perturb SARS-CoV-2 genome sequences to mimic the error profiles of common sequencing platforms and show the robustness of different simulation methods to such adversarial attacks on the input sequences.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The rapid spread of the COVID-19 pandemic has resulted in an unprecedented amount of sequence data of the SARS-CoV-2  genome --- millions of sequences and counting.  This amount of data, while being orders of magnitude beyond the capacity of traditional approaches to understanding the diversity, dynamics, and evolution of viruses is nonetheless a rich resource for machine learning (ML)  approaches as alternatives for extracting such important information from these data.  It is of hence utmost importance to design a framework for testing and benchmarking the robustness of these ML models.

        This paper makes the first effort (to our knowledge) to benchmark the robustness of ML models by simulating biological sequences with errors. In this paper, we introduce several ways to perturb SARS-CoV-2 genome sequences to mimic the error profiles of common sequencing platforms such as Illumina and PacBio.  We show from experiments on a wide array of ML models that some simulation-based approaches are more robust (and accurate) than others for specific embedding methods
        to certain adversarial attacks to the input sequences. Our benchmarking framework may assist researchers in properly assessing different ML models and help them understand the behavior of the SARS-CoV-2 virus or avoid possible future pandemics.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LDlwAkFNi4B&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/sarwanpasha/Adversarial_attack_on_biological_sequences" target="_blank" rel="nofollow noreferrer">https://github.com/sarwanpasha/Adversarial_attack_on_biological_sequences</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The datasets for NovaSeq and PacBio are in different folders with self-contained names in the following URL:
        https://drive.google.com/drive/folders/1adtr8FImIYTqxM20wgInRqIZ8EJY4HVS?usp=sharing
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset that we used is publically available and no license is needed for that.

        Our code is written by us so no license is needed for that too</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="6Hl7XoPNAVX" data-number="231">
        <h4>
          <a href="/forum?id=6Hl7XoPNAVX">
              Ambiguous Images With Human Judgments for Robust Visual Event Classification
          </a>


            <a href="/pdf?id=6Hl7XoPNAVX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kate_Sanders1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kate_Sanders1">Kate Sanders</a>, <a href="/profile?id=~Reno_Kriz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Reno_Kriz1">Reno Kriz</a>, <a href="/profile?id=~Anqi_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anqi_Liu2">Anqi Liu</a>, <a href="/profile?id=~Benjamin_Van_Durme2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Van_Durme2">Benjamin Van Durme</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#6Hl7XoPNAVX-details-37" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Hl7XoPNAVX-details-37"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Datasets, Computer Vision, Cognitive Science, Uncertainty, Perception, Model Calibration</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a procedure for creating datasets of ambiguous images and use it to produce DAI (Dataset of Ambiguous Images), a collection of noisy images extracted from videos and corresponding human uncertainty judgments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Contemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce DAI (Dataset of Ambiguous Images), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images, and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=6Hl7XoPNAVX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset included in the supplementary material. Dataset will be publicly released upon paper acceptance.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY 4.0
        https://creativecommons.org/licenses/by/4.0/</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="tkwcU7QHZt" data-number="230">
        <h4>
          <a href="/forum?id=tkwcU7QHZt">
              CityLifeSim: A High-Fidelity Pedestrian and Vehicle Simulation with Complex Behaviors
          </a>


            <a href="/pdf?id=tkwcU7QHZt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cheng_Yao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Yao_Wang1">Cheng Yao Wang</a>, <a href="/profile?id=~Eyal_Ofek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eyal_Ofek1">Eyal Ofek</a>, <a href="/profile?id=~Daniel_McDuff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_McDuff1">Daniel McDuff</a>, <a href="/profile?id=~Oron_Nir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oron_Nir1">Oron Nir</a>, <a href="/profile?id=~Sai_Vemprala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sai_Vemprala1">Sai Vemprala</a>, <a href="/profile?id=~Ashish_Kapoor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashish_Kapoor1">Ashish Kapoor</a>, <a href="/profile?id=~Mar_Gonzalez-Franco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mar_Gonzalez-Franco1">Mar Gonzalez-Franco</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#tkwcU7QHZt-details-724" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tkwcU7QHZt-details-724"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">pedestrian simulation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">CityLifeSim is a flexible, high-fidelity simulation that allows users to define complex pedestrian scenarios</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Simulations are a powerful tool in machine learning, particularly in the case of safety critical scenarios. However, simulating complex temporal events in multi-agent scenarios with vehicles and pedestrians, such as those that exist in urban environments, is challenging.  We present CityLifeSim, a simulation for the research community that focuses on rich pedestrian behavior, such as the one that arises when different individual personalities, environmental events, and group goals are simulated. In our datasets we can see cases of people jay walking a red light, sitting on a bench, waiting for the bus, or calling on the phone, but also more complex creation and management of crowds that might even line up or just keep moving while observing interpersonal distances.  CityLifeSim is configurable and can create unlimited scenarios with detailed logging capabilities. As a demonstration we have run CityLifeSim to create a demo dataset that includes 17 different cameras, views from a moving vehicle in the street under different weather conditions (rain, snow, sun), and from a drone with frontal and downward views. All content with the corresponding original configuration files, ground truth pedestrian segmentation, and RGB-D frames. We evaluate our dataset on a pedestrian segmentation and identification task with state of the art Multi-Object Tracker (MOT), showing the limitations and opportunities for synthetic data in this use case. We also release an executable to configure new simulations on-demand. Our simulation, code and the dataset are available on our project page - CityLifeSim.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=tkwcU7QHZt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://citylifesim.github.io/" target="_blank" rel="nofollow noreferrer">https://citylifesim.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://citylifesim.github.io/" target="_blank" rel="nofollow noreferrer">https://citylifesim.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">No dataset embargo.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="AI9pujYWf7" data-number="229">
        <h4>
          <a href="/forum?id=AI9pujYWf7">
              Common Corruption Robustness of Point Cloud Detectors: Benchmark and Enhancement
          </a>


            <a href="/pdf?id=AI9pujYWf7" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shuangzhi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuangzhi_Li1">Shuangzhi Li</a>, <a href="/profile?id=~Zhijie_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhijie_Wang3">Zhijie Wang</a>, <a href="/profile?id=~Felix_Juefei-Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_Juefei-Xu1">Felix Juefei-Xu</a>, <a href="/profile?id=~Qing_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qing_Guo3">Qing Guo</a>, <a href="/profile?id=~Xingyu_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingyu_Li3">Xingyu Li</a>, <a href="/profile?id=~Lei_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Ma1">Lei Ma</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">8 Replies</span>


        </div>

          <a href="#AI9pujYWf7-details-691" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AI9pujYWf7-details-691"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robustness, benchmark, point cloud detection</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Object detection through LiDAR-based point cloud has recently been important in autonomous driving. Although achieving high accuracy on public benchmarks, the state-of-the-art detectors may still go wrong and cause a heavy loss due to the widespread corruptions in the real world like rain, snow, sensor noise, etc. Nevertheless, there is a lack of a large-scale dataset covering diverse scenes and realistic corruption types with different severities to develop practical and robust point cloud detectors, which is challenging due to the heavy collection costs. To alleviate the challenge and start the first step for robust point cloud detection, we propose the physical-aware simulation methods to generate degraded point clouds under different real-world common corruptions. Then, for the first attempt, we construct a benchmark based on the physical-aware common corruptions for point cloud detectors, which contains a total of 1,122,150 examples covering 7,481 scenes, 25 common corruption types, and 6 severities. With such a novel benchmark, we conduct extensive empirical studies on 7 state-of-the-art detectors that contain 6 different detection frameworks. Thus we get several insight observations revealing the vulnerabilities of the detectors and indicating the enhancement directions. Moreover, we further study the effectiveness of existing robustness enhancement methods based on data augmentation and data denoising. The benchmark can potentially be a new platform for evaluating point cloud detectors, opening a door for developing novel robustness enhancement methods. We make this benchmark publicly available on https://github.com/Castiel-Lee/robustness_pc_detector.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=AI9pujYWf7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Castiel-Lee/robustness_pc_detector" target="_blank" rel="nofollow noreferrer">https://github.com/Castiel-Lee/robustness_pc_detector</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="SykAbDXmTN" data-number="228">
        <h4>
          <a href="/forum?id=SykAbDXmTN">
              Lesion size matters: A validation dataset for melanoma detection
          </a>


            <a href="/pdf?id=SykAbDXmTN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Emily_A._Cowen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_A._Cowen1">Emily A. Cowen</a>, <a href="/profile?email=mehtap1%40mskcc.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="mehtap1@mskcc.org">Paras Mehta</a>, <a href="/profile?id=~Nicholas_Raymond_Kurtansky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Raymond_Kurtansky1">Nicholas Raymond Kurtansky</a>, <a href="/profile?id=~Erik_P_Duhaime1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_P_Duhaime1">Erik P Duhaime</a>, <a href="/profile?id=~Pascale_Guitera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pascale_Guitera1">Pascale Guitera</a>, <a href="/profile?email=qishen.ha%40h2o.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="qishen.ha@h2o.ai">Qishen Ha</a>, <a href="/profile?id=~Alan_Halpern1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Halpern1">Alan Halpern</a>, <a href="/profile?id=~Kivanc_Kose1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kivanc_Kose1">Kivanc Kose</a>, <a href="/profile?email=boli%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="boli@nvidia.com">Bo Liu</a>, <a href="/profile?email=fuxuliu0329%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="fuxuliu0329@gmail.com">Fuxu Liu</a>, <a href="/profile?email=marchetm%40mskcc.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="marchetm@mskcc.org">Michael A. Marchetti</a>, <a href="/profile?id=~Jochen_Weber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jochen_Weber1">Jochen Weber</a>, <a href="/profile?email=rotembev%40mskcc.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="rotembev@mskcc.org">Veronica Rotemberg</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#SykAbDXmTN-details-933" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SykAbDXmTN-details-933"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">melanoma classification, binary classification, dermatology, dermoscopic images</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a dataset of consecutive biopsies for melanoma across year 2020 with novel annotations to validate melanoma classifier algorithms according to size and lesion difficulty.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Cutaneous melanoma is one of the most common malignancies globally, and early detection is key for reduced morbidity and mortality. Artificial intelligence has shown great potential as an accessible tool offering additional diagnostic value, both to clinicians faced with challenging lesions, such as featureless melanocytic lesions and small-diameter ($\leq5mm$) melanoma, and to individuals in regions that lack access to dermatologic care. Conversely, histopathologically equivocal lesions pose a challenge to the ground truth precision in developing these new tools. Existing datasets of lesions suspicious for melanoma do not include detailed lesion size or crowd assessment of lesion difficulty, or histopathologically intermediate cases. We have curated a dataset of 1,295 consecutive lesions biopsied to rule out melanoma in 2020 at a tertiary cancer center in the United States, annotated with patient demographics, lesion diameter, human labelers, MPATH-Dx, and reference standard histopathological diagnosis. We validated this dataset against an open-source binary classifier for melanoma that won the International Skin Imaging Collaboration (ISIC) grand challenge for melanoma detection in 2020 held on Kaggle, achieving an overall area under the receiver-operator-curve (AUC) of 0.810 for the dataset as a whole and 0.745 for small-diameter lesions. Both the algorithm and humans had more difficulty with small-diameter lesions. Through this dataset and benchmark, we demonstrate that algorithms for melanoma classification are susceptible to lesion size, and algorithms should be trained and adjusted with this in mind. This dataset can be used by future developers to validate melanoma classifier algorithms according to size and lesion difficulty.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=SykAbDXmTN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://doi.org/10.34970/151324" target="_blank" rel="nofollow noreferrer">https://doi.org/10.34970/151324</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset: https://doi.org/10.34970/151324
        Metadata: https://github.com/ISIC-Research/lesion-size-2020
        Code: https://github.com/haqishen/SIIM-ISIC-Melanoma-Classification-1st-Place-Solution</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset: CC-BY
        Code: Apache 2.0 open source license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ctX2eXYIW3" data-number="227">
        <h4>
          <a href="/forum?id=ctX2eXYIW3">
              Evaluating Self-Supervised Learning for Molecular Graph Embeddings
          </a>


            <a href="/pdf?id=ctX2eXYIW3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hanchen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanchen_Wang1">Hanchen Wang</a>, <a href="/profile?id=~Jean_Kaddour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean_Kaddour1">Jean Kaddour</a>, <a href="/profile?id=~Shengchao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengchao_Liu1">Shengchao Liu</a>, <a href="/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Tang1">Jian Tang</a>, <a href="/profile?id=~Matt_Kusner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matt_Kusner1">Matt Kusner</a>, <a href="/profile?id=~Joan_Lasenby1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joan_Lasenby1">Joan Lasenby</a>, <a href="/profile?id=~Qi_Liu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Liu5">Qi Liu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ctX2eXYIW3-details-913" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ctX2eXYIW3-details-913"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Graph Self-Supervised Learning (GSSL) paves the way for learning graph embeddings without expert annotation, which is particularly impactful for molecular graphs since the number of possible molecules is enormous and labels are expensive to obtain. However, by design, GSSL methods are not trained to perform well on one downstream task but aim for transferability to many, making evaluating them less straightforward. As a step toward obtaining profiles of molecular graph embeddings with diverse and interpretable attributes, we introduce Molecular Graph Representation Evaluation (MolGraphEval), a suite of probe tasks, categorised into (i) topological-, (ii) substructure-, and (iii) embedding space properties. By benchmarking existing GSSL methods on both existing downstream datasets and MolGraphEval, we discover surprising discrepancies between conclusions drawn from existing datasets alone versus more fine-grained probing, suggesting that current evaluation protocols do not provide the whole picture. Our modular, automated end-to-end GSSL pipeline code will be released upon acceptance, including standardised graph loading, experiment management, and embedding evaluation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ctX2eXYIW3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We will release our modular, automated end-to-end GSSL pipeline code, including standardised graph loading, experiment management, and embedding evaluation. The evaluation metrics are calculated on-the-fly using our code on the released datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We will release the our modular, automated end-to-end GSSL pipeline code, including standardised graph loading, experiment management, and embedding evaluation, in the next few months due to the high workloads. The evaluation metrics are automated calculated with our code on the previously released datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We use The MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8OIMLJekxJN" data-number="225">
        <h4>
          <a href="/forum?id=8OIMLJekxJN">
              ACNet: A Benchmark for Activity Cliff Prediction
          </a>


            <a href="/pdf?id=8OIMLJekxJN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ziqiao_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziqiao_Zhang1">Ziqiao Zhang</a>, <a href="/profile?email=byzhao21%40m.fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="byzhao21@m.fudan.edu.cn">Bangyi Zhao</a>, <a href="/profile?email=alxie21%40m.fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="alxie21@m.fudan.edu.cn">Ailin Xie</a>, <a href="/profile?id=~Yatao_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yatao_Bian1">Yatao Bian</a>, <a href="/profile?id=~Shuigeng_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuigeng_Zhou1">Shuigeng Zhou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#8OIMLJekxJN-details-707" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8OIMLJekxJN-details-707"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Drug Discovery, Activity Cliffs, Molecular Property Prediction Models, Quantitative Structure-Activity Relationship</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A large-scale benchmark for Activity Cliff prediction with over 400K pairs of similar compounds against 190 targets, which is challenging for deep models to cope with due to the imbalanced, low-data and out-of-distribution features.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Activity Cliffs (ACs), which are generally defined as pairs of structurally similar molecules that are active against the same bio-target but have large difference in the binding potency, are of great importance to drug discovery. However, the AC prediction task, i.e., to predict whether a pair of similar molecules exhibit AC relationship, has not yet been fully explored. This work introduces ACNet, a large-scale benchmark for AC prediction. ACNet curates over 400K Matched Molecular Pairs (MMPs) against 190 targets, including over 20K MMP-Cliffs and 380K non-AC MMPs, and provides five subsets for model development and evaluation. 15 molecular property prediction models adopted as molecular representation encoders for the AC prediction task are evaluated in experiments. Our experimental results show that the traditional FingerPrint-based method is superior to the deep learning models in the AC prediction task, and the imbalanced, low-data and out-of-distribution features of the ACNet benchmark make it challenging for the existing molecular property prediction models to cope with. Our work is the first large-scale benchmark for the AC prediction task, which may stimulate the study of AC prediction models and prompt further breakthroughs in AI-aided drug discovery.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Homepage URL: https://drugai.github.io/ACNet/</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">To access the ACNet benchmark, please visit the homepage of ACNet project.
        Hompage URL: https://drugai.github.io/ACNet/

        The codes of ACNet benchmark are provided in Github repository. Illustration of ACNet is also provided in the Readme in this repository.
        Codes URL: https://github.com/DrugAI/ACNet

        The dataset files can be accessed via google drive.
        Datafiles URL: https://drive.google.com/drive/folders/1JogBAg9AI0pUxY44w9_g8RHboLf7V5q7?usp=sharing</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Copyright 2022 Ziqiao Zhang, Yatao Bian

            Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

            The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

            THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="yO_1tKMzl7Y" data-number="224">
        <h4>
          <a href="/forum?id=yO_1tKMzl7Y">
              XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence
          </a>


            <a href="/pdf?id=yO_1tKMzl7Y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ming_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming_Zhu1">Ming Zhu</a>, <a href="/profile?id=~Aneesh_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aneesh_Jain1">Aneesh Jain</a>, <a href="/profile?id=~Karthik_Suresh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karthik_Suresh2">Karthik Suresh</a>, <a href="/profile?id=~Roshan_Ravindran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roshan_Ravindran1">Roshan Ravindran</a>, <a href="/profile?id=~Sindhu_Tipirneni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sindhu_Tipirneni1">Sindhu Tipirneni</a>, <a href="/profile?id=~Chandan_K._Reddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chandan_K._Reddy1">Chandan K. Reddy</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#yO_1tKMzl7Y-details-720" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yO_1tKMzl7Y-details-720"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Code Intelligence, Machine Learning, Software Engineering, Code Translation, AI for Code</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present the largest parallel dataset for programming code tasks both in terms of size and number of languages.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like GitHub enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=yO_1tKMzl7Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/reddy-lab-code-research/XLCoST" target="_blank" rel="nofollow noreferrer">https://github.com/reddy-lab-code-research/XLCoST</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/reddy-lab-code-research/XLCoST" target="_blank" rel="nofollow noreferrer">https://github.com/reddy-lab-code-research/XLCoST</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code: Apache License 2.0 License
        Data: CC BY-SA 4.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="jKJdn4-szQ" data-number="223">
        <h4>
          <a href="/forum?id=jKJdn4-szQ">
              A Large-Scale Annotated Multivariate Time Series Aviation Maintenance Dataset from the NGAFID
          </a>


            <a href="/pdf?id=jKJdn4-szQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hong_Yang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hong_Yang5">Hong Yang</a>, <a href="/profile?id=~Travis_Desell2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Travis_Desell2">Travis Desell</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#jKJdn4-szQ-details-927" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jKJdn4-szQ-details-927"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Aircraft, Maintenance, Time Series, Dataset, Prognostic Health Management, Predictive Maintenance, Machine Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A real life dataset of aircraft maintenance and associated flight recordings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents the first publicly available, non-simulated, fleet-wide aircraft flight recording and maintenance log data for use in predictive maintenance. We present 31,177 hours of flight data across 28,935 flights, which occur relative to 2,111 unplanned maintenance events clustered into 36 types of maintenance issues. Flights are annotated as before or after maintenance, with some flights occurring on the day of maintenance.  Collecting data to evaluate predictive maintenance systems is challenging because it is difficult, dangerous, and unethical to generate data from compromised aircraft. To overcome this, we use the National General Aviation Flight Information Database (NGAFID), which contains flights recorded during regular operation of aircraft, and maintenance logs to construct a predictive maintenance dataset. Unlike previous datasets generated with simulations or in laboratory settings, the NGAFID Aviation Maintenance Dataset contains real flight records and maintenance logs from different seasons, weather conditions, pilots, and flight patterns. Additionally, we provide Python code to easily download the dataset and a Colab environment to reproduce our benchmarks on three different models. Our dataset presents a difficult challenge for machine learning researchers, which can fuel the development of novel multivariate time series classification algorithms, and is a valuable benchmark for prognostic health maintenance methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=jKJdn4-szQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/hyang0129/NGAFIDDATASET" target="_blank" rel="nofollow noreferrer">https://github.com/hyang0129/NGAFIDDATASET</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/hyang0129/NGAFIDDATASET
        https://www.kaggle.com/datasets/hooong/aviation-maintenance-dataset-from-the-ngafid
        https://doi.org/10.5281/zenodo.6624956</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">                    GNU GENERAL PUBLIC LICENSE
                               Version 3, 29 June 2007

         Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;
         Everyone is permitted to copy and distribute verbatim copies
         of this license document, but changing it is not allowed.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xUXTbq6gWsB" data-number="222">
        <h4>
          <a href="/forum?id=xUXTbq6gWsB">
              NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks
          </a>


            <a href="/pdf?id=xUXTbq6gWsB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Renbo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renbo_Tu1">Renbo Tu</a>, <a href="/profile?id=~Nicholas_Roberts2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Roberts2">Nicholas Roberts</a>, <a href="/profile?id=~Mikhail_Khodak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Khodak1">Mikhail Khodak</a>, <a href="/profile?id=~Junhong_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junhong_Shen1">Junhong Shen</a>, <a href="/profile?id=~Frederic_Sala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederic_Sala1">Frederic Sala</a>, <a href="/profile?id=~Ameet_Talwalkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ameet_Talwalkar1">Ameet Talwalkar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#xUXTbq6gWsB-details-843" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xUXTbq6gWsB-details-843"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">automated machine learning, neural architecture search, diverse tasks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We provide a benchmark for neural architecture search on a diverse set of understudied tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=xUXTbq6gWsB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://nb360.ml.cmu.edu/" target="_blank" rel="nofollow noreferrer">https://nb360.ml.cmu.edu/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://nb360.ml.cmu.edu/" target="_blank" rel="nofollow noreferrer">https://nb360.ml.cmu.edu/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Code license: MIT.
        Dataset licenses:
        • CIFAR-100: CC BY 4.0 (on https://www.tensorflow.org/datasets/catalog/cifar100)
        • Spherical CIFAR-100: CC BY-SA
        • NinaPro: CC BY-ND
        • FSD50k: CC BY 4.0
        • Darcy Flow: MIT
        • DeepCov, PSICOV: GPL
        • Cosmic: Open License (https://registry.opendata.aws/hst/)
        • ECG: ODC-BY 1.0
        • Satellite: GPL 3.0
        • Deepsea: CC BY 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="4lIyA3oCao" data-number="221">
        <h4>
          <a href="/forum?id=4lIyA3oCao">
              A Weakly Supervised Learning Dataset for Research Replication Prediction
          </a>


            <a href="/pdf?id=4lIyA3oCao" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tianyi_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Luo1">Tianyi Luo</a>, <a href="/profile?id=~Juntao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juntao_Wang1">Juntao Wang</a>, <a href="/profile?id=~Yang_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu3">Yang Liu</a>, <a href="/profile?id=~Yiling_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiling_Chen1">Yiling Chen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#4lIyA3oCao-details-430" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4lIyA3oCao-details-430"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Research Replication Prediction, Dataset, Weakly Supervised Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a weakly supervised Research Replication Prediction (RRP) dataset to facilitate the development of ML approaches for the task of predicting whether a research claim can be reproduced or not.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Knowing whether a published research result can be replicated is important and machine learning (ML) methods have been adopted to predict scientific claims' replicability due to the high cost of direct replication. We present a weakly supervised Research Replication Prediction (RRP) dataset to facilitate the development of ML approaches for the task of predicting whether a research claim can be reproduced or not. In this dataset, we collected two types of data with different costs: one with direct verification (expensive with smaller size) and one using crowdsourcing (larger scale but potentially noisy). In total, our dataset contains 399 directly replicated samples and 2,682 crowdsourced samples. We benchmark the performances of several representative weakly supervised baseline methods. We report several commonly used metrics (accuracy, precision, recall, and F1) to evaluate the models.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://replication.noisylabels.com/" target="_blank" rel="nofollow noreferrer">http://replication.noisylabels.com/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_LZLeiktZyB" data-number="220">
        <h4>
          <a href="/forum?id=_LZLeiktZyB">
              A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams
          </a>


            <a href="/pdf?id=_LZLeiktZyB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sarah_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_Zhang1">Sarah Zhang</a>, <a href="/profile?email=rshuttle%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rshuttle@mit.edu">Reece Shuttleworth</a>, <a href="/profile?id=~Derek_Austin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Austin1">Derek Austin</a>, <a href="/profile?id=~Yann_Hicke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yann_Hicke1">Yann Hicke</a>, <a href="/profile?id=~Leonard_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonard_Tang1">Leonard Tang</a>, <a href="/profile?id=~Sathwik_Karnik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sathwik_Karnik1">Sathwik Karnik</a>, <a href="/profile?email=darnellg%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="darnellg@mit.edu">Darnell Granberry</a>, <a href="/profile?id=~Iddo_Drori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iddo_Drori1">Iddo Drori</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#_LZLeiktZyB-details-186" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_LZLeiktZyB-details-186"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Can a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer MIT final exams in Introduction to Machine Learning at a human level. The course is a large undergraduate class with around five hundred students each semester. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and STEM courses at a human level. In this work, we solve questions from final exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from eight MIT Introduction to Machine Learning final exams between Fall 2017 and Spring 2022 and provide code for automatically answering these questions and generating new questions. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, GPT-3 pre-trained on text and Codex fine-tuned on code on a range of machine learning topics and find that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_LZLeiktZyB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/idrori/mlfinalsQ" target="_blank" rel="nofollow noreferrer">https://github.com/idrori/mlfinalsQ</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/idrori/mlfinalsQ/data" target="_blank" rel="nofollow noreferrer">https://github.com/idrori/mlfinalsQ/data</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-0QznlyWJ2j" data-number="219">
        <h4>
          <a href="/forum?id=-0QznlyWJ2j">
              jazzNet: An Open-Source Dataset of Fundamental Piano Patterns for Machine Learning Research in Music
          </a>


            <a href="/pdf?id=-0QznlyWJ2j" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tosiron_Adegbija1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tosiron_Adegbija1">Tosiron Adegbija</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">05 Jun 2022 (modified: 07 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#-0QznlyWJ2j-details-345" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-0QznlyWJ2j-details-345"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">music dataset, music information retrieval, automatic music recognition, music patterns</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The paper presents a dataset of fundamental piano chords, arpeggios, scales, and chord progressions for machine learning research in music information retrieval.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces the jazzNet Dataset, a freely available multi-class dataset of fundamental piano musical patterns. The dataset is intended to promote the development of machine learning algorithms for challenging tasks in the area of music information retrieval (MIR). The dataset is inspired by how efficient musical learning works in the real world, including its hierarchies of complexity. A human being learns best when they understand the underlying musical patterns---chords, arpeggios, scales, and chord progressions---behind musical pieces. By learning the underlying fundamental patterns, new complex patterns can be learned or generated, leading to better improvisation. The dataset comprises a structured set of piano chords, arpeggios, scales, and chord progressions along with their inversions, for a total of 161,840 labeled piano patterns, resulting in 265GB and 26,935 hours of audio recordings. We describe the dataset, its composition, creation, and generation. We also summarize different possible uses of the dataset and report the performance of a simple experiment using a convolutional recurrent neural network (CRNN) model to predict different classes and sub-classes of musical patterns.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-0QznlyWJ2j&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>