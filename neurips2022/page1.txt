<ul class="list-unstyled submissions-list">
    <li class="note " data-id="NETsAaKwzMp" data-number="469">
        <h4>
          <a href="/forum?id=NETsAaKwzMp">
              PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search
          </a>


            <a href="/pdf?id=NETsAaKwzMp" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Thang_M._Pham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thang_M._Pham1">Thang M. Pham</a>, <a href="/profile?id=~Seunghyun_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Seunghyun_Yoon1">Seunghyun Yoon</a>, <a href="/profile?id=~Trung_Bui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Trung_Bui1">Trung Bui</a>, <a href="/profile?id=~Anh_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anh_Nguyen1">Anh Nguyen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">09 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#NETsAaKwzMp-details-846" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NETsAaKwzMp-details-846"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">phrase-in-context, contextualized embeddings, phrase embeddings, phrase understanding, retrieval, semantic search, disambiguation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose PiC a Phrase-in-Context suite of three tasks: Phrase Similarity, Phrase Retrieval, and Phrase Sense Disambiguation for evaluating and training contextualized phrase embeddings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Since BERT (Devlin et al. 2019), learning contextualized word embeddings has been a de-facto standard in NLP. However, the progress of learning contextualized phrase embeddings is hindered by the lack of a human-annotated, phrase-in-context benchmark. To fill this gap, we propose PiC---a dataset of ~28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks of increasing difficulty for evaluating the quality of phrase embeddings. We find that training on our dataset improves ranking models' accuracy and remarkably pushes QA models to near-human accuracy (which is 95% EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the QA models learn to better capture the common meaning of a phrase regardless of its actual context. That is, on our Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially (60% EM), failing to differentiate between two different senses of the same phrase under two different contexts. Further results of our 3-task benchmark reveal that learning contextualized phrase embeddings remains an interesting, open challenge.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NETsAaKwzMp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://huggingface.co/PiC" target="_blank" rel="nofollow noreferrer">https://huggingface.co/PiC</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Download: https://huggingface.co/PiC
        Description: https://phrase-in-context.github.io/</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is licensed under CC-BY-NC 4.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Z_0_pTPvI4W" data-number="467">
        <h4>
          <a href="/forum?id=Z_0_pTPvI4W">
              FlowchartQA: The First Large-Scale Benchmark for Reasoning over Flowcharts
          </a>


            <a href="/pdf?id=Z_0_pTPvI4W" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Simon_Tannert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Tannert1">Simon Tannert</a>, <a href="/profile?email=feighels%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="feighels@gmail.com">Marcelo Feighelstein</a>, <a href="/profile?id=~Jasmina_Bogojeska2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jasmina_Bogojeska2">Jasmina Bogojeska</a>, <a href="/profile?id=~Joseph_Shtok1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joseph_Shtok1">Joseph Shtok</a>, <a href="/profile?id=~Assaf_Arbelle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Assaf_Arbelle1">Assaf Arbelle</a>, <a href="/profile?id=~Peter_W._J._Staar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peter_W._J._Staar2">Peter W. J. Staar</a>, <a href="/profile?id=~Anika_Schumann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anika_Schumann1">Anika Schumann</a>, <a href="/profile?id=~Jonas_Kuhn2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonas_Kuhn2">Jonas Kuhn</a>, <a href="/profile?id=~Leonid_Karlinsky3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Leonid_Karlinsky3">Leonid Karlinsky</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">08 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Z_0_pTPvI4W-details-533" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Z_0_pTPvI4W-details-533"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Flowcharts are a very popular type of diagrams in many kinds of documents, conveying large amounts of useful information and knowledge (e.g. on processes, workflows, or causality). In this paper, we propose FlowchartQA – a novel, and first of its kind, large scale benchmark with close to 1M flowchart images and 6M question-answer pairs. The questions in FlowchartQA cover different aspects of geometric, topological, and semantic information contained in the charts, and are carefully balanced to reduce biases. We accompany our proposed benchmark with a comprehensive set of baselines based on text-only, image and graph, together with a set of comprehensive ablation studies and qualitative analysis in order to establish a good basis for future work. We report some interesting findings following our baseline experiments, and believe that FlowchartQA will provide the community with a (currently absent) testbed for flowchart understanding.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Z_0_pTPvI4W&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Instructions on how to access the datasets can be found in the supplementary material or under Dataset Url.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The datasets are hosted on the IBM Cloud.

        For FlowchartQA, the total dataset is of size 62 GB and is split into 63 parts for easy download. In order to download the file please follow this command:

        wget https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/flowchartsQA/{list.txt,LICENSE-CC-BY-NC-SA-4.0.md};
        while read f; do wget https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/flowchartsQA/$f -O - &gt;&gt; flowchartqa.tar.gz; done; &lt; list.txt;



        The real-world test data is 9.2 MB and can be download using this command:

        wget https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/flowchartsQA/{real-world_test_data.tar.gz,LICENSE-CC-BY-NC-SA-4.0.md};</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The datasets are licensed under CC-BY-NC-SA-4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="PIb7WU8jQQH" data-number="466">
        <h4>
          <a href="/forum?id=PIb7WU8jQQH">
              Does Action Recognition Need Pose Estimation?
          </a>


            <a href="/pdf?id=PIb7WU8jQQH" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Antoine_Mercier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Antoine_Mercier1">Antoine Mercier</a>, <a href="/profile?id=~Guillaume_Berger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Guillaume_Berger1">Guillaume Berger</a>, <a href="/profile?id=~Sunny_Panchal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sunny_Panchal1">Sunny Panchal</a>, <a href="/profile?id=~Florian_Dietrichkeit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florian_Dietrichkeit1">Florian Dietrichkeit</a>, <a href="/profile?id=~Cornelius_B%C3%B6hm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cornelius_Böhm1">Cornelius Böhm</a>, <a href="/profile?id=~Ingo_Bax1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ingo_Bax1">Ingo Bax</a>, <a href="/profile?id=~Roland_Memisevic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roland_Memisevic1">Roland Memisevic</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">08 Jun 2022 (modified: 10 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#PIb7WU8jQQH-details-483" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PIb7WU8jQQH-details-483"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">end-to-end learning, action recognition, 3D convolution, video, fitness</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">With appropriately labeled data end-to-end learning on raw pixels can compete with pose estimation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">End-to-end learning has taken hold of many computer vision tasks, in particular, related to still images, with task-specific optimization yielding very strong performance. Nevertheless, human-centric action recognition is still largely dominated by hand-crafted pipelines, and only individual components are replaced by neural networks that typically operate on individual frames. In this work, we show that when appropriately labeled data is available, end-to-end learning on raw pixels from video can compete with, and in many cases outperform, state-of-the-art action recognition pipelines based on pose estimation. We present a new dataset as a real-world benchmark to compare transfer learning capabilities. This dataset consists of 40 granular video-level classes, 9 frame-level classes, spanning four fitness exercises, recorded in a home workout setting. Additionally, we train our models for real-time repetition counting using a temporally-annotated subset.

         </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=PIb7WU8jQQH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset will be hosted under https://developer.qualcomm.com/.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We are in the process of hosting the dataset and plan to make the url available on completion.
        We anticipate completion prior to the conference.
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">For an example of a similar, existing dataset and license, see: https://developer.qualcomm.com/software/ai-datasets/jester</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="FPYO0n8JxB" data-number="465">
        <h4>
          <a href="/forum?id=FPYO0n8JxB">
              CaltechFN: Distorted and Partially Occluded Digits
          </a>


            <a href="/pdf?id=FPYO0n8JxB" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Patrick_Rim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Patrick_Rim1">Patrick Rim</a>, <a href="/profile?id=~Snigdha_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Snigdha_Saha1">Snigdha Saha</a>, <a href="/profile?id=~Marcus_Rim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marcus_Rim1">Marcus Rim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">07 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#FPYO0n8JxB-details-34" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FPYO0n8JxB-details-34"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">machine learning, computer vision, digits, distorted, occluded, image classification, object detection, dataset, benchmark, sports analytics, football, human performance</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present CaltechFN, a dataset of distorted and partially occluded digits that we hope will serve as a state-of-the-art benchmark for future classification and detection models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Digit datasets are widely used as compact, generalizable benchmarks for novel computer vision models. However, modern deep learning architectures have surpassed the human performance benchmarks on existing digit datasets, given that these datasets contain digits that have limited variability. In this paper, we introduce Caltech Football Numbers (CaltechFN), an image dataset of highly variable American football digits that aims to serve as a more difficult state-of-the-art benchmark for classification and detection tasks. Currently, CaltechFN contains 61,728 images with 264,572 labeled digits. Given the many different ways that digits on American football jerseys can be distorted and partially occluded in a live-action capture, we find that in comparison to humans, current computer vision models struggle to classify and detect the digits in our dataset. By comparing the performance of the latest task-specific models on CaltechFN and on an existing digit dataset, we show that our dataset indeed presents a far more difficult set of digits and that models trained on it still demonstrate high cross-dataset generalization. We also provide human performance benchmarks for our dataset to demonstrate the current gap between the abilities of humans and computers in the tasks of classifying and detecting the digits in our dataset. Finally, we describe two real-world applications that can be advanced using our dataset. CaltechFN is publicly available at \url{https://data.caltech.edu/records/20174}, and all benchmark code is available at \url{https://github.com/snigdhasaha7/caltechfn_eval}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=FPYO0n8JxB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Dataset: https://data.caltech.edu/records/20174 ; Benchmark code: https://github.com/snigdhasaha7/caltechfn_eval</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://data.caltech.edu/records/20174" target="_blank" rel="nofollow noreferrer">https://data.caltech.edu/records/20174</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset: Creative Commons Attribution (CC-BY) License
        Benchmark code: MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xwL3PfhRz4c" data-number="463">
        <h4>
          <a href="/forum?id=xwL3PfhRz4c">
              State of the Benchmark: Existing Efforts across Modalities supporting Single and Multipurpose AI
          </a>


            <a href="/pdf?id=xwL3PfhRz4c" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Maria_Glenski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maria_Glenski1">Maria Glenski</a>, <a href="/profile?id=~Ellyn_Ayton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ellyn_Ayton1">Ellyn Ayton</a>, <a href="/profile?id=~Robin_Cosbey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robin_Cosbey1">Robin Cosbey</a>, <a href="/profile?email=pcomising%40yahoo.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="pcomising@yahoo.com">Alyssa Pauline Comising</a>, <a href="/profile?id=~Svitlana_Volkova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Svitlana_Volkova1">Svitlana Volkova</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#xwL3PfhRz4c-details-355" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xwL3PfhRz4c-details-355"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Apace with the increased use of artificial intelligence or machine learning models to support and augment decision making in real-world systems, there is a corresponding reliance on benchmark datasets and evaluations to validate performance and reliability. In this paper, we provide an overview of existing benchmark datasets across modalities (text, image, video, graph, tabular, code, multimodal) and analyze the geographic concentration of development and the diversity (or lack) of the global representation of authors and languages within data that may impact the use of state-of-the-art benchmarks to enable fair and accountable evaluation of AI/ML models. Our review considers a diverse set of tasks and relevant, well-known problems (emerging and established) assembled for efficient collaboration among researchers.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="S-sjQg88L9z" data-number="462">
        <h4>
          <a href="/forum?id=S-sjQg88L9z">
              Constructing the CORD-19 Vaccine Dataset
          </a>


            <a href="/pdf?id=S-sjQg88L9z" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Manisha_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Manisha_Singh1">Manisha Singh</a>, <a href="/profile?id=~Divy_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Divy_Sharma1">Divy Sharma</a>, <a href="/profile?id=~Bridget_Tyree1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bridget_Tyree1">Bridget Tyree</a>, <a href="/profile?id=~Alonso_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alonso_Ma1">Alonso Ma</a>, <a href="/profile?id=~Margaret_Mitchell3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Margaret_Mitchell3">Margaret Mitchell</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#S-sjQg88L9z-details-446" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="S-sjQg88L9z-details-446"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">CORD-19, Vaccine, Vaccination, Language of CORD-19, CORD-19-Vaccination, Dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">‘CORD-19-Vaccination’ dataset to cater to scientists specifically looking into COVID-19 vaccine-related research</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce new dataset ‘CORD-19-Vaccination’ to cater to scientists specifically looking into COVID-19 vaccine-related research. This dataset is extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new columns for language detail, author demography, keywords, and topic per paper. Facebook’s fastText model is used to identify languages [Joulin et al., 2016]. To establish author demography (author affiliation, lab/institution location, and lab/institution country columns) we processed the JSON file for each paper and then further enhanced using Google’s search API to determine country values. ‘Yake’ was used to extract keywords from the title, abstract, and body of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to add topic information [Campos et al.,2020, 2018a,b]. To evaluate the dataset, we demonstrate a question-answering task like the one used in the CORD-19 Kaggle challenge [Goldbloom et al., 2022].For further evaluation, sequential sentence classification was performed on each paper’s abstract using the model from Dernoncourt et al. [2016]. We partially hand-annotated the training dataset and used a pre-trained BERT-PubMed layer. ‘CORD-19-Vaccination’ contains 30k research papers and can be immensely valuable for NLP research such as text mining, information extraction, and question answering, specific to the domain of COVID-19 vaccine research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=S-sjQg88L9z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/manisha-Singh-UW/CORD-19-Vaccination/" target="_blank" rel="nofollow noreferrer">https://github.com/manisha-Singh-UW/CORD-19-Vaccination/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">URL : https://github.com/manisha-Singh-UW/CORD-19-Vaccination/tree/main/dataset

        Multiple methods have been provided to access the dataset.

        A) Instructions to download the file:
        1. Download the dataset from the GitHub repo
        2. Extract to the desired folder

        B) Python code to access the dataset
        Python code to download and unzip the dataset is provided at 'https://github.com/manisha-Singh-UW/CORD-19-Vaccination'</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The license information of the dataset and code is as below.

        Dataset: ‘CORD-19-Vaccination’ dataset is extracted from ‘CORD-19’ dataset, so ‘CORD-19-Vaccination’ dataset also follows all the CORD-19 Dataset License (https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-13/COVID.DATA.LIC.AGMT.pdf)

        Code: The code used for data augmentation is at (https://github.com/manisha-Singh-UW/CORD-19-Vaccination/tree/main/code_data_augmentation) and is licensed under 'license: Apache-2.0 license'</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MKDdTASg_1y" data-number="460">
        <h4>
          <a href="/forum?id=MKDdTASg_1y">
              A Benchmark for Compositional Visual Reasoning
          </a>


            <a href="/pdf?id=MKDdTASg_1y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Aimen_Zerroug1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aimen_Zerroug1">Aimen Zerroug</a>, <a href="/profile?id=~Mohit_Vaishnav1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohit_Vaishnav1">Mohit Vaishnav</a>, <a href="/profile?id=~Julien_Colin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julien_Colin2">Julien Colin</a>, <a href="/profile?id=~Sebastian_Musslick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sebastian_Musslick1">Sebastian Musslick</a>, <a href="/profile?id=~Thomas_Serre1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Thomas_Serre1">Thomas Serre</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 13 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#MKDdTASg_1y-details-743" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MKDdTASg_1y-details-743"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Abstract Visual Reasoning, Compositionality, Data Efficiency, Transfer learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A visual reasoning benchmark that incorporates a large number of novel relations and focuses on evaluating compositionality and sample efficiency.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, a major gap remains in terms of the sample efficiency with which humans and AI systems learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- such that they can efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and associated image datasets at scale. Our proposed benchmark includes measures of sample efficiency, generalization and transfer across task rules, as well as the ability to leverage compositionality. We systematically evaluate modern neural architectures and find that, surprisingly, convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are a lot less data efficient compared to humans even after learning informative visual representations using self-supervision. Overall, we hope that our challenge will spur interest in the development of neural architectures that can learn to harness compositionality toward more efficient learning. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MKDdTASg_1y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/aimzer/CVR" target="_blank" rel="nofollow noreferrer">https://github.com/aimzer/CVR</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/aimzer/CVR" target="_blank" rel="nofollow noreferrer">https://github.com/aimzer/CVR</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset CVR and the dataset generation method are published under the Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ET4VQBSOL3n" data-number="458">
        <h4>
          <a href="/forum?id=ET4VQBSOL3n">
              SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics
          </a>


            <a href="/pdf?id=ET4VQBSOL3n" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Atul_Kumar_Sinha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Atul_Kumar_Sinha1">Atul Kumar Sinha</a>, <a href="/profile?id=~Daniele_Paliotta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniele_Paliotta1">Daniele Paliotta</a>, <a href="/profile?id=~B%C3%A1lint_M%C3%A1t%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bálint_Máté1">Bálint Máté</a>, <a href="/profile?id=~John_Andrew_Raine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~John_Andrew_Raine1">John Andrew Raine</a>, <a href="/profile?email=sebastian.pina-otey%40cern.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="sebastian.pina-otey@cern.ch">Sebastian Pina-Otey</a>, <a href="/profile?email=tobias.golling%40cern.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="tobias.golling@cern.ch">Tobias Golling</a>, <a href="/profile?id=~Fran%C3%A7ois_Fleuret2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~François_Fleuret2">François Fleuret</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#ET4VQBSOL3n-details-315" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ET4VQBSOL3n-details-315"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Simulator, Physics, HEP, Generative Models, Point Clouds</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep learning methods have gained popularity in high energy physics for fast modeling of particle showers in detectors. Detailed simulation frameworks such as the gold standard Geant4 are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simulation. The development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. Our contribution is SUPA, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. The generation is extremely fast and easy to use compared to Geant4, but still exhibits the key characteristics and challenges of the detailed simulation. The proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitudes over Geant4, and stores detailed geometric information about the shower propagation. SUPA provides much greater flexibility for setting initial conditions and defining multiple benchmarks for the development of models. Moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ET4VQBSOL3n&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/itsdaniele/SUPA" target="_blank" rel="nofollow noreferrer">https://github.com/itsdaniele/SUPA</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/itsdaniele/SUPA" target="_blank" rel="nofollow noreferrer">https://github.com/itsdaniele/SUPA</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2022 Atul Kumar Sinha, Daniele Paliotta, Bálint Máté

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="xojHpEKoWWF" data-number="456">
        <h4>
          <a href="/forum?id=xojHpEKoWWF">
              LibSignal: An Open Library for Traffic Signal Control
          </a>


            <a href="/pdf?id=xojHpEKoWWF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~hao_mei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~hao_mei1">hao mei</a>, <a href="/profile?id=~Xiaoliang_Lei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiaoliang_Lei1">Xiaoliang Lei</a>, <a href="/profile?id=~Longchao_Da1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Longchao_Da1">Longchao Da</a>, <a href="/profile?id=~Bin_Shi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bin_Shi2">Bin Shi</a>, <a href="/profile?id=~Hua_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hua_Wei1">Hua Wei</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#xojHpEKoWWF-details-714" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xojHpEKoWWF-details-714"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces a library for cross-simulator comparison of reinforcement learning models in traffic signal control tasks. This library is developed to implement recent state-of-the-art reinforcement learning models with extensible interfaces and unified cross-simulator evaluation metrics. It supports commonly-used simulators in traffic signal control tasks, including Simulation of Urban Mobility(SUMO) and CityFlow, and multiple benchmark datasets for fair comparisons. We conducted experiments to validate our implementation of the models and to calibrate the simulators so that the experiments from one simulator could be referential to the other. Based on the validated models and calibrated environments, this paper compares and reports the performance of current state-of-the-art RL algorithms across different datasets and simulators. This is the first time that these methods have been compared fairly under the same datasets with different simulators. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=xojHpEKoWWF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://darl-libsignal.github.io/" target="_blank" rel="nofollow noreferrer">https://darl-libsignal.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU General Public License 3.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="oeOvU1ZEN3y" data-number="455">
        <h4>
          <a href="/forum?id=oeOvU1ZEN3y">
              AD-Mol: A Dataset for Antibiotic Discovery
          </a>


            <a href="/pdf?id=oeOvU1ZEN3y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Daniel_Flam-Shepherd1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Flam-Shepherd1">Daniel Flam-Shepherd</a>, <a href="/profile?id=~Ella_Miray_Rajaonson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ella_Miray_Rajaonson1">Ella Miray Rajaonson</a>, <a href="/profile?id=~Kevin_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kevin_Zhu1">Kevin Zhu</a>, <a href="/profile?id=~Bhavya_Kasera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bhavya_Kasera1">Bhavya Kasera</a>, <a href="/profile?id=~Alan_Aspuru-Guzik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alan_Aspuru-Guzik2">Alan Aspuru-Guzik</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#oeOvU1ZEN3y-details-491" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oeOvU1ZEN3y-details-491"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">antibiotics, generative models</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There is a growing need for new antibiotics, accelerated by the rapid emergence of antibiotic-resistant bacteria. Recently, machine learning models have shown to be important potential tools in the antibiotic discovery pipeline, useful in predicting antibacterial activity for screening libraries of molecules for possible antibiotics. However, there are no datasets of antibiotic-like structures that are large enough for training generative models or other machine learning methods to directly discover novel candidates. In order to help accelerate machine learning assisted antibiotic discovery and as a new benchmark for molecular design-- we introduce AD-Mol, a new dataset of over 500K molecular structures that are likely inhibitors of Escherichia coli growth, curated by screening over one billion molecules from two publicly available chemical libraries. In order to demonstrate the potential of our dataset to assist in antibiotic discovery, we benchmark a few chemical language models trained on string molecular representations. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=oeOvU1ZEN3y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="7e6W6LEOBg3" data-number="453">
        <h4>
          <a href="/forum?id=7e6W6LEOBg3">
              Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning
          </a>


            <a href="/pdf?id=7e6W6LEOBg3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hua_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hua_Wei1">Hua Wei</a>, <a href="/profile?id=~Jingxiao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingxiao_Chen1">Jingxiao Chen</a>, <a href="/profile?id=~Xiyang_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiyang_Ji1">Xiyang Ji</a>, <a href="/profile?id=~Qin_Hongyang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qin_Hongyang1">Qin Hongyang</a>, <a href="/profile?id=~Minwen_Deng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minwen_Deng2">Minwen Deng</a>, <a href="/profile?id=~Siqin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Siqin_Li1">Siqin Li</a>, <a href="/profile?id=~Liang_Wang10" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liang_Wang10">Liang Wang</a>, <a href="/profile?id=~Weinan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weinan_Zhang1">Weinan Zhang</a>, <a href="/profile?id=~Yong_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yong_Yu1">Yong Yu</a>, <a href="/profile?id=~Liu_Linc1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Liu_Linc1">Liu Linc</a>, <a href="/profile?id=~Lanxiao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lanxiao_Huang1">Lanxiao Huang</a>, <a href="/profile?id=~Deheng_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Deheng_Ye1">Deheng Ye</a>, <a href="/profile?id=~QIANG_FU8" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~QIANG_FU8">QIANG FU</a>, <a href="/profile?id=~Yang_Wei2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yang_Wei2">Yang Wei</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#7e6W6LEOBg3-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7e6W6LEOBg3-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement learning, competitive reinforcement learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces Honor of Kings Arena, a reinforcement learning (RL) environment based on the Honor of Kings, one of the world’s most popular games at present. Compared to other environments studied in most previous work, ours presents new generalization challenges for competitive reinforcement learning. It is a multi-agent problem with one agent competing against its opponent; and it requires the generalization ability as it has diverse targets to control and diverse opponents to compete with. We describe the observation, action, and reward specifications for the Honor of Kings domain and provide an open-source Python-based interface for communicating with the game engine. We provide twenty target heroes with a variety of tasks in Honor of Kings Arena and present initial baseline results for RL-based methods with feasible computing resources.  Finally, we showcase the generalization challenges imposed by Honor of Kings Arena and possible remedies to the challenges. All of the software, including the environment-class, are publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=7e6W6LEOBg3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/tencent-ailab/hok_env" target="_blank" rel="nofollow noreferrer">https://github.com/tencent-ailab/hok_env</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Mg59hlrvV9x" data-number="451">
        <h4>
          <a href="/forum?id=Mg59hlrvV9x">
              TCAB: A Large-Scale Text Classification Attack Benchmark
          </a>


            <a href="/pdf?id=Mg59hlrvV9x" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kalyani_Asthana1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kalyani_Asthana1">Kalyani Asthana</a>, <a href="/profile?id=~Zhouhang_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhouhang_Xie1">Zhouhang Xie</a>, <a href="/profile?id=~Wencong_You1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wencong_You1">Wencong You</a>, <a href="/profile?id=~Adam_Noack1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adam_Noack1">Adam Noack</a>, <a href="/profile?id=~Jonathan_Brophy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jonathan_Brophy1">Jonathan Brophy</a>, <a href="/profile?id=~Sameer_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sameer_Singh1">Sameer Singh</a>, <a href="/profile?id=~Daniel_Lowd1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Lowd1">Daniel Lowd</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 12 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Mg59hlrvV9x-details-550" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mg59hlrvV9x-details-550"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">adversarial nlp, text classification, tcab, adversarial attack, dataset, benchmark, attack identification, attack detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce TCAB, a large-scale benchmark dataset for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce the Text Classification Attack Benchmark (TCAB), a dataset for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers. TCAB includes 1.5 million attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. Unlike standard text classification, text attacks must be understood in the context of the target classifier that is being attacked, and thus features of the target classifier are important as well.

        TCAB includes all attack instances that are successful in flipping the predicted label; a subset of the attacks are also labeled by human annotators to determine how frequently the primary semantics are preserved. The process of generating attacks is automated, so that TCAB can easily be extended to incorporate new text attacks and better classifiers as they are developed. In addition to the primary tasks of detecting and labeling attacks, TCAB can also be used for attack localization, attack target labeling, and attack characterization. TCAB code, dataset, and the leaderboard is available at https://react-nlp.github.io/tcab/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Mg59hlrvV9x&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://react-nlp.github.io/tcab/" target="_blank" rel="nofollow noreferrer">https://react-nlp.github.io/tcab/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://react-nlp.github.io/tcab/" target="_blank" rel="nofollow noreferrer">https://react-nlp.github.io/tcab/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released with a Creative Commons Attribution 4.0 International license.
        The code is released with an Apache-2.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="5xuowSQ17vy" data-number="449">
        <h4>
          <a href="/forum?id=5xuowSQ17vy">
              MTNeuro:  A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction
          </a>


            <a href="/pdf?id=5xuowSQ17vy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jorge_Quesada1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jorge_Quesada1">Jorge Quesada</a>, <a href="/profile?id=~Lakshmi_Sathidevi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lakshmi_Sathidevi1">Lakshmi Sathidevi</a>, <a href="/profile?id=~Ran_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ran_Liu2">Ran Liu</a>, <a href="/profile?id=~Nauman_Ahad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nauman_Ahad1">Nauman Ahad</a>, <a href="/profile?id=~Joy_M_Jackson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joy_M_Jackson1">Joy M Jackson</a>, <a href="/profile?id=~Mehdi_Azabou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mehdi_Azabou2">Mehdi Azabou</a>, <a href="/profile?id=~Jingyun_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingyun_Xiao1">Jingyun Xiao</a>, <a href="/profile?email=zliding3%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="zliding3@gatech.edu">Chris Liding</a>, <a href="/profile?email=curzay%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="curzay@gatech.edu">Carolina Urzay</a>, <a href="/profile?id=~William_Gray-Roncal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~William_Gray-Roncal1">William Gray-Roncal</a>, <a href="/profile?id=~Erik_Christopher_Johnson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Erik_Christopher_Johnson1">Erik Christopher Johnson</a>, <a href="/profile?id=~Eva_L_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eva_L_Dyer1">Eva L Dyer</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#5xuowSQ17vy-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5xuowSQ17vy-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, visual reasoning, neuroanatomy, semantic segmentation, multi-scale context aggregation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A multi-task, multi-scale benchmark for evaluating representations of volumetric brain data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There are multiple scales of abstraction from which we can describe the same image, depending on whether we are focusing on fine-grained details or a more global attribute of the image. In brain mapping, learning to automatically parse  images to build representations of both small-scale features (e.g., the presence of cells or blood vessels) and global properties of an image (e.g., source brain region) is a crucial and open challenge. However, most existing datasets and benchmarks for neuroanatomy consider only a single downstream task at a time. We introduce a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography imaging of a large thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions, that reveals dense reconstructions of the underlying microstructure (i.e., cell bodies, vasculature, and axons). We generated a number of different prediction challenges and evaluated several supervised and self-supervised models for brain-region prediction and pixel-level semantic segmentation of microstructures.  Our experiments not only highlight the rich heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations that  capture multiple attributes of a single image and perform well on a variety of downstream tasks.  Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=5xuowSQ17vy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://mtneuro.github.io/" target="_blank" rel="nofollow noreferrer">https://mtneuro.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://mtneuro.github.io/" target="_blank" rel="nofollow noreferrer">https://mtneuro.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released under the CC by 4.0 license (https://creativecommons.org/licenses/by/4.0/)
        The code is released under the MIT license (https://opensource.org/licenses/MIT)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mZke5vYdF99" data-number="448">
        <h4>
          <a href="/forum?id=mZke5vYdF99">
              OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics
          </a>


            <a href="/pdf?id=mZke5vYdF99" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mohit_Prabhushankar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mohit_Prabhushankar1">Mohit Prabhushankar</a>, <a href="/profile?id=~Kiran_Premdat_Kokilepersaud1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kiran_Premdat_Kokilepersaud1">Kiran Premdat Kokilepersaud</a>, <a href="/profile?id=~Yash-yee_Logan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yash-yee_Logan1">Yash-yee Logan</a>, <a href="/profile?id=~Stephanie_Trejo_Corona1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stephanie_Trejo_Corona1">Stephanie Trejo Corona</a>, <a href="/profile?id=~Ghassan_AlRegib1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ghassan_AlRegib1">Ghassan AlRegib</a>, <a href="/profile?email=ccwmd%40retinaconsultantstexas.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="ccwmd@retinaconsultantstexas.com">Charles Wykoff</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#mZke5vYdF99-details-770" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mZke5vYdF99-details-770"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Ophthamology datasets, Biomarker analysis, Treatment prediction, Self-supervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a first-of-its-kind dataset that combines clinical labels, biomarkers, fundus, OCT scans, for disease prediction, treatment analysis and biomarker detection. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 3 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mZke5vYdF99&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Dataset: https://doi.org/10.5281/zenodo.6622145, Benchmark: https://github.com/olivesgatech/OLIVES_Dataset </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset: https://doi.org/10.5281/zenodo.6622145
        Labels: https://www.dropbox.com/sh/ehw8pqhekgvka3d/AAA4E1n26pEblK7SSqiiSkpma?dl=0</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is released under Creative Commons Attribution 4.0 International.
        The code is released under MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="WCVjhq8I2M6" data-number="444">
        <h4>
          <a href="/forum?id=WCVjhq8I2M6">
              Generating Synthetic RGB-D Datasets for Texture-less Surfaces Reconstruction
          </a>


            <a href="/pdf?id=WCVjhq8I2M6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Muhammad_Saif_Ullah_Khan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhammad_Saif_Ullah_Khan1">Muhammad Saif Ullah Khan</a>, <a href="/profile?id=~Didier_Stricker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Didier_Stricker1">Didier Stricker</a>, <a href="/profile?id=~Muhammad_Zeshan_Afzal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Muhammad_Zeshan_Afzal1">Muhammad Zeshan Afzal</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#WCVjhq8I2M6-details-886" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WCVjhq8I2M6-details-886"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">3d reconstruction, textureless surfaces, dataset generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Presents a large RGB-D dataset containing 302k images with groundtruth depth and normal maps for texture-less surfaces reconstruction, and provides the generation method for further dataset generation.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Monocular 3D reconstruction is a major computer vision task. The state-of-the-art approaches mainly focus on datasets with highly textured images. Most of these methods are trained on datasets like ShapeNet which contain rendered images of well-textured objects. However, in real scenes, many objects are texture-less and it is difficult to reconstruct them. Unlike textured surfaces, reconstruction of texture-less surfaces has not received as much attention mainly because of a lack of large-scale annotated datasets. Some recent works have focused on texture-less surfaces as well, many of which are trained on a small real-world dataset containing 26k images of 5 different texture-less clothing items. To facilitate further research in this direction, we present a dataset generation strategy for texture-less images. We also make available a large dataset containing 302k images with corresponding groundtruth depth maps and surface normal maps. In addition to clothing items, our dataset also contains images of more everyday objects including animals, furniture, statues, vehicles, and other miscellaneous items. There are 35 different objects in total. This will enable future work in reconstructing a wider variety of texture-less surfaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=WCVjhq8I2M6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://projects.dfki.uni-kl.de/textureless_object_data/" target="_blank" rel="nofollow noreferrer">https://projects.dfki.uni-kl.de/textureless_object_data/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">MAIN DATASET
        - Landing Page: https://projects.dfki.uni-kl.de/textureless_object_data/
        Dataset can be downloaded using the appropriate download buttons on this page.

        SUPPLEMENTARY DATASET
        - Kaggle Repository: https://www.kaggle.com/saifkhichi96/textureless-real-data/
        Dataset can be directly downloaded from this page.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All data are released under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, and source code used for dataset generation, and code of data loaders, etc. is provided under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="duN8lWSjCQL" data-number="443">
        <h4>
          <a href="/forum?id=duN8lWSjCQL">
              AI-Olympics: A Multi-Agent Benchmark for Evaluation of Active Inference
          </a>


            <a href="/pdf?id=duN8lWSjCQL" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yan_Song5" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yan_Song5">Yan Song</a>, <a href="/profile?email=yahui.cui%40ia.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="yahui.cui@ia.ac.cn">YaHui Cui</a>, <a href="/profile?id=~Shu_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shu_Lin1">Shu Lin</a>, <a href="/profile?id=~Haifeng_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Haifeng_Zhang3">Haifeng Zhang</a>, <a href="/profile?id=~Jun_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jun_Wang2">Jun Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#duN8lWSjCQL-details-872" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="duN8lWSjCQL-details-872"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">active inference, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">a benchmark environment for potential active inference research</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">A biological organism tends to maintain a boundary that isolates its internal identity from the external environment. Current settings about artificial agents such as the ones in reinforcement learning, however, either (1) ignore such separation (e.g., model-free methods) or (2) consider the internal state observable and as an accurate reconstruction of the outside world (e.g., model-based methods). Therefore, it is of great importance to investigate the creation of statistical partitioning of internal and external states by adopting the Free Energy Principle and Active Inference, and explore whether such internal abstraction can help generalizing decision-making across multiple tasks. To evaluate such generalization ability of methods, we introduce AI-Olympics benchmark, a 2D physical simulator build from scratch along with multiple partially-observed multi-agent game scenarios. On top of these, we organized several AI-Olympics online competition events targeting various research topics such as multi-agent, multi-task, etc. According to the competition results, AI-Olympics benchmark demonstrates better evaluation of generalizability of various artificial agents. Our experiments also hints the poor learning efficiency of the current deep active inference methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=duN8lWSjCQL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/jidiai/olympics_engine" target="_blank" rel="nofollow noreferrer">https://github.com/jidiai/olympics_engine</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License

        Copyright (c) 2022 jidiai-olympics

        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LbOdQrnOb2q" data-number="441">
        <h4>
          <a href="/forum?id=LbOdQrnOb2q">
              Forecasting Future World Events With Neural Networks
          </a>


            <a href="/pdf?id=LbOdQrnOb2q" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Andy_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andy_Zou1">Andy Zou</a>, <a href="/profile?id=~Tristan_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tristan_Xiao1">Tristan Xiao</a>, <a href="/profile?id=~Ryan_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ryan_Jia1">Ryan Jia</a>, <a href="/profile?id=~Joe_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joe_Kwon1">Joe Kwon</a>, <a href="/profile?id=~Mantas_Mazeika3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mantas_Mazeika3">Mantas Mazeika</a>, <a href="/profile?id=~Richard_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Richard_Li2">Richard Li</a>, <a href="/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dawn_Song1">Dawn Song</a>, <a href="/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jacob_Steinhardt1">Jacob Steinhardt</a>, <a href="/profile?id=~Owain_Evans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Owain_Evans1">Owain Evans</a>, <a href="/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dan_Hendrycks1">Dan Hendrycks</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#LbOdQrnOb2q-details-799" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LbOdQrnOb2q-details-799"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Forecasting, Epistemics, Retrieval, Question Answering</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset for forecasting diverse future world events.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also include a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LbOdQrnOb2q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/andyzoujm/autocast" target="_blank" rel="nofollow noreferrer">https://github.com/andyzoujm/autocast</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="9gcweSoBE2" data-number="440">
        <h4>
          <a href="/forum?id=9gcweSoBE2">
              Evaluating histopathology transfer learning with ChampKit
          </a>


            <a href="/pdf?id=9gcweSoBE2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jakub_R._Kaczmarzyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jakub_R._Kaczmarzyk1">Jakub R. Kaczmarzyk</a>, <a href="/profile?id=~Tahsin_Kurc1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tahsin_Kurc1">Tahsin Kurc</a>, <a href="/profile?id=~Shahira_Abousamra2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shahira_Abousamra2">Shahira Abousamra</a>, <a href="/profile?id=~Rajarsi_R._Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rajarsi_R._Gupta1">Rajarsi R. Gupta</a>, <a href="/profile?id=~Joel_Saltz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Joel_Saltz1">Joel Saltz</a>, <a href="/profile?id=~Peter_K_Koo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peter_K_Koo1">Peter K Koo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#9gcweSoBE2-details-401" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9gcweSoBE2-details-401"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">histopathology, transfer learning, benchmarks, pretraining, cancer</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">ChampKit is a reproducible benchmarking toolkit for patch-based histopathology image classification.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Histopathology remains the gold standard for diagnosis of various cancers. Recent advances in computer vision, specifically deep learning, have facilitated the analysis of histopathology images for various tasks, including immune cell detection and microsatellite instability classification. The state-of-the-art for each task often employs base architectures that have been pretrained for image classification on ImageNet. The standard approach to develop classifiers in histopathology tends to  focus narrowly on optimizing models for a single task, not considering the aspects of modeling innovations that improve generalization across tasks. Here we present ChampKit (Comprehensive Histopathology Assessment of Model Predictions toolKit): an extensible, fully reproducible benchmarking toolkit that consists of a broad collection of patch-level image classification tasks across different cancers. ChampKit enables a way to systematically document the performance impact of proposed improvements in models and methodology. ChampKit source code and data are freely accessible at https://github.com/kaczmarj/champkit.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/kaczmarj/champkit" target="_blank" rel="nofollow noreferrer">https://github.com/kaczmarj/champkit</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code related to this submission (https://github.com/kaczmarj/champkit) is licensed under Apache-2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="My5AI9aM49R" data-number="439">
        <h4>
          <a href="/forum?id=My5AI9aM49R">
              Video compression dataset and benchmark of learning-based video-quality metrics
          </a>


            <a href="/pdf?id=My5AI9aM49R" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Anastasia_Antsiferova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anastasia_Antsiferova1">Anastasia Antsiferova</a>, <a href="/profile?id=~Sergey_Lavrushkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sergey_Lavrushkin1">Sergey Lavrushkin</a>, <a href="/profile?id=~Maksim_Smirnov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maksim_Smirnov1">Maksim Smirnov</a>, <a href="/profile?id=~Aleksandr_Gushchin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Aleksandr_Gushchin1">Aleksandr Gushchin</a>, <a href="/profile?id=~Dmitriy_Vatolin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Dmitriy_Vatolin1">Dmitriy Vatolin</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#My5AI9aM49R-details-91" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="My5AI9aM49R-details-91"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Video-quality measurement is one of the most important tasks in video processing. Nowadays, many implementations of new encoding standards, such as AV1 and LCEVC, use deep-learning-based decoding algorithms with perceptual metrics serving as optimization objectives. However, the performance of modern video and image quality metrics is commonly investigated using videos, compressed with older standards, such as AVC. In this paper, we present a new benchmark of video-quality metrics that evaluates various video compression. The benchmark is based on a new dataset consisting of over 1,500 streams encoded using different standards, including AVC, HEVC, AV1, VP9, VVC and others. Subjective scores were collected using crowd-sourced pairwise comparisons. The list of evaluated metrics includes new methods based on machine learning and neural networks. The results show that new no-reference metrics have high correlation with subjective quality and are close to top full-reference metrics.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=My5AI9aM49R&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://videoprocessing.ai/benchmarks/video-quality-metrics.html" target="_blank" rel="nofollow noreferrer">https://videoprocessing.ai/benchmarks/video-quality-metrics.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Link: https://calypso.gml-team.ru:5001/sharing/lxSWi6vtg
        Password: c943=R3/tJwVV%P%</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="CZAd_6uiUx0" data-number="437">
        <h4>
          <a href="/forum?id=CZAd_6uiUx0">
              This is the way - lessons learned from designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish
          </a>


            <a href="/pdf?id=CZAd_6uiUx0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lukasz_Augustyniak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lukasz_Augustyniak1">Lukasz Augustyniak</a>, <a href="/profile?id=~Kamil_Tagowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kamil_Tagowski1">Kamil Tagowski</a>, <a href="/profile?email=albert.sawczyn%40pwr.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="albert.sawczyn@pwr.edu.pl">Albert Sawczyn</a>, <a href="/profile?id=~Denis_Janiak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Denis_Janiak1">Denis Janiak</a>, <a href="/profile?email=roman.bartusiak%40pwr.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="roman.bartusiak@pwr.edu.pl">Roman Bartusiak</a>, <a href="/profile?id=~Adrian_Dominik_Szymczak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adrian_Dominik_Szymczak1">Adrian Dominik Szymczak</a>, <a href="/profile?email=arkadiusz.janz%40pwr.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="arkadiusz.janz@pwr.edu.pl">Arkadiusz Janz</a>, <a href="/profile?id=~Piotr_Szyma%C5%84ski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Piotr_Szymański1">Piotr Szymański</a>, <a href="/profile?email=marcin.watroba%40pwr.edu.pl" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="marcin.watroba@pwr.edu.pl">Marcin Wątroba</a>, <a href="/profile?id=~Miko%C5%82aj_Morzy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mikołaj_Morzy1">Mikołaj Morzy</a>, <a href="/profile?id=~Tomasz_Jan_Kajdanowicz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tomasz_Jan_Kajdanowicz1">Tomasz Jan Kajdanowicz</a>, <a href="/profile?id=~Maciej_Piasecki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Maciej_Piasecki1">Maciej Piasecki</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#CZAd_6uiUx0-details-381" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CZAd_6uiUx0-details-381"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmark, leaderboard, NLP benchmarking, Polish language</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper we introduce LEPISZCZE (lepiszczeis the Polish word for glew, the Middle English predecessor of glue) a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a \emph{de facto} standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in the field of benchmarking for under-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.

        In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glue) a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark. We design LEPISZCZE with flexibility in mind. The inclusion of new models, datasets, and tasks are as simple as possible, while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the main contribution of the paper, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other under-resourced languages.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/CLARIN-PL/LEPISZCZE" target="_blank" rel="nofollow noreferrer">https://github.com/CLARIN-PL/LEPISZCZE</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Licenses are provided in the HuggingFace dataset cards either in GitHub repositories. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Xm2a_9YaJQ" data-number="436">
        <h4>
          <a href="/forum?id=Xm2a_9YaJQ">
              Anomaly Detection and Inter-Sensor Transfer Learning on Smart Manufacturing Datasets
          </a>


            <a href="/pdf?id=Xm2a_9YaJQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mustafa_Abdallah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mustafa_Abdallah1">Mustafa Abdallah</a>, <a href="/profile?email=bjoung%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="bjoung@purdue.edu">Byung-Gun Joung</a>, <a href="/profile?email=wojaelee%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="wojaelee@gmail.com">Wo Jae Lee</a>, <a href="/profile?email=cmousoul%40purdue.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="cmousoul@purdue.edu">Charilaos Mousoulis</a>, <a href="/profile?id=~John_Sutherland1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~John_Sutherland1">John Sutherland</a>, <a href="/profile?id=~Saurabh_Bagchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saurabh_Bagchi1">Saurabh Bagchi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Xm2a_9YaJQ-details-629" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xm2a_9YaJQ-details-629"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Smart Manufacturing, Anomaly Detection, Transfer Learning, Defect Type Classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We perform anomaly detection and defect type classification on real datasets for enhancing predictive maintenance in smart manufacturing systems.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Smart manufacturing systems are being deployed at a growing rate because of their ability to interpret a wide variety of sensed information and act on the knowledge gleaned from system observations. In many cases, the principal goal of the smart manufacturing system is to rapidly detect (or anticipate) failures to reduce operational cost and eliminate downtime. This often boils down to detecting anomalies within the sensor date acquired from the system. The smart manufacturing application domain poses certain salient technical challenges. In particular, there are often multiple types of sensors with varying capabilities and costs. The sensor data characteristics change with the operating point of the environment or machines, such as, the RPM of the motor. The anomaly detection process therefore has to be calibrated near an operating point. In this paper, we analyze four datasets from sensors deployed from manufacturing testbeds. We evaluate the performance of several traditional and ML-based forecasting models for predicting the time series of sensor data. Then, considering the sparse data from one kind of sensor, we perform transfer learning from a high data rate sensor to perform defect type classification. Taken together, we show that predictive failure classification can be achieved, thus paving the way for predictive maintenance. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Xm2a_9YaJQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/drive/u/2/folders/1QX3chnSTKO3PsEhi5kBdf9WwMBmOriJ8" target="_blank" rel="nofollow noreferrer">https://drive.google.com/drive/u/2/folders/1QX3chnSTKO3PsEhi5kBdf9WwMBmOriJ8</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://drive.google.com/drive/u/2/folders/1QX3chnSTKO3PsEhi5kBdf9WwMBmOriJ8

        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We chose a creative common Zero 1.0 Universal license for the GitHub repository. We will use that GitHub repository to update the code and datasets and enhance them.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="dOLkgunW5A6" data-number="435">
        <h4>
          <a href="/forum?id=dOLkgunW5A6">
              Benchmarking Multimodal Variational Autoencoders: GeBiD Dataset and Toolkit
          </a>


            <a href="/pdf?id=dOLkgunW5A6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Gabriela_Sejnova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gabriela_Sejnova1">Gabriela Sejnova</a>, <a href="/profile?id=~Michal_Vavrecka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michal_Vavrecka1">Michal Vavrecka</a>, <a href="/profile?id=~Karla_Stepanova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karla_Stepanova1">Karla Stepanova</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#dOLkgunW5A6-details-3" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dOLkgunW5A6-details-3"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Generative Models, Benchmarks, Latent Variable Models, Representation Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a toolkit and a dataset for systematic evaluation, comparison and development of Multimodal Variational Autoencoders.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Multimodal Variational Autoencoders (VAEs) have been a subject of intense research in the past years as they can integrate multiple modalities into a joint representation and can thus serve as a promising tool for both data classification and generation. Several approaches toward multimodal VAE learning have been proposed so far, their comparison and evaluation have however been rather inconsistent. One reason is that the models differ at the implementation level, another problem is that the datasets commonly used in these cases were not initially designed for the evaluation of multimodal generative models. This paper addresses both mentioned issues. First, we propose a toolkit for systematic multimodal VAE training and comparison. Second, we present a synthetic bimodal dataset designed for a comprehensive evaluation of the joint generation and cross-generation capabilities. We demonstrate the utility of the dataset by comparing selected state-of-the-art models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=dOLkgunW5A6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/gabinsane/multimodal-vae-comparison" target="_blank" rel="nofollow noreferrer">https://github.com/gabinsane/multimodal-vae-comparison</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/gabinsane/multimodal-vae-comparison" target="_blank" rel="nofollow noreferrer">https://github.com/gabinsane/multimodal-vae-comparison</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The provided dataset and supplementary code are copyrighted by us and published under the CC BY-NC-SA 4.0 license\footnote{\url{https://creativecommons.org/licenses/by-nc-sa/4.0/}}.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="DEigo9L8xZA" data-number="434">
        <h4>
          <a href="/forum?id=DEigo9L8xZA">
              Free High-Resolution Satellite Imagery: The WorldStrat Dataset
          </a>


            <a href="/pdf?id=DEigo9L8xZA" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Julien_Cornebise1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julien_Cornebise1">Julien Cornebise</a>, <a href="/profile?id=~Ivan_Orsolic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ivan_Orsolic1">Ivan Orsolic</a>, <a href="/profile?id=~Freddie_Kalaitzis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Freddie_Kalaitzis1">Freddie Kalaitzis</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#DEigo9L8xZA-details-169" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DEigo9L8xZA-details-169"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">satellite imagery, land use, transfer learning, multi-resolution, stratification, high-resolution</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Largest high-resolution satellite imagery dataset, stratified to cover all land uses and urbanisation along with humanitarian uses, paired with lower-resolution, to best represent the planet.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the  WorldStratified dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate 10,000 sq km of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel.
        We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox.
        We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution.
        License-wise, the high-resolution Airbus imagery is CC-BY-NC, while the labels, Sentinel2 imagery, and trained weights are under CC-BY, and the source code under BSD, to allow for the widest use and dissemination.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=DEigo9L8xZA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Private URL sent to the reviewers in official note. Public URL coming next week on Zenodo.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset will be released on Zenodo within a few days of submission to NeurIPS.
        Meanwhile, a temporary view via private Google Drive is being sent to the reviewers via private note.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">License-wise, the high-resolution Airbus imagery is CC-BY-NC, while the labels, Sentinel2 imagery, and trained weights are under CC-BY, and the source code under BSD, to allow for the widest use and dissemination.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="sOHZsPRfz5f" data-number="433">
        <h4>
          <a href="/forum?id=sOHZsPRfz5f">
              Fraud Dataset Benchmark for Automated Machine Learning Pipelines
          </a>


            <a href="/pdf?id=sOHZsPRfz5f" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Prince_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Prince_Grover1">Prince Grover</a>, <a href="/profile?id=~Zheng_Li15" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zheng_Li15">Zheng Li</a>, <a href="/profile?id=~Jianbo_Liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jianbo_Liu4">Jianbo Liu</a>, <a href="/profile?id=~Jakub_Zablocki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jakub_Zablocki1">Jakub Zablocki</a>, <a href="/profile?id=~Hao_Zhou12" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hao_Zhou12">Hao Zhou</a>, <a href="/profile?id=~Julia_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Julia_Xu2">Julia Xu</a>, <a href="/profile?id=~Anqi_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Anqi_Cheng1">Anqi Cheng</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#sOHZsPRfz5f-details-539" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sOHZsPRfz5f-details-539"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fraud detection, benchmark, automl, amazon fraud detector, autogluon, h2o</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Fraud Dataset Benchmark is a compilation of publicly available datasets relevant to fraud detection, with a Python tool that standardizes datasets to maintain reproducibility. We analyze the performance of 3 AutoML frameworks on this benchmark.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Automated Machine Learning (AutoML) makes it easier and faster to build machine learning models. However, generic AutoML solutions may not work well for fraud dataset as fraud detection often requires specialized feature engineering, sampling, and learning to achieve the best performance. Fraud detection datasets are often different than datasets in other classification tasks in terms of a large class imbalance between fraud to legit population, number, type and granularity of features available for modeling (IP address, personal identifiable information, event timestamp, categorical, free form text and numeric features), frequently changing fraud patterns, and adversarial nature of the problem. The Fraud Dataset Benchmark (FDB) is a compilation of publicly available datasets relevant to fraud detection. The FDB aims to cover a wide variety of fraud detection tasks, ranging from card not present transaction fraud, bot attacks, malicious traffic, loan risk and content moderation. The Python based data loaders from FDB provide dataset loading, standardized train-test splits and performance evaluation metrics. The goal of our work is to provide researchers working in the field of fraud and abuse detection a standardized set of benchmarking datasets and evaluation tools for their experiments. Using FDB tools we evaluate 3 AutoML pipelines including AutoGluon, H2O and Amazon Fraud Detector across 9 different fraud detection datasets and discuss the results.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=sOHZsPRfz5f&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">https://github.com/groverpr/fraud_dataset_benchmark (Currently, the repository is private. Reviewers can either request access to the repository by providing their GitHub username, or we can send offline zip file of codes, if needed. We plan to make the source code repository public once the internal legal review is finalized). </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/groverpr/fraud_dataset_benchmark

        The landing page of this Url contains information on how to access the codes that will pull datasets from the source and modify on the fly using Python. Currently, the repository is private and will go through a review from our organizations's IP/legal team. As soon as the review is finalized, we will make it public. If the repository is not public when the conference reviewers start, we request the reviewers to provide their GitHub username that we can use in order to provide access to the repository. If the reviewers can not share their GitHub username for any reason, we can also provide offline zip file of the GitHub repo containing the codes. Please let us know accordingly. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We plan to make the code to pull and standardize datasets public after the legal review from our organization. Our goal is to complete the legal review process before conference reviews start. For any reason, if the repository is still private when conference reviews start, we have provided instructions to give reviewers access to the repository.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT-0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="YVPdQJRRTK4" data-number="432">
        <h4>
          <a href="/forum?id=YVPdQJRRTK4">
              PorTis: A biological phenotype classification dataset for benchmarking nanopore signal-based protein analysis
          </a>


            <a href="/pdf?id=YVPdQJRRTK4" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Cailin_Winston1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cailin_Winston1">Cailin Winston</a>, <a href="/profile?id=~Marc_Exp%C3%B2sit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marc_Expòsit1">Marc Expòsit</a>, <a href="/profile?id=~Jeffrey_Nivala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jeffrey_Nivala1">Jeffrey Nivala</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#YVPdQJRRTK4-details-122" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YVPdQJRRTK4-details-122"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The phenotype of a biological tissue is dependent upon a composite of many differentially expressed protein molecules, and characterizing this complex protein assortment provides insights for medical diagnostics and biological discovery. One emerging technology that can potentially provide a highly sensitive and scalable solution for protein detection are nanopore sensor devices. The conventional approach to nanopore protein detection has been to empirically model the ionic current time series signals associated with individual protein types since they cannot currently be determined a priori. This setting is limited because new data must be collected and a new model trained to detect every different type of protein that may contribute to a complex biological phenotype, such as a disease state. In this work, we take a protein identification-agnostic approach that is inspired by natural biological sensing mechanisms, such as the gustatory system (sense of taste), which uses a limited set of sensors to detect and classify highly complex molecular mixtures by passing a plurality of individual signals through neural networks for classification. To study and benchmark this new approach for protein-based tissue phenotype classification using nanopore sensor arrays, we introduce PorTis, a dataset composed of over two million individual nanopore signal events collected from proteomic samples derived from three different human tissue types. We explore various clustering, deep learning, and sample batching methods and provide classification accuracies using deep learning models. These results lay out a collection of useful benchmarks for approaches that combine machine learning-based classification methods with noisy nanopore sensor time series data and provide a foundational dataset for further developments.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=YVPdQJRRTK4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6654078" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6654078</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset was released under the Creative Commons Attribution Non Commercial Share Alike 4.0 International license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="LkAFwrqdRY6" data-number="430">
        <h4>
          <a href="/forum?id=LkAFwrqdRY6">
              FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning
          </a>


            <a href="/pdf?id=LkAFwrqdRY6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xiao-Yang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Xiao-Yang_Liu1">Xiao-Yang Liu</a>, <a href="/profile?id=~Ziyi_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ziyi_Xia1">Ziyi Xia</a>, <a href="/profile?email=ayrui%40connect.hku.hk" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="ayrui@connect.hku.hk">Jingyang Rui</a>, <a href="/profile?id=~Jiechao_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiechao_Gao1">Jiechao Gao</a>, <a href="/profile?email=hy2500%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="hy2500@columbia.edu">Hongyang Yang</a>, <a href="/profile?id=~Ming_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ming_Zhu2">Ming Zhu</a>, <a href="/profile?email=christina.wang%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="christina.wang@nyu.edu">Christina Dan Wang</a>, <a href="/profile?id=~Zhaoran_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhaoran_Wang1">Zhaoran Wang</a>, <a href="/profile?id=~Jian_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jian_Guo2">Jian Guo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#LkAFwrqdRY6-details-576" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LkAFwrqdRY6-details-576"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Market environments, financial reinforcement learning, data-driven machine learning, finance benchmarks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A collection of market environments and benchmarks for data-driven financial reinforcement learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Finance is a particularly difficult playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks on financial reinforcement learning are challenging due to three major factors, namely,  low signal-to-noise ratio of financial data, survivorship bias of historical data, and information leakage in the backtesting stage. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the FinRL community. First, following a DataOps paradigm, we provide hundreds of market environments through an automatic pipeline that collects dynamic datasets from real-world markets and processes them into standard gym-style market environments. Second, we benchmark popular papers as stepping stones for users to design new trading strategies. We also hold our benchmarks on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized in a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: \url{https://github.com/AI4Finance-Foundation/FinRL-Meta}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=LkAFwrqdRY6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/AI4Finance-Foundation/FinRL-Meta" target="_blank" rel="nofollow noreferrer">https://github.com/AI4Finance-Foundation/FinRL-Meta</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://finrl.readthedocs.io/en/latest/finrl_meta/Data_layer.html" target="_blank" rel="nofollow noreferrer">https://finrl.readthedocs.io/en/latest/finrl_meta/Data_layer.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="uDlkiCI5N7Y" data-number="429">
        <h4>
          <a href="/forum?id=uDlkiCI5N7Y">
              Evaluating Out-of-Distribution Performance on Document Image Classifiers
          </a>


            <a href="/pdf?id=uDlkiCI5N7Y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Stefan_Larson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Stefan_Larson1">Stefan Larson</a>, <a href="/profile?id=~Gordon_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gordon_Lim1">Gordon Lim</a>, <a href="/profile?email=ellenai%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="ellenai@umich.edu">Yutong Ai</a>, <a href="/profile?email=dakuang%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="dakuang@umich.edu">David Kuang</a>, <a href="/profile?id=~Kevin_Leach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kevin_Leach1">Kevin Leach</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#uDlkiCI5N7Y-details-77" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uDlkiCI5N7Y-details-77"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">document classification, RVL-CDIP, out-of-distribution</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Our paper introduces new out-of-distribution data for evaluating document classifiers, and finds that models trained on RVL-CDIP but tested on our new out-of-distribution data tend to underperform.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The ability of a document classifier to handle inputs that are drawn from a distribution different from the training distribution is crucial for robust deployment and generalizability. The RVL-CDIP corpus is the de facto standard benchmark for document classification, yet to our knowledge all studies that use this corpus do not include evaluation on out-of-distribution documents. In this paper, we curate and release a new out-of-distribution benchmark for evaluating out-of-distribution performance for document classifiers. Our new out-of-distribution benchmark consists of two types of documents: those that are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-N), and those that are one of the 16 in-domain categories yet are drawn from a distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-O). While prior work on document classification for in-domain RVL-CDIP documents reports high accuracy scores, we find that these models exhibit accuracy drops of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark. Our new benchmark researchers with a valuable new resource for analyzing out-of-distribution performance on document classifiers.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://tinyurl.com/4he6my23" target="_blank" rel="nofollow noreferrer">https://tinyurl.com/4he6my23</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://tinyurl.com/4he6my23" target="_blank" rel="nofollow noreferrer">https://tinyurl.com/4he6my23</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 3.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="IC8VP2iJ5PR" data-number="426">
        <h4>
          <a href="/forum?id=IC8VP2iJ5PR">
              Measuring Data
          </a>


            <a href="/pdf?id=IC8VP2iJ5PR" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Margaret_Mitchell3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Margaret_Mitchell3">Margaret Mitchell</a>, <a href="/profile?id=~Yacine_Jernite1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yacine_Jernite1">Yacine Jernite</a>, <a href="/profile?id=~Marissa_Gerchick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marissa_Gerchick1">Marissa Gerchick</a>, <a href="/profile?id=~Angelina_McMillan-Major1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Angelina_McMillan-Major1">Angelina McMillan-Major</a>, <a href="/profile?id=~Ezinwanne_Ozoani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ezinwanne_Ozoani1">Ezinwanne Ozoani</a>, <a href="/profile?id=~Nazneen_Rajani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nazneen_Rajani1">Nazneen Rajani</a>, <a href="/profile?id=~Tristan_Thrush1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tristan_Thrush1">Tristan Thrush</a>, <a href="/profile?id=~Sasha_Luccioni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sasha_Luccioni1">Sasha Luccioni</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#IC8VP2iJ5PR-details-929" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IC8VP2iJ5PR-details-929"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">data, datasets, measurements, metrics</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Data can be "measured" as part of Responsible AI work.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We describe a well-known term and corresponding terminology to name the quantification of dataset properties. Similar to an item's height, width, and volume, dataset measurements are the amounts present in a dataset, which can be determined by ``measuring'' the data in the dataset. In what follows, we draw from previous literature to understand a distinction that has eluded shared understanding in Machine Learning data work, in order to trace out a common ground. The grounding in this distinction narrows in on a task that has been suggested through different perspectives on how to understand and analyze data in the machine learning pipeline. This task can be simply referred to as ``measuring data’’.    </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=IC8VP2iJ5PR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="dwi57JI_-K" data-number="425">
        <h4>
          <a href="/forum?id=dwi57JI_-K">
              SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles
          </a>


            <a href="/pdf?id=dwi57JI_-K" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chejian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Chejian_Xu1">Chejian Xu</a>, <a href="/profile?id=~Wenhao_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Wenhao_Ding1">Wenhao Ding</a>, <a href="/profile?id=~Weijie_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Weijie_Lyu2">Weijie Lyu</a>, <a href="/profile?id=~Zuxin_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zuxin_Liu1">Zuxin Liu</a>, <a href="/profile?id=~Shuai_Wang15" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Shuai_Wang15">Shuai Wang</a>, <a href="/profile?id=~Yihan_He2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Yihan_He2">Yihan He</a>, <a href="/profile?id=~Hanjiang_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hanjiang_Hu1">Hanjiang Hu</a>, <a href="/profile?id=~Ding_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ding_Zhao1">Ding Zhao</a>, <a href="/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Bo_Li19">Bo Li</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#dwi57JI_-K-details-362" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dwi57JI_-K-details-362"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">safety benchmarking platform, autonomous driving, safety-critical scenarios</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the first unified platform SafeBench to effectively and efficiently evaluate autonomous driving algorithms against different types of safety-critical testing scenarios.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As shown by recent studies, machine intelligence-enabled systems are vulnerable to test cases resulting from either adversarial manipulation or natural distribution shifts. This has raised great concerns about deploying machine learning algorithms for real-world applications, especially in the safety-critical domains such as autonomous driving (AD). On the other hand, traditional AD testing on naturalistic scenarios requires hundreds of millions of driving miles due to the high dimensionality and rareness of the safety-critical scenarios in the real world. As a result, several approaches for autonomous driving evaluation have been explored, which are usually, however, based on different simulation platforms, types of safety-critical scenarios, scenario generation algorithms, and driving route variations. Thus, despite a large amount of effort in autonomous driving testing, it is still challenging to compare and understand the effectiveness and efficiency of different testing scenario generation algorithms and testing mechanisms under similar conditions. In this paper, we aim to provide the first unified platform SafeBench to integrate different types of safety-critical testing scenarios, scenario generation algorithms, and other variations such as driving routes and environments. In particular, we consider 8 safety-critical testing scenarios following National Highway Traffic Safety Administration (NHTSA) and develop 4 scenario generation algorithms considering 10 variations for each scenario. Meanwhile, we implement 4 deep reinforcement learning-based AD algorithms with 4 types of input (e.g., bird’s-eye view, camera) to perform fair comparisons on SafeBench. We find our generated testing scenarios are indeed more challenging and observe the trade-off between the performance of AD agents under benign and safety-critical testing scenarios. We believe our unified platform SafeBench for large-scale and effective autonomous driving testing will motivate the development of new testing scenario generation and safe AD algorithms. SafeBench is available at https://safebench.github.io.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=dwi57JI_-K&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://safebench.github.io" target="_blank" rel="nofollow noreferrer">https://safebench.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://safebench.github.io" target="_blank" rel="nofollow noreferrer">https://safebench.github.io</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our work will be released under the CC BY-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rbrouCKPiej" data-number="424">
        <h4>
          <a href="/forum?id=rbrouCKPiej">
              AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection
          </a>


            <a href="/pdf?id=rbrouCKPiej" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Marius_Dragoi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Marius_Dragoi1">Marius Dragoi</a>, <a href="/profile?id=~Elena_Burceanu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Elena_Burceanu1">Elena Burceanu</a>, <a href="/profile?id=~Emanuela_Haller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Emanuela_Haller1">Emanuela Haller</a>, <a href="/profile?id=~Andrei_Manolache1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andrei_Manolache1">Andrei Manolache</a>, <a href="/profile?id=~Florin_Brad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Florin_Brad1">Florin Brad</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#rbrouCKPiej-details-279" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rbrouCKPiej-details-279"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">non-stationary, distribution shift, anomaly, unsupervised, network intrusion detection, network traffic, natural distribution shifts</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Analysing and introducing a benchmark for Unsupervised Anomaly Detection on network traffic data, proposing chronological testing splits, emphasising the gradually distribution shift over time.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning, leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This kind of data meets the premise of shifting the input distribution: it covers a large time span (10 years), with naturally occurring changes over time (e.g. users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models (MLM to classical Isolation Forest). Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical IID training (by up to 3%, on average). Dataset and code are available at https://github.com/bit-ml/AnoShift/.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rbrouCKPiej&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/bit-ml/AnoShift/" target="_blank" rel="nofollow noreferrer">https://github.com/bit-ml/AnoShift/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/bit-ml/AnoShift/" target="_blank" rel="nofollow noreferrer">https://github.com/bit-ml/AnoShift/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">BSD 3-Clause License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1kIZiRelqFt" data-number="423">
        <h4>
          <a href="/forum?id=1kIZiRelqFt">
              FLAIR: Federated Learning Annotated Image Repository
          </a>


            <a href="/pdf?id=1kIZiRelqFt" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Congzheng_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Congzheng_Song2">Congzheng Song</a>, <a href="/profile?id=~Filip_Granqvist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Filip_Granqvist1">Filip Granqvist</a>, <a href="/profile?id=~Kunal_Talwar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kunal_Talwar1">Kunal Talwar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#1kIZiRelqFt-details-160" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1kIZiRelqFt-details-160"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Federated Learning, Differential Privacy, Image Classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper describes the FLAIR dataset that we are releasing later this month to accelerate research in Federated Learning. This is a large image dataset that is heterogenous, with images grouped by Flicker users and annotated by human.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Cross-device federated learning is an emerging machine learning (ML) paradigm where a large population of devices collectively train an ML model while the data remains on the devices.
        This research field has a unique set of practical challenges, and to systematically make advances, new datasets curated to be compatible with this paradigm are needed.
        Existing federated learning benchmarks in the image domain do not accurately capture the scale and heterogeneity of many real-world use cases.
        We introduce FLAIR, a challenging large-scale annotated image dataset for multi-label classification suitable for federated learning.
        FLAIR has 429,078 images from  51,414  Flickr users and captures many of the intricacies typically encountered in federated learning, such as heterogeneous user data and a long-tailed label distribution.
        We implement multiple baselines in different learning setups for different tasks on this dataset.
        We believe FLAIR can serve as a challenging benchmark for advancing the state-of-the art in federated learning.
        Dataset access and the code for the benchmark are available at https://github.com/apple/ml-flair.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1kIZiRelqFt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/apple/ml-flair" target="_blank" rel="nofollow noreferrer">https://github.com/apple/ml-flair</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/apple/ml-flair" target="_blank" rel="nofollow noreferrer">https://github.com/apple/ml-flair</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Copyright (C) 2020 Apple Inc. All Rights Reserved.

        IMPORTANT: This Apple software is supplied to you by Apple Inc. ("Apple") in consideration of your agreement to the following terms, and your use, installation, modification or redistribution of this Apple software constitutes acceptance of these terms. If you do not agree with these terms, please do not use, install, modify or redistribute this Apple software.

        In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the "Apple Software"), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.

        The Apple Software is provided by Apple on an "AS IS" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.

        IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="DzJQ3r7EnTQ" data-number="422">
        <h4>
          <a href="/forum?id=DzJQ3r7EnTQ">
              In Search of Out-of-Distribution Generalization on Graphs
          </a>


            <a href="/pdf?id=DzJQ3r7EnTQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mucong_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mucong_Ding1">Mucong Ding</a>, <a href="/profile?id=~Kezhi_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kezhi_Kong1">Kezhi Kong</a>, <a href="/profile?id=~Jiuhai_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiuhai_Chen1">Jiuhai Chen</a>, <a href="/profile?id=~John_Kirchenbauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~John_Kirchenbauer1">John Kirchenbauer</a>, <a href="/profile?id=~Micah_Goldblum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Micah_Goldblum1">Micah Goldblum</a>, <a href="/profile?id=~David_Wipf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Wipf1">David Wipf</a>, <a href="/profile?id=~Furong_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Furong_Huang1">Furong Huang</a>, <a href="/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tom_Goldstein1">Tom Goldstein</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#DzJQ3r7EnTQ-details-436" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DzJQ3r7EnTQ-details-436"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Domain Generalization, Distribution Shifts</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We curate GDS, a graph-classification benchmark of eight datasets reflecting a wide variety of distribution shifts across graphs.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distribution shifts, in which the training distribution differs from the testing distribution, can significantly degrade the performance of Graph Neural Networks (GNNs). Although some existing graph classification benchmarks take distribution shifts into account, we are far from understanding the impact of distribution shifts on graphs and how they differ from distribution shifts in tensor data such as images. We ask: (1) To what extent are tensorial domain generalization methods applicable to addressing distribution shifts on graph data? (2) Can GNNs generalize to test graphs from unseen distributions? In this paper, we curate GDS, a graph classification benchmark of eight datasets reflecting a wide variety of distribution shifts across graphs. We observe that in most cases, we need both a suitable domain generalization algorithm and a robust GNN backbone model to optimize out-of-distribution test performance. However, even with a careful selection of such model-algorithm combinations, the out-of-distribution performance remains significantly poorer than the in-distribution performance. These large performance gaps underscore the need for graph-specific domain generalization techniques and robust GNNs that generalize effectively to out-of-distribution graphs. To facilitate further research, we provide an open-source package that administers the GDS benchmark with modular combinations of popular domain generalization algorithms and GNN backbone models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=DzJQ3r7EnTQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/johnding1996/Graph-Distribution-Shift" target="_blank" rel="nofollow noreferrer">https://github.com/johnding1996/Graph-Distribution-Shift</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/johnding1996/Graph-Distribution-Shift" target="_blank" rel="nofollow noreferrer">https://github.com/johnding1996/Graph-Distribution-Shift</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="AnXBo3P8y8z" data-number="421">
        <h4>
          <a href="/forum?id=AnXBo3P8y8z">
              Towards Understanding How Machines Can Learn Causal Overhypotheses
          </a>


            <a href="/pdf?id=AnXBo3P8y8z" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eliza_Kosoy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eliza_Kosoy1">Eliza Kosoy</a>, <a href="/profile?id=~David_Chan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Chan3">David Chan</a>, <a href="/profile?id=~Adrian_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adrian_Liu1">Adrian Liu</a>, <a href="/profile?id=~Jasmine_Collins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jasmine_Collins1">Jasmine Collins</a>, <a href="/profile?email=bryannakaufmann%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="bryannakaufmann@berkeley.edu">Bryanna Kaufmann</a>, <a href="/profile?id=~Sandy_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sandy_Huang1">Sandy Huang</a>, <a href="/profile?id=~Jessica_B_Hamrick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jessica_B_Hamrick1">Jessica B Hamrick</a>, <a href="/profile?id=~John_Canny1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~John_Canny1">John Canny</a>, <a href="/profile?id=~Nan_Rosemary_Ke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nan_Rosemary_Ke1">Nan Rosemary Ke</a>, <a href="/profile?id=~Alison_Gopnik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Alison_Gopnik1">Alison Gopnik</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#AnXBo3P8y8z-details-773" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AnXBo3P8y8z-details-773"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">causal reasoning, intervention, causal overhypotheses, Reinforcement learning, gpt-3</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a new benchmark, a flexible environment which allows for the evaluation of existing techniques under variable causal overhypotheses</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent work in machine learning and cognitive science has suggested that understanding causal information is essential to the development of intelligence. The extensive literature in cognitive science using the "Blicket detector" environment shows that children are adept at many kinds of causal inference and learning. We propose to adapt that environment to machine learning agents. In particular, one of the key challenges for current machine learning algorithms is modeling and understanding causal overhypotheses: transferable abstract hypotheses about sets of causal relationships. In contrast, even young children spontaneously learn causal overhypotheses. In this work, we present a new benchmark---a flexible environment which allows for the evaluation of existing techniques under variable causal overhypotheses---and demonstrate that many existing state-of-the-art methods have trouble generalizing in this environment.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=AnXBo3P8y8z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/CannyLab/causal_overhypotheses" target="_blank" rel="nofollow noreferrer">https://github.com/CannyLab/causal_overhypotheses</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A (No New Data)</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">N/A (No new data)</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A (Baseline is currently available)</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Primary License: MIT, Regents of the University of California
        Some subset of the code is licensed under MIT by the authors of https://arxiv.org/abs/2106.01345
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="srHMs3mPD5y" data-number="420">
        <h4>
          <a href="/forum?id=srHMs3mPD5y">
              FETA: Towards Specializing Foundational Models for Expert Task Applications
          </a>


            <a href="/pdf?id=srHMs3mPD5y" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Amit_Alfassy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amit_Alfassy1">Amit Alfassy</a>, <a href="/profile?id=~Assaf_Arbelle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Assaf_Arbelle1">Assaf Arbelle</a>, <a href="/profile?id=~Oshri_Halimi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Oshri_Halimi1">Oshri Halimi</a>, <a href="/profile?id=~Sivan_Harary1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sivan_Harary1">Sivan Harary</a>, <a href="/profile?id=~Roei_Herzig2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roei_Herzig2">Roei Herzig</a>, <a href="/profile?id=~Eli_Schwartz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Eli_Schwartz1">Eli Schwartz</a>, <a href="/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rameswar_Panda1">Rameswar Panda</a>, <a href="/profile?id=~Michele_Dolfi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Michele_Dolfi1">Michele Dolfi</a>, <a href="/profile?id=~Christoph_Auer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Christoph_Auer1">Christoph Auer</a>, <a href="/profile?id=~Peter_W._J._Staar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peter_W._J._Staar2">Peter W. J. Staar</a>, <a href="/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kate_Saenko1">Kate Saenko</a>, <a href="/profile?id=~Rogerio_Feris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rogerio_Feris1">Rogerio Feris</a>, <a href="/profile?id=~Leonid_Karlinsky3" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Leonid_Karlinsky3">Leonid Karlinsky</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#srHMs3mPD5y-details-827" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="srHMs3mPD5y-details-827"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">foundational models, multi-modal, long tail content adaptation, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Self-supervised adaptation of foundational models to application domains in the long tail of their pretraining data distribution</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">    Foundational Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, the parameter capacity of FMs is still limited, leading to poor out-of-the-box performance of FMs on many expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for many practical expert tasks currently being `overlooked' by standard benchmarks focusing on common objects.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=srHMs3mPD5y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/FETA/feta.tar.gz" target="_blank" rel="nofollow noreferrer">https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/FETA/feta.tar.gz</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">to download the data please run the following commands:
        &gt;&gt; wget https://ai-vision-public-datasets.s3.eu.cloud-object-storage.appdomain.cloud/FETA/feta.tar.gz
        &gt;&gt; tar -xzvf feta.tar.gz
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">GNU/GPL v.3</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Y6qh5KF0Y_v" data-number="419">
        <h4>
          <a href="/forum?id=Y6qh5KF0Y_v">
              Imitrob: Imitation Learning Dataset for Training and Evaluating 6D Object  Pose Estimators
          </a>


            <a href="/pdf?id=Y6qh5KF0Y_v" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Karla_Stepanova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Karla_Stepanova1">Karla Stepanova</a>, <a href="/profile?id=~Jiri_Sedlar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jiri_Sedlar1">Jiri Sedlar</a>, <a href="/profile?id=~Matus_Tuna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matus_Tuna1">Matus Tuna</a>, <a href="/profile?id=~Radoslav_%C5%A0koviera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Radoslav_Škoviera1">Radoslav Škoviera</a>, <a href="/profile?id=~Gabriela_Sejnova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gabriela_Sejnova1">Gabriela Sejnova</a>, <a href="/profile?id=~Jan_Kristof_Behrens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jan_Kristof_Behrens1">Jan Kristof Behrens</a>, <a href="/profile?id=~Josef_Sivic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Josef_Sivic1">Josef Sivic</a>, <a href="/profile?id=~Robert_Babuska1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robert_Babuska1">Robert Babuska</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Y6qh5KF0Y_v-details-793" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y6qh5KF0Y_v-details-793"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">6D object pose estimation, imitation learning, perception for grasping and manipulation, real-world dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces a dataset for training and evaluation of methods for 6D pose estimation of hand-held tools in task demonstrations captured by a standard RGB camera. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces a dataset for training and evaluating methods for 6D pose estimation of hand-held tools in task demonstrations captured by a standard RGB camera. Despite the significant progress of 6D pose estimation methods, their performance is usually limited for heavily occluded objects, which is a common case in imitation learning where the object is often partially occluded by the manipulating hand. Currently, there is a lack of datasets that would enable the development of robust 6D pose estimation methods under these conditions. To overcome this problem, we collect a new dataset (Imitrob) aimed at 6D pose estimation in imitation learning and other applications where a human holds a tool and performs a task. The dataset contains image sequences of three different tools and six manipulation tasks with two camera viewpoints, four human subjects, and left/right hand. Each image is accompanied by an accurate ground truth measurement of the 6D object pose, obtained by the HTC Vive motion tracking device. The use of the dataset is demonstrated by training and evaluation of a recent 6D object pose estimation method (DOPE) in various setups. The dataset and code are publicly available at http://imitrob.ciirc.cvut.cz/imitrobdataset.php.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Y6qh5KF0Y_v&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://imitrob.ciirc.cvut.cz/imitrobdataset.php" target="_blank" rel="nofollow noreferrer">http://imitrob.ciirc.cvut.cz/imitrobdataset.php</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset URL: http://imitrob.ciirc.cvut.cz/imitrobdataset.php
        GitHub website: https://github.com/imitrob/imitrob_dataset_code</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY-NC-SA-4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="qnfYsave0U4" data-number="418">
        <h4>
          <a href="/forum?id=qnfYsave0U4">
              The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World
          </a>


            <a href="/pdf?id=qnfYsave0U4" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~William_A_Gaviria_Rojas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~William_A_Gaviria_Rojas1">William A Gaviria Rojas</a>, <a href="/profile?id=~Sudnya_Diamos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Sudnya_Diamos1">Sudnya Diamos</a>, <a href="/profile?id=~Keertan_Ranjan_Kini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Keertan_Ranjan_Kini1">Keertan Ranjan Kini</a>, <a href="/profile?id=~David_Kanter2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Kanter2">David Kanter</a>, <a href="/profile?id=~Vijay_Janapa_Reddi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Vijay_Janapa_Reddi1">Vijay Janapa Reddi</a>, <a href="/profile?id=~Cody_Coleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Cody_Coleman1">Cody Coleman</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#qnfYsave0U4-details-794" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qnfYsave0U4-details-794"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">deep learning, computer vision, supervised learning, ai for social good, dataset, creative commons</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present Dollar Street, a supervised dataset that contains 38,479 images of everyday household items from homes around the world, including tags for objects and demographic data such as region, country and home monthly income.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">It is crucial that image datasets for computer vision are representative and contain accurate demographic information to ensure their robustness and fairness, especially for smaller subpopulations. To address this issue, we present Dollar Street - a supervised dataset that contains 38,479 images of everyday household items from homes around the world. This dataset was manually curated and fully labeled, including tags for objects (e.g. “toilet,” “toothbrush,” “stove”) and demographic data such as region, country and home monthly income. This dataset includes images from homes with no internet access and incomes as low as <span>$</span>26.99 per month, visually capturing valuable socioeconomic diversity of traditionally under-represented populations. All images and data are licensed under CC-BY, permitting their use in academic and commercial work. Moreover, we show that this dataset can improve the performance of classification tasks for images of household items from lower income homes, addressing a critical need for datasets that combat bias.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=qnfYsave0U4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">See note in Checklist 3a</span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">See note in Checklist 3a</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All images and data are licensed under CC-BY 4.0, permitting their use in academic and commercial work.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bBgcnvwv41l" data-number="417">
        <h4>
          <a href="/forum?id=bBgcnvwv41l">
              The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture
          </a>


            <a href="/pdf?id=bBgcnvwv41l" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Amanda_A._Boatswain_Jacques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Amanda_A._Boatswain_Jacques1">Amanda A. Boatswain Jacques</a>, <a href="/profile?id=~Etienne_Lord1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Etienne_Lord1">Etienne Lord</a>, <a href="/profile?id=~Abdoulaye_Banire_Diallo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Abdoulaye_Banire_Diallo1">Abdoulaye Banire Diallo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#bBgcnvwv41l-details-971" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bBgcnvwv41l-details-971"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">satellite imagery, remote sensing, dataset, agriculture, cropland, supervised learning, image classification, deep learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">In this paper, we present a novel temporal patch-based dataset of Canadian agricultural croplands and provide baseline models in a benchmarking test.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited such high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, a RGB image, and additional bands corresponding to commonly used vegetation indices. Individually, each category contains at least 4,800 images. Moreover, as a benchmark, we provide models and source code that allow a user to predict the crop class using a single image (ResNet, DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the same location. In perspective, we expect this evolving dataset to propel the creation of robust agro-environmental models that can accelerate the comprehension of complex agricultural regions by providing accurate and continuous monitoring of land cover.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bBgcnvwv41l&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/bioinfoUQAM/Canadian-cropland-dataset" target="_blank" rel="nofollow noreferrer">https://github.com/bioinfoUQAM/Canadian-cropland-dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Explanations on how to accessand retrieve the images dataset can be found directly on the Github url. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N / A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">A pdf document stating the dataset license and permissions are attached in both the supplementary material and the Appendix (Appendix A.8) of the paper. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Fp7__phQszn" data-number="416">
        <h4>
          <a href="/forum?id=Fp7__phQszn">
              Why do tree-based models still outperform deep learning on tabular data?
          </a>


            <a href="/pdf?id=Fp7__phQszn" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Leo_Grinsztajn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Leo_Grinsztajn1">Leo Grinsztajn</a>, <a href="/profile?id=~Edouard_Oyallon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Edouard_Oyallon1">Edouard Oyallon</a>, <a href="/profile?id=~Gael_Varoquaux1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Gael_Varoquaux1">Gael Varoquaux</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Fp7__phQszn-details-318" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Fp7__phQszn-details-318"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo></math></mjx-assistive-mml></mjx-container>10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural networks: 1) be robust to uninformative features, 2) preserve the orientation of the data, and 3) be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and every point of a 20,000 compute hours hyperparameter search for each learner.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Fp7__phQszn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/LeoGrin/tabular-benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/LeoGrin/tabular-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mYjd0GK4lXh" data-number="415">
        <h4>
          <a href="/forum?id=mYjd0GK4lXh">
              PsyMo: A Dataset for Estimating of Self-Reported Psychometrics from Walking Patterns
          </a>


            <a href="/pdf?id=mYjd0GK4lXh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Adrian_Cosma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Adrian_Cosma1">Adrian Cosma</a>, <a href="/profile?email=emilian.radoi%40upb.ro" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="emilian.radoi@upb.ro">Ion Emilian Radoi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#mYjd0GK4lXh-details-75" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mYjd0GK4lXh-details-75"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">gait analysis, psychometrics, pose estimation, psychology</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a dataset for estimating psychometrics (personality, fatigue, aggressiveness, mental health issues) from walking patterns.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work we propose PsyMo (Psychometrics from Motion), a novel, multi-purpose dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totalling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. Potential applications include occupational psychology, marketing and advertisement, and unintrusive and automatic detection of mental illnesses. Alongside the estimation of self-reported psychometrics from gait, the dataset can be used to benchmark methods for identification and gender estimation from gait. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes and human poses.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mYjd0GK4lXh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://bit.ly/3Q91ypD" target="_blank" rel="nofollow noreferrer">https://bit.ly/3Q91ypD</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">PsyMo is released under CC-BY-NC-ND.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="QqRhz3_aDVS" data-number="414">
        <h4>
          <a href="/forum?id=QqRhz3_aDVS">
              POGym: A Partially Observable Benchmark Suite for Reinforcement Learning
          </a>


            <a href="/pdf?id=QqRhz3_aDVS" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Benjamin_Kov%C3%A1cs1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Benjamin_Kovács1">Benjamin Kovács</a>, <a href="/profile?id=~Qisong_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Qisong_Yang1">Qisong Yang</a>, <a href="/profile?id=~Matthijs_T._J._Spaan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Matthijs_T._J._Spaan1">Matthijs T. J. Spaan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#QqRhz3_aDVS-details-862" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QqRhz3_aDVS-details-862"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">reinforcement learning, environment</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This work present a benchmark suite for long-horizon partially observable reinforcement learning environments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement learning-based algorithms achieved huge successes in the past decade, outperforming handcrafted algorithms on numerous complex problems like playing video games or controlling manufacturing processes. Standard benchmark suites make measurement of the scientific process possible while accelerating the development progress by providing multiple-scale environments for prototyping and verifying RL agents. There is a large set of benchmarks available for fully observable environments. In real-world environments like robotics, partial observability and decision making based on past events is common. However, there is a lack of available benchmark sets for problems modelled as partially observable Markov decision processes. This work introduces a set of environments focusing on long-horizon problems, where performant agents need to learn processing long observation sequences efficiently, instead of deciding based on a single state vector. The environments are parameterizable by the sequence length, to enable benchmarking on multiple problem variants. In many cases, real-world applications also concern safety-related requirements. Therefore, an additional safety signal is provided for safe-RL applications. The introduced environments simulate realistic control problems, with the goal of stimulating research for this challenging and highly relevant problem setup. The open-source suite will be available online.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=QqRhz3_aDVS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.dropbox.com/s/8gnhdphaonla536/POGym.zip?dl=0" target="_blank" rel="nofollow noreferrer">https://www.dropbox.com/s/8gnhdphaonla536/POGym.zip?dl=0</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset is provided in the supplementary material archive and the following link:
        https://www.dropbox.com/s/8gnhdphaonla536/POGym.zip?dl=0
        Upon acceptance, it will be made available online on GitHub.
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-ShareAlike 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Z0s5T89qfjc" data-number="413">
        <h4>
          <a href="/forum?id=Z0s5T89qfjc">
              ENS-10: A Dataset For Post-Processing Ensemble Weather Forecasts
          </a>


            <a href="/pdf?id=Z0s5T89qfjc" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Saleh_Ashkboos2" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Saleh_Ashkboos2">Saleh Ashkboos</a>, <a href="/profile?id=~Langwen_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Langwen_Huang1">Langwen Huang</a>, <a href="/profile?id=~Nikoli_Dryden1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikoli_Dryden1">Nikoli Dryden</a>, <a href="/profile?id=~Tal_Ben-Nun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tal_Ben-Nun1">Tal Ben-Nun</a>, <a href="/profile?id=~Peter_Dominik_Dueben1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Peter_Dominik_Dueben1">Peter Dominik Dueben</a>, <a href="/profile?id=~Lukas_Gianinazzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lukas_Gianinazzi1">Lukas Gianinazzi</a>, <a href="/profile?id=~Luca_Nicola_Kummer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luca_Nicola_Kummer1">Luca Nicola Kummer</a>, <a href="/profile?id=~Torsten_Hoefler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Torsten_Hoefler1">Torsten Hoefler</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#Z0s5T89qfjc-details-493" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Z0s5T89qfjc-details-493"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Ensemble Post-Processing, Ensemble Weather Forecasting, Prediction Correction</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset containing ten ensemble members over 20 years for post-processing ensemble weather forecasts.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Post-processing ensemble prediction systems can improve weather forecasting, especially for extreme event prediction.
        In recent years, different machine learning models have been developed to improve the quality of the post-processing step. However, these models heavily rely on the data and generating such ensemble members requires multiple runs of numerical weather prediction models, at high computational cost.
        This paper introduces the ENS-10 dataset, consisting of ten ensemble members spread over 20 years (1998-2017). The ensemble members are generated by perturbing numerical weather simulations to capture the chaotic behavior of the Earth.
        To represent the three-dimensional state of the atmosphere, ENS-10 provides the most relevant atmospheric variables in 11 distinct pressure levels as well as the surface at 0.5 degree resolution.
        The dataset targets the prediction correction task at 48-hour lead time, which is essentially improving the forecast quality by removing the biases of the ensemble members. To this end, ENS-10 provides the weather variables for forecast lead times T=0, 24, and 48 hours (two data points per week). We provide a set of baselines for this task on ENS-10 and compare their performance in correcting the prediction of different weather variables. We also assess our baselines for predicting extreme events using our dataset. The ENS-10 dataset is available under the Creative Commons Attribution 4.0 International (CC BY 4.0) licence.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Z0s5T89qfjc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/spcl/ens10" target="_blank" rel="nofollow noreferrer">https://github.com/spcl/ens10</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The ENS-10 dataset can be downloaded directly from https://storage.ecmwf.europeanweather.cloud/MAELSTROM_AP4/ or through a
        downloader Python script. Examining the raw data, preprocessing, normalization, and training are all provided through a set of Python APIs
        in https://github.com/spcl/ens10, and examples written in PyTorch, and Jupyter notebooks are in https://github.com/spcl/climetlab-maelstrom-ens10.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">(C) Copyright 2021 European Centre for Medium-Range Weather Forecasts (ECMWF)

        Access to this dataset is governed by the following Terms of Use.

        * ECMWF retains all Intellectual Property Rights and copyright over its data.
        * ECMWF must be acknowledged (attributed) as the source.
        * ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.
        * ECMWF makes no warranty as to the accuracy or completeness of its data products or  the uninterrupted provision of such data products. All data products are provided on an "as is" basis. Any warranty implied by statute or otherwise is hereby excluded to the fullest extent permissible by law.
        * ECMWF shall not be liable should ECMWF discontinue the provision of its data products at any time.
        * ECMWF shall have no liability in contract, tort or otherwise arising out of or in connection with these Terms of Use.
        * Users must remove attribution if requested by ECMWF.
        * These Terms of Use shall be governed by and construed in accordance with the laws of England and Wales.
        * The parties shall attempt to settle any dispute between them in relation these Terms of Use and any data licensed under these Terms of Use in an amicable manner. If the dispute cannot be so settled, it shall be finally settled under the Rules of Arbitration of the International Chamber of Commerce by one or three arbitrators appointed in accordance with the said rules; sitting in London, England. The proceedings shall be in the English language. In accordance with Sections 45 and 69 of the Arbitration Act 1996, the right of appeal by either party to the English courts on a question of law arising in the course of any arbitral proceedings or out of an award made in any arbitral proceedings is hereby agreed to be excluded.
        * Nothing in these Terms of Use shall be construed as a waiver of any of the privileges and immunities conferred upon ECMWF by its Member States through its Convention and Protocol on Privileges and Immunities.
        * Access to the data products and services may be unavailable, delayed or interrupted. ECMWF will make reasonable efforts to restore the access following the report of a problem, but ECMWF will not be liable for, any unavailability, delay or interruption in access.

        This data product is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). To view a copy of this licence, visit https://creativecommons.org/licenses/by/4.0/

        You are free to:

        * Share - copy and redistribute the material in any medium or format
        * Adapt - remix, transform, and build upon the material for any purpose, even commercially.

        Under the following terms:

        * You must give appropriate credit (attribution) to ECMWF as outlined below, provide a link to the licence, and indicate if changes were made.
        * No additional restrictions - You may not apply legal terms or technological measures that legally restrict others from doing anything the licence permits.

        The following wording shall be attached to the use of this ECMWF data product:

        1. Copyright statement: Copyright "© [year] European Centre for Medium-Range Weather Forecasts (ECMWF)".
        2. Source: www.ecmwf.int
        3. Licence Statement: This data is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). https://creativecommons.org/licenses/by/4.0/
        4. Disclaimer: ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.
        5. Where applicable, an indication if the material has been modified and an indication of previous modifications.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="gud0qopqJc4" data-number="411">
        <h4>
          <a href="/forum?id=gud0qopqJc4">
              SKINCON: A skin disease dataset densely annotated by domain experts for fine-grained debugging and analysis
          </a>


            <a href="/pdf?id=gud0qopqJc4" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Roxana_Daneshjou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roxana_Daneshjou1">Roxana Daneshjou</a>, <a href="/profile?id=~Mert_Yuksekgonul1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Mert_Yuksekgonul1">Mert Yuksekgonul</a>, <a href="/profile?id=~Zhuo_Ran_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Zhuo_Ran_Cai1">Zhuo Ran Cai</a>, <a href="/profile?id=~Roberto_A._Novoa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Roberto_A._Novoa1">Roberto A. Novoa</a>, <a href="/profile?id=~James_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~James_Zou1">James Zou</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#gud0qopqJc4-details-180" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gud0qopqJc4-details-180"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">explainability, interpretability, concepts, fine grained error analysis, healthcare</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">SKINCON is a skin disease dataset densely annotated by domain experts for developing interpretability/explainability methods and fine-grained error analysis.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">For the deployment of artificial intelligence (AI) in high risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels which are semantically meaningful to humans.  However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Previous densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as osteoarthritis or melanoma. In dermatology, skin disease is described using an established clinical lexicon that allow clinicians to describe physical exam findings to one another. To provide the first medical dataset densely annotated by domain experts to provide annotations useful across multiple disease processes, we developed SKINCON: a skin disease dataset densely annotated by dermatologists. SKINCON includes 3230 images from the Fitzpatrick 17k skin disease dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include "plaque", "scale", and "erosion". These same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SKINCON dataset, such as probing models, concept-based explanations, concept bottlenecks, error analysis, and slice discovery. Furthermore, we use SKINCON to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=gud0qopqJc4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://skincon-dataset.github.io/" target="_blank" rel="nofollow noreferrer">https://skincon-dataset.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">We provide instructions on our website: https://skincon-dataset.github.io.  Both Fitzpatrick 17k and DDI require credentialized access due to the depiction of human skin disease.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://skincon-dataset.github.io/" target="_blank" rel="nofollow noreferrer">https://skincon-dataset.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">No embargo</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We release our annotations and experimental code under the MIT License.

        We develop SkinCON based on two prior datasets, and below we list their licenses.

        Images from the Fitpzatrick17k dataset are released under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.
        Images from the DDI dataset are released under the Stanford University Data Research Use License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1GVpwr2Tfdg" data-number="409">
        <h4>
          <a href="/forum?id=1GVpwr2Tfdg">
              Towards Better Evaluation for Dynamic Link Prediction
          </a>


            <a href="/pdf?id=1GVpwr2Tfdg" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Farimah_Poursafaei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Farimah_Poursafaei1">Farimah Poursafaei</a>, <a href="/profile?id=~Andy_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Andy_Huang1">Andy Huang</a>, <a href="/profile?id=~Kellin_Pelrine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Kellin_Pelrine1">Kellin Pelrine</a>, <a href="/profile?id=~Reihaneh_Rabbany1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Reihaneh_Rabbany1">Reihaneh Rabbany</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#1GVpwr2Tfdg-details-965" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1GVpwr2Tfdg-details-965"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dynamic link prediction, evaluation, dynamic graphs representation learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">There has been recent success learning from static graphs, but despite their prevalence, learning from time-evolving graphs remains challenging. We design new, more stringent evaluation procedures for link prediction specific to dynamic graphs, which reflect real-world considerations and can better compare different methods’ strengths and weaknesses. In particular, we create two visualization techniques to understand the recurring patterns of edges over time. They show that many edges reoccur at later time steps. Therefore, we propose a pure memorization baseline called EdgeBank. It achieves surprisingly strong performance across multiple settings, partly due to the easy negative edges used in the current evaluation setting. Hence, we introduce two more challenging negative sampling strategies that improve robustness and better match real-world applications. Lastly, we introduce five new dynamic graph datasets from a diverse set of domains missing from current benchmarks, providing new challenges and opportunities for future research.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1GVpwr2Tfdg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/fpour/DGB" target="_blank" rel="nofollow noreferrer">https://github.com/fpour/DGB</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bqF6crrW-eo" data-number="408">
        <h4>
          <a href="/forum?id=bqF6crrW-eo">
              Datasets for Understanding the Illicit Massage Industry
          </a>


            <a href="/pdf?id=bqF6crrW-eo" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Rui_Ouyang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rui_Ouyang1">Rui Ouyang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#bqF6crrW-eo-details-884" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bqF6crrW-eo-details-884"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">nlp, web, scraping, classification</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Studied the illicit massage economy by combining domain-specific with general purpose online data</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">    Illicit massage businesses (IMBs) offer commercial sexual services and can
            be found throughout the United States and can serve as a venue for human
            trafficking. We frame the problem of distinguishing illicit
            from regular massage businesses as a binary classification problem. Our primary dataset is Google Places review
            text for each business, which are then labelled using an IMB-specific
            website. We train a logistic regression classifier
            on the corpus and compare to several baselines. In the second part, we
            then consider several potential correlates with illicit activity based on
            domain expert input. In particular, since IMBs are though to be
            predominately Asian, we consider the impact of ethnicity features on the
            classifier performance.  By comparing different features and performing
            ablations on the text corpus, we show how algorithmic fairness can be
            evaluated in a fast and practical way.  We discuss both the possible impact
            of our work as well as actionable thoughts on the value of diverse
            applications in motivating machine learning research directions. Improved
            understanding of IMBs allows for a common foundation for stakeholders (e.g.
            policy makers and non-profits) to work toward reducing harms in the industry
            that are born disproportionately by women of color. Our dataset is both
            available as code to replicate (and verify) the dataset or directly with
            approval.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bqF6crrW-eo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="UiRSQykVNiC" data-number="407">
        <h4>
          <a href="/forum?id=UiRSQykVNiC">
              Myriad: a real-world testbed to bridge trajectory optimization and deep learning
          </a>


            <a href="/pdf?id=UiRSQykVNiC" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nikolaus_H._R._Howe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nikolaus_H._R._Howe1">Nikolaus H. R. Howe</a>, <a href="/profile?id=~Simon_Dufort-Labb%C3%A91" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Simon_Dufort-Labbé1">Simon Dufort-Labbé</a>, <a href="/profile?id=~Nitarshan_Rajkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Nitarshan_Rajkumar1">Nitarshan Rajkumar</a>, <a href="/profile?id=~Pierre-Luc_Bacon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Pierre-Luc_Bacon1">Pierre-Luc Bacon</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#UiRSQykVNiC-details-955" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UiRSQykVNiC-details-955"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">testbed, benchmark, real-world problems, reinforcement learning, imitation learning, optimal control, trajectory optimization, neural ordinary differential equations, system identification, end-to-end learning, implicit planning, nonlinear programming</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a testbed to benchmark imitation learning and reinforcement learning algorithms against trajectory optimization-based methods in challenging real-world environments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present Myriad, a testbed written in JAX which enables machine learning researchers to benchmark imitation learning and reinforcement learning algorithms against trajectory optimization-based methods in challenging real-world environments. Myriad contains 18 optimal control problems presented in continuous time and ranging from biology to medicine to engineering. As such, Myriad strives to serve as a stepping stone towards application of modern machine learning techniques for impactful real-world tasks. The repository also provides machine learning practitioners access to trajectory optimization techniques, not only for standalone use, but also for integration within a typical automatic differentiation workflow. Indeed, the combination of classical control theory and deep learning in a fully GPU-compatible package unlocks potential for new algorithms to arise. We present one such novel approach for use in dynamics learning and control tasks. Trained in a fully end-to-end fashion, our model leverages an implicit planning module over neural ordinary differential equations, enabling simultaneous learning and planning with complex environment dynamics. All environments, optimizers and tools are available in the software package at \url{https://github.com/nikihowe/myriad}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=UiRSQykVNiC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/nikihowe/myriad" target="_blank" rel="nofollow noreferrer">https://github.com/nikihowe/myriad</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The repository is licensed under the Apache License, Version 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-oYFYIaDh2-" data-number="406">
        <h4>
          <a href="/forum?id=-oYFYIaDh2-">
              A data-centric approach to table structure recognition
          </a>


            <a href="/pdf?id=-oYFYIaDh2-" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Brandon_Smock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Brandon_Smock1">Brandon Smock</a>, <a href="/profile?id=~Rohith_Pesala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Rohith_Pesala1">Rohith Pesala</a>, <a href="/profile?id=~Robin_Abraham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Robin_Abraham1">Robin Abraham</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#-oYFYIaDh2--details-287" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-oYFYIaDh2--details-287"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We show that a data-centric approach yields significant improvements to model performance for table structure recognition.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In this work we take a data-centric approach to the problem of table structure recognition (TSR) using deep learning. We adopt FinTabNet as our baseline training dataset, ICDAR-2013 as our baseline evaluation dataset, and DETR as our baseline model architecture. Baseline exact match table recognition accuracy of DETR trained with the original FinTabNet dataset is 74.6% evaluated on FinTabNet and 51.8% on ICDAR-2013. We perform dataset ablations and show that a series of improvements to FinTabNet alone, including canonicalization and filtering out potentially low-quality samples, significantly improve accuracy to 83.9% on FinTabNet and 55.1% on ICDAR-2013. Automated and manual corrections to ICDAR-2013's labels improves performance an additional 6.9% to 62.0%. Finally, we show that adding PubTables-1M to the training, with no changes to the model or training schedule, improves accuracy on ICDAR-2013 substantially to 78.7%, a new state of the art. Overall these results establish a new gold standard evaluation benchmark for table structure recognition and highlight the significant effects that improvements to label accuracy and sample diversity can have on TSR model performance. We will release all data, the dataset pre-processing code, and pre-trained model weights.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-oYFYIaDh2-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">A detailed description of the datasets and file URLs for downloading are available at: https://cantabs.blob.core.windows.net/cantabs/README.txt</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CDLAv2 for data, MIT for code</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="-VyJim9UBxQ" data-number="404">
        <h4>
          <a href="/forum?id=-VyJim9UBxQ">
              Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment
          </a>


            <a href="/pdf?id=-VyJim9UBxQ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Daniel_Vera_Nieto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Daniel_Vera_Nieto1">Daniel Vera Nieto</a>, <a href="/profile?id=~Luigi_Celona1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Luigi_Celona1">Luigi Celona</a>, <a href="/profile?id=~Clara_Fernandez_Labrador1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Clara_Fernandez_Labrador1">Clara Fernandez Labrador</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#-VyJim9UBxQ-details-274" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-VyJim9UBxQ-details-274"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Image aesthetic assessment, Dataset, Photo critiques, Aesthetic image captioning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Computational inference of aesthetics is an ill-defined task due to its subjective nature. Many datasets have been proposed to tackle the problem by providing pairs of images and aesthetic scores based on human ratings. However, humans are better at expressing their opinion, taste, and emotions by means of language rather than summarizing them in a single number. In fact, photo critiques provide much richer information as they reveal how and why users rate the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback. The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely (i) the large scale of the dataset and the extension of the comments criticizing different aspects of the image, (ii) it contains mostly UltraHD images, and (iii) it can easily be extended to new data as it is collected through an automatic pipeline. To the best of our knowledge, in this work, we propose the first attempt to estimate the aesthetic quality of visual stimuli from the critiques. To this end, we exploit the polarity of the sentiment of criticism as an indicator of aesthetic judgment. We demonstrate how sentiment polarity correlates positively with the aesthetic judgment available for two aesthetic assessment benchmarks. Finally, we experiment with several models by using the sentiment scores as a target for ranking images. Dataset and baselines are available\footnote{\url{https://github.com/mediatechnologycenter/aestheval}}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=-VyJim9UBxQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mediatechnologycenter/aestheval" target="_blank" rel="nofollow noreferrer">https://github.com/mediatechnologycenter/aestheval</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/mediatechnologycenter/aestheval" target="_blank" rel="nofollow noreferrer">https://github.com/mediatechnologycenter/aestheval</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We comply with Reddit User Agreement\footnote{\url{https://www.redditinc.com/policies/user-agreement/}}, Reddit API terms of use~\footnote{\url{https://docs.google.com/a/reddit.com/forms/d/e/1FAIpQLSezNdDNK1-P8mspSbmtC2r86Ee9ZRbC66u929cG2GX0T9UMyw/viewform}} and PushShift database CReative Commons License~\footnote{\url{https://zenodo.org/record/3608135\#.Yp3XEXZBw2w}}. In particular, we refer to the Section 2.d of Reddit API Terms of Use, which states: "User Content.  Reddit user photos, text and videos ("User Content") are owned by the users and not by Reddit. Subject to the terms and conditions of these Terms, Reddit grants You a non-exclusive, non-transferable, non-sublicensable, and revocable license to copy and display the User Content using the Reddit API through your application, website, or service to end users.  You may not modify the User Content except to format it for such display. You will comply with any requirements or restrictions imposed on usage of User Content by their respective owners, which may include "all rights reserved" notices, Creative Commons licenses or other terms and conditions that may be agreed upon between you and the owners." We do not provide access to the images directly, but a URL to the corresponding post and image. This information may be used to retrieve the images using the provided tools under other researchers' personal license to use the Reddit API. Moreover, we do not modify the original content by no means, while we provide the necessary tools to process the data and run the same experiments we carried out.

        We release the dataset under the Creative Commons Attribution 4.0 International license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="6iRDhV3yrmo" data-number="403">
        <h4>
          <a href="/forum?id=6iRDhV3yrmo">
              TweetDIS: A Large Twitter Dataset for Natural Disasters Built using Weak Supervision
          </a>


            <a href="/pdf?id=6iRDhV3yrmo" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ramya_Tekumalla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ramya_Tekumalla1">Ramya Tekumalla</a>, <a href="/profile?id=~Juan_M_Banda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Juan_M_Banda1">Juan M Banda</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 10 Jul 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#6iRDhV3yrmo-details-938" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6iRDhV3yrmo-details-938"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Weak Supervision, Social Media data analysis, Text classification, Large scale data analysis</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Social media is often utilized as a lifeline for communication during natural disasters. Traditionally, natural disaster tweets are filtered from the Twitter stream using the name of the natural disaster and the filtered tweets are sent for human annotation. The process of human annotation to create labeled sets for machine learning models is laborious, time consuming, at times inaccurate, and more importantly not scalable in terms of size and real-time use. In this work, we curate a silver standard dataset using weak supervision. In order to validate its utility, we train machine learning models on the weakly supervised data to identify three different types of natural disasters i.e earthquakes, hurricanes and floods. Our results demonstrate that models trained on the silver standard dataset achieved performance greater than 90% when classifying a manually curated, gold-standard dataset. To enable reproducible research and additional downstream utility, we release the silver standard dataset for the scientific community.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The dataset is currently in google drive and can be reviewed by the reviewers. It will be moved to Zenodo repository which contains a DOI. We already reserved the DOI for the repository and will move the code and data after acceptance of paper

        Zenodo link - https://doi.org/10.5281/zenodo.6628961</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="b0VDQiNLPy9" data-number="402">
        <h4>
          <a href="/forum?id=b0VDQiNLPy9">
              ETAB: A Benchmark Suite for Visual Representation Learning in Echocardiography
          </a>


            <a href="/pdf?id=b0VDQiNLPy9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ahmed_Alaa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Ahmed_Alaa1">Ahmed Alaa</a>, <a href="/profile?email=aphilipp%40broadinstitute.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="aphilipp@broadinstitute.org">Anthony Philippakis</a>, <a href="/profile?id=~David_Sontag1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~David_Sontag1">David Sontag</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#b0VDQiNLPy9-details-150" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b0VDQiNLPy9-details-150"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Echocardiography is one of the most commonly used diagnostic imaging modalities in cardiology. Application of deep learning models to echocardiograms can enable automated identification of cardiac structures, estimation of cardiac function, and prediction of clinical outcomes. However, a major hindrance to realizing the full potential of deep learning is the lack of large-scale, fully curated and annotated data sets required for supervised training. High-quality pre-trained representations that can transfer useful visual features of echocardiograms to downstream tasks can help adapt deep learning models to new setups using fewer annotated examples. In this paper, we design a suite of benchmarks that can be used to evaluate echocardiographic representations with respect to various clinically-relevant tasks using publicly accessible data sets. In addition, we develop a unified evaluation protocol that measures how well a visual representation of echocardiograms generalizes to common downstream tasks of interest. We use our benchmarking setup to evaluate state-of-the-art vision architectures, pre-training and transfer learning algorithms. We envision that our standardized, publicly accessible benchmarks would encourage future research in high-impact application domains and expedite progress in applying deep learning models to practical problems in cardiovascular medicine.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=b0VDQiNLPy9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="C_QwhQq4r4x" data-number="401">
        <h4>
          <a href="/forum?id=C_QwhQq4r4x">
              IDEO: Large Scale Egocentric 3D Object Dataset and Benchmark Challenges
          </a>


            <a href="/pdf?id=C_QwhQq4r4x" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Tien_Do1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Tien_Do1">Tien Do</a>, <a href="/profile?id=~Lance_Lemke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Lance_Lemke1">Lance Lemke</a>, <a href="/profile?id=~Jingfan_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Jingfan_Guo1">Jingfan Guo</a>, <a href="/profile?id=~Khiem_Vuong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Khiem_Vuong1">Khiem Vuong</a>, <a href="/profile?id=~Minh_Vo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Minh_Vo1">Minh Vo</a>, <a href="/profile?id=~Hyun_Soo_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="" data-original-title="~Hyun_Soo_Park1">Hyun Soo Park</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">06 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#C_QwhQq4r4x-details-618" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="C_QwhQq4r4x-details-618"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Existing datasets for human egocentric vision have been, by and large, focused on 2D tasks, e.g., object detection, segmentation, activity recognition, and gaze detection in egocentric videos. The 2D representations for these tasks are, however, a main impediment of learning human skills from large corpus of egocentric videos for extended tasks such as AR and robotics where 3D understanding is prerequisite.     In this paper, we aim to address this gap by presenting a new dataset called Indoor 3D Egocentric Object (IDEO) and its benchmark challenges for 3D metric object pose and shape reconstruction. IDEO is composed of 58K egocentric images of 6 object categories used in daily activities, collected by 85 subjects using RGB-D cameras. Each image is associated with precisely fitted multiple 3D objects with respect to 3D shape and 9-DOF pose (translation, rotation, scale, and aspect ratio). The dataset includes not only static objects but also dynamic objects manipulated by the subjects, which introduces nontrivial characteristic occlusion and uncommon object poses. The action labels for each activity in the video sequence is also annotated. We formulate two benchmark challenges to stimulate community effort in addressing the egocentric 3D object perception: (1) category-level object pose and scale prediction from single RGB(-D) image; and (2) category-level object shape prediction from single view RGB image. We show that IDEO adds a new dimension to egocentric vision, complimenting existing third-person datasets such as CAMERA and Objectron, quantitatively and qualitatively. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=C_QwhQq4r4x&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ideo-benchmark-challenges/IDEO" target="_blank" rel="nofollow noreferrer">https://github.com/ideo-benchmark-challenges/IDEO</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/ideo-benchmark-challenges/IDEO" target="_blank" rel="nofollow noreferrer">https://github.com/ideo-benchmark-challenges/IDEO</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>