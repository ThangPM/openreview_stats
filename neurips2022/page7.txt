<ul class="list-unstyled submissions-list">
    <li class="note " data-id="5wNiiIDynDF" data-number="107">
        <h4>
          <a href="/forum?id=5wNiiIDynDF">
              CGLB: Benchmark Tasks for Continual Graph Learning
          </a>


            <a href="/pdf?id=5wNiiIDynDF" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xikun_ZHANG2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xikun_ZHANG2">Xikun ZHANG</a>, <a href="/profile?id=~Dongjin_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongjin_Song2">Dongjin Song</a>, <a href="/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dacheng_Tao1">Dacheng Tao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#5wNiiIDynDF-details-104" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5wNiiIDynDF-details-104"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">graph representation learning, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Continual learning on graph data, which aims to accommodate new tasks over newly emerged graph data while maintaining the model performance over existing tasks, is attracting increasing attention from the community. Unlike continual learning on Euclidean data ($\textit{e.g.}$, images, texts, etc.) that has established benchmarks and unified experimental settings, benchmark tasks are rare for Continual Graph Learning (CGL). Moreover, due to the variety of graph data and its complex topological structures, existing works adopt different protocols to configure datasets and experimental settings. This creates a great obstacle to compare different techniques and thus hinders the development of CGL. To this end, we systematically study the task configurations in different application scenarios and develop a comprehensive Continual Graph Learning Benchmark (CGLB) curated from different public datasets. Specifically, CGLB contains both node-level and graph-level continual graph learning tasks under task-incremental (currently widely adopted) and class-incremental (more practical, challenging, yet underexplored) settings, as well as a toolkit for training, evaluating, and visualizing different CGL methods. Within CGLB, we also systematically explain the difference among these task configurations by comparing them to classical continual learning settings. Finally, we comprehensively compare state-of-the-art baselines on CGLB to investigate their effectiveness. Given CGLB and the developed toolkit, the barrier to exploring CGL has been greatly lowered and researchers can focus more on the model development without worrying about tedious work on pre-processing of datasets or encountering unseen pitfalls. The benchmark and the toolkit are available through https://github.com/QueuQ/CGLB.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=5wNiiIDynDF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/QueuQ/CGLB" target="_blank" rel="nofollow noreferrer">https://github.com/QueuQ/CGLB</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is licensed under a Attribution-NonCommercial 4.0 International license (CC BY-NC 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="sMIV0bGHn9R" data-number="106">
        <h4>
          <a href="/forum?id=sMIV0bGHn9R">
              A Medical Low-Back Pain Physical Rehabilitation Database for Human Body Movement Analysis
          </a>


            <a href="/pdf?id=sMIV0bGHn9R" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Sao_Mai_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sao_Mai_Nguyen1">Sao Mai Nguyen</a>, <a href="/profile?id=~Maxime_Devanne2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxime_Devanne2">Maxime Devanne</a>, <a href="/profile?id=~Olivier_Remy-Neris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Remy-Neris1">Olivier Remy-Neris</a>, <a href="/profile?id=~Andr%C3%A9_Th%C3%A9paut1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Thépaut1">André Thépaut</a>, <a href="/profile?id=~Mathieu_LEMPEREUR1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathieu_LEMPEREUR1">Mathieu LEMPEREUR</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#sMIV0bGHn9R-details-406" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sMIV0bGHn9R-details-406"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Medical Dataset, Physical Rehabilitation. Human Body Movement Analysis</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">To allow the development and assessment of physical rehabilitation body movements, we identify four challenges to address and propose an annotated medical database of clinical patients carrying out low back-pain rehabilitation exercises.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">  While automatic monitoring and coaching of exercises are showing encouraging results in non-medical applications, they still have limitations such as errors and limited use contexts.  To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we identify in this article four challenges to address and propose a medical database of clinical patients carrying out low back-pain rehabilitation exercises. The dataset includes 3D Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and medical annotations to assess the correctness, and error classification and localisation of body part and timespan. We propose along this dataset, two baseline movement recognition algorithms, pertaining to two different approaches : the probabilistic approach with a Gaussian Mixture Model (GMM), and the deep learning approach with a Long-Short Term Memory (LSTM).
          This dataset is valuable because it includes rehabilitation relevant motions in a clinical setting with patients in their rehabilitation program, using a cost-effective, portable, and convenient sensor, and because it shows the potential for improvement on these challenges.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=sMIV0bGHn9R&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://nguyensmai.free.fr/KeraalDataset.html" target="_blank" rel="nofollow noreferrer">http://nguyensmai.free.fr/KeraalDataset.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://nguyensmai.free.fr/KeraalDataset.html" target="_blank" rel="nofollow noreferrer">http://nguyensmai.free.fr/KeraalDataset.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CreativeCommons Attribution/Non-Commercial/Share-Alike license (CC-BY-NC-SA)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="1tsFygAnpk" data-number="104">
        <h4>
          <a href="/forum?id=1tsFygAnpk">
              BigIssue: A Realistic Bug Localization Benchmark
          </a>


            <a href="/pdf?id=1tsFygAnpk" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Paul_Kassianik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Kassianik1">Paul Kassianik</a>, <a href="/profile?id=~Erik_Nijkamp2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Nijkamp2">Erik Nijkamp</a>, <a href="/profile?id=~Bo_Pang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Pang4">Bo Pang</a>, <a href="/profile?id=~Yingbo_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingbo_Zhou1">Yingbo Zhou</a>, <a href="/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#1tsFygAnpk-details-963" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1tsFygAnpk-details-963"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">bug localization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">BigIssues is a large and diverse set of real and synthetic bugs designed to advance state of the art in NL-based Bug Localization methods.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As machine learning tools progress, the inevitable question arises: How can machine learning help us write better code? With significant progress being achieved in natural language processing with models like GPT-3 and Bert, the applications of natural language processing techniques to code is starting to be explored. Most of the research has been focused on automatic program repair (APR), and while the results on synthetic or highly filtered datasets are promising, such models are hard to apply in real-world scenarios because of inadequate bug localization. We propose BigIssue: a benchmark for realistic bug localization. The goal of the benchmark is two-fold. We provide (1) a general benchmark with a diversity of real and synthetic Java bugs and (2) a motivation to improve bug localization capabilities of models through attention to the full repository context. With the introduction of BigIssue, we hope to advance the state of the art in bug localization, in turn improving APR performance and increase its applicability to the modern development cycle.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=1tsFygAnpk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://console.cloud.google.com/storage/browser/bigissue-research" target="_blank" rel="nofollow noreferrer">https://console.cloud.google.com/storage/browser/bigissue-research</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The MIT License (MIT)

        Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

        The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

        THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="rc8o_j8I8PX" data-number="103">
        <h4>
          <a href="/forum?id=rc8o_j8I8PX">
              MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge
          </a>


            <a href="/pdf?id=rc8o_j8I8PX" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Linxi_Fan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linxi_Fan2">Linxi Fan</a>, <a href="/profile?id=~Guanzhi_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guanzhi_Wang1">Guanzhi Wang</a>, <a href="/profile?id=~Yunfan_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunfan_Jiang1">Yunfan Jiang</a>, <a href="/profile?id=~Ajay_Mandlekar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ajay_Mandlekar1">Ajay Mandlekar</a>, <a href="/profile?id=~Yuncong_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuncong_Yang1">Yuncong Yang</a>, <a href="/profile?id=~Haoyi_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoyi_Zhu1">Haoyi Zhu</a>, <a href="/profile?id=~Andrew_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Tang1">Andrew Tang</a>, <a href="/profile?id=~De-An_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~De-An_Huang1">De-An Huang</a>, <a href="/profile?id=~Yuke_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuke_Zhu1">Yuke Zhu</a>, <a href="/profile?id=~Anima_Anandkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anima_Anandkumar1">Anima Anandkumar</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#rc8o_j8I8PX-details-321" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rc8o_j8I8PX-details-321"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Embodied Agents, Minecraft, Open-ended Learning, Multitask Learning, Internet Knowledge Base, Reinforcement Learning, Large Pre-training</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">MineDojo is a new framework built on the Minecraft game for developing open-ended, generally capable embodied agents.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the code and knowledge bases (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=rc8o_j8I8PX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://minedojo.org" target="_blank" rel="nofollow noreferrer">https://minedojo.org</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Please find the dataset access instructions and guidelines on https://minedojo.org and our supplementary material.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Simulator and benchmarking suite code: MIT license.
        MineDojo Youtube database:  Creative Commons Attribution 4.0 International (CC BY 4.0).
        MineDojo Wiki database: Creative Commons Attribution Non Commercial Share Alike 3.0 Unported.
        MineDojo Reddit database:  Creative Commons Attribution 4.0 International (CC BY 4.0).
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="tszONfnYOnz" data-number="102">
        <h4>
          <a href="/forum?id=tszONfnYOnz">
              Motley: Benchmarking Heterogeneity and Personalization in Federated Learning
          </a>


            <a href="/pdf?id=tszONfnYOnz" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shanshan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shanshan_Wu1">Shanshan Wu</a>, <a href="/profile?id=~Tian_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tian_Li1">Tian Li</a>, <a href="/profile?id=~Zachary_Charles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Charles1">Zachary Charles</a>, <a href="/profile?id=~Yu_Xiao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Xiao3">Yu Xiao</a>, <a href="/profile?id=~Ken_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ken_Liu1">Ken Liu</a>, <a href="/profile?id=~Zheng_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zheng_Xu2">Zheng Xu</a>, <a href="/profile?id=~Virginia_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Virginia_Smith1">Virginia Smith</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#tszONfnYOnz-details-285" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tszONfnYOnz-details-285"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Motley is the first benchmark that develops baselines for personalized federated learning in cross-device and cross-silo settings.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Personalized federated learning considers learning models unique to each client in a heterogeneous network. The resulting client-specific models have been purported to improve metrics such as accuracy, fairness, and robustness in federated networks. However, despite a plethora of work in this area, it remains unclear: (1) which personalization techniques are most effective in various settings, and (2) how important personalization truly is for realistic federated applications. To better answer these questions, we propose Motley, a benchmark for personalized federated learning. Motley consists of a suite of cross-device and cross-silo federated datasets from varied problem domains, as well as thorough evaluation metrics for better understanding the possible impacts of personalization. We establish baselines on the benchmark by comparing a number of representative personalized federated learning methods. These initial results highlight strengths and weaknesses of existing approaches, and raise several open questions for the community. Motley aims to provide a reproducible means with which to advance developments in personalized and heterogeneity-aware federated learning, as well as the related areas of transfer learning, meta-learning, and multi-task learning. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=tszONfnYOnz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">Code to reproduce our experiments: https://github.com/google-research/federated/tree/master/personalization_benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The datasets used in our benchmark are already in the public domain (see Appendix B in our paper).</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is licensed under the Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="u-vvZCUXvrd" data-number="101">
        <h4>
          <a href="/forum?id=u-vvZCUXvrd">
              EEGCogNet: an EEG Dataset for Real-life Cognitive Task Classification
          </a>


            <a href="/pdf?id=u-vvZCUXvrd" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xiaodong_Qu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodong_Qu1">Xiaodong Qu</a>, <a href="/profile?id=~Abdelrahman_Abdelmonsef1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abdelrahman_Abdelmonsef1">Abdelrahman Abdelmonsef</a>, <a href="/profile?id=~Guangyao_Dou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guangyao_Dou1">Guangyao Dou</a>, <a href="/profile?id=~Brian_Dong_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_Dong_Xiang1">Brian Dong Xiang</a>, <a href="/profile?id=~Tianyi_Wang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Wang5">Tianyi Wang</a>, <a href="/profile?id=~Peiyan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peiyan_Liu1">Peiyan Liu</a>, <a href="/profile?id=~Timothy_Hickey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Hickey1">Timothy Hickey</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#u-vvZCUXvrd-details-305" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="u-vvZCUXvrd-details-305"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Brain-Computer Interfaces, EEG, Machine Learning, Deep Learning, LDA, SVM, kNN, Random Forest, Decision Tree</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">EEGCogNET dataset for Cognitive Task Classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper presents a dataset and benchmark in hopes of furthering research for cognitive function classification based on brain activities. We present EEGCogNet, a dataset of Electroencephalography (EEG) recordings collected with affordable technology from 120 non-expert subjects while performing frequent and common cognitive activities. The dataset is the aggregation of the results from four separate experiments: Read-Write-Type (RWT), Think-Count-Recall (TCR), GRE-Relax (GRE), Math-Shut-Read-Open (MSR). The goal of this paper is to inspire new data collection methods and new machine learning approaches for decoding behavior from growing large-scale EEG datasets, particularly for cognitive functions. Our code is publicly available and uses a compact and complete interface to evaluate several learning algorithms.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=u-vvZCUXvrd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://eegcog.net/" target="_blank" rel="nofollow noreferrer">http://eegcog.net/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="http://eegcog.net/" target="_blank" rel="nofollow noreferrer">http://eegcog.net/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="GgM5DiAb6A2" data-number="100">
        <h4>
          <a href="/forum?id=GgM5DiAb6A2">
              FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Settings
          </a>


            <a href="/pdf?id=GgM5DiAb6A2" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jean_Ogier_du_Terrail1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean_Ogier_du_Terrail1">Jean Ogier du Terrail</a>, <a href="/profile?id=~Samy-Safwan_Ayed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samy-Safwan_Ayed1">Samy-Safwan Ayed</a>, <a href="/profile?id=~Edwige_Cyffers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edwige_Cyffers1">Edwige Cyffers</a>, <a href="/profile?id=~Felix_Grimberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_Grimberg1">Felix Grimberg</a>, <a href="/profile?id=~Chaoyang_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaoyang_He1">Chaoyang He</a>, <a href="/profile?id=~Regis_Loeb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Regis_Loeb1">Regis Loeb</a>, <a href="/profile?id=~Paul_Mangold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Mangold1">Paul Mangold</a>, <a href="/profile?id=~Tanguy_Marchand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tanguy_Marchand1">Tanguy Marchand</a>, <a href="/profile?id=~Othmane_Marfoq2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Othmane_Marfoq2">Othmane Marfoq</a>, <a href="/profile?id=~Erum_Mushtaq1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erum_Mushtaq1">Erum Mushtaq</a>, <a href="/profile?id=~Boris_Muzellec2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Muzellec2">Boris Muzellec</a>, <a href="/profile?id=~Constantin_Philippenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Constantin_Philippenko1">Constantin Philippenko</a>, <a href="/profile?id=~Santiago_Silva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santiago_Silva1">Santiago Silva</a>, <a href="/profile?id=~Maria_Tele%C5%84czuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Teleńczuk1">Maria Teleńczuk</a>, <a href="/profile?id=~Shadi_Albarqouni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shadi_Albarqouni1">Shadi Albarqouni</a>, <a href="/profile?id=~Salman_Avestimehr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Salman_Avestimehr1">Salman Avestimehr</a>, <a href="/profile?id=~Aur%C3%A9lien_Bellet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aurélien_Bellet1">Aurélien Bellet</a>, <a href="/profile?id=~Aymeric_Dieuleveut1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aymeric_Dieuleveut1">Aymeric Dieuleveut</a>, <a href="/profile?id=~Martin_Jaggi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Jaggi1">Martin Jaggi</a>, <a href="/profile?id=~Sai_Praneeth_Karimireddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sai_Praneeth_Karimireddy1">Sai Praneeth Karimireddy</a>, <a href="/profile?id=~Marco_Lorenzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marco_Lorenzi1">Marco Lorenzi</a>, <a href="/profile?id=~Giovanni_Neglia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giovanni_Neglia1">Giovanni Neglia</a>, <a href="/profile?id=~Marc_Tommasi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Tommasi1">Marc Tommasi</a>, <a href="/profile?id=~Mathieu_Andreux1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathieu_Andreux1">Mathieu Andreux</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#GgM5DiAb6A2-details-559" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GgM5DiAb6A2-details-559"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic cross-silo FL datasets exist, thereby slowing algorithmic research in these critical applications. In this work, we propose a novel cross-silo dataset suite, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL. FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally benchmark standard FL algorithms on all datasets. Our flexible and modular suite allows researchers to easily download datasets, reproduce results and re-use the different components for their research. FLamby will be available at~\url{github-url}.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=GgM5DiAb6A2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Dataset code is included in the submission. Our dataset code will be available on a public github repository after the submission to ensure anonymity of the submission.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">As we intend to submit this work in a double blind fashion, we plan to publicly release the github repository no later than by the conference.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code of FLamby is provided under the MIT license.

        FLamby does not provide new datasets, but rather code to easily access existing datasets. Users are still required to follow the licenses of these datasets, which are listed below:
        - Camelyon16: CC 0;
        - LIDC-IDRI: CC-BY 3 and TCIA data usage policy;
        - IXITiny: CC-BY SA 3;
        - TCGA-BRCA: GDC open access (https://gdc.cancer.gov/access-data/data-access-processes-and-tools);
        - KITS2019: CC-BY NC SA 3;
        - ISIC2019: CC-BY 4;
        - Heart Disease: CC-BY 4.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="djnKHOjpb7I" data-number="98">
        <h4>
          <a href="/forum?id=djnKHOjpb7I">
              EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations
          </a>


            <a href="/pdf?id=djnKHOjpb7I" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ahmad_Darkhalil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmad_Darkhalil1">Ahmad Darkhalil</a>, <a href="/profile?id=~Dandan_Shan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dandan_Shan1">Dandan Shan</a>, <a href="/profile?id=~Bin_Zhu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bin_Zhu7">Bin Zhu</a>, <a href="/profile?id=~Jian_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Ma3">Jian Ma</a>, <a href="/profile?id=~Amlan_Kar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amlan_Kar2">Amlan Kar</a>, <a href="/profile?id=~Richard_Ely_Locke_Higgins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Ely_Locke_Higgins1">Richard Ely Locke Higgins</a>, <a href="/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>, <a href="/profile?id=~David_Fouhey2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Fouhey2">David Fouhey</a>, <a href="/profile?id=~Dima_Damen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dima_Damen1">Dima Damen</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#djnKHOjpb7I-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="djnKHOjpb7I-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Egocentric Vision, Pixel Segmentations, Hands, Active Objects, Action, Long-Term Understanding</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">New dataset and benchmark suite for long-term pixel-level segmentations of hand-object interactions in egocentric video</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce VISOR, a new dataset of pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric video. VISOR annotates videos from EPIC-KITCHENS, which comes with a new set of challenges not encountered in current video segmentation datasets. Specifically, we need to ensure both short- and long-term consistency of pixel-level annotations as objects undergo transformative interactions, e.g. an onion is peeled, diced and cooked - where we aim to obtain accurate pixel-level annotations of the peel, onion pieces, chopping board, knife, pan, as well as the acting hands. VISOR introduces an annotation pipeline, AI-powered in parts, for scalability and quality. In total, we publicly release 271K manual semantic masks of 255 object classes, more than 10M interpolated dense masks, 67K hand-object relations, covering 36 hours of 179 untrimmed videos. Along with the annotations, we introduce three challenges in video object segmentation, interaction understanding and long-term reasoning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=djnKHOjpb7I&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://epic-kitchens.github.io/VISOR/" target="_blank" rel="nofollow noreferrer">https://epic-kitchens.github.io/VISOR/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://epic-kitchens.github.io/VISOR/" target="_blank" rel="nofollow noreferrer">https://epic-kitchens.github.io/VISOR/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 4.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="oocV8W9ovR9" data-number="97">
        <h4>
          <a href="/forum?id=oocV8W9ovR9">
              FAD: A Chinese Dataset for Fake Audio Detection
          </a>


            <a href="/pdf?id=oocV8W9ovR9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Haoxin_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoxin_Ma1">Haoxin Ma</a>, <a href="/profile?id=~Jiangyan_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiangyan_Yi1">Jiangyan Yi</a>, <a href="/profile?id=~chenglong_wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~chenglong_wang4">chenglong wang</a>, <a href="/profile?id=~Xinrui_Yan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinrui_Yan1">Xinrui Yan</a>, <a href="/profile?id=~Jianhua_Tao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhua_Tao2">Jianhua Tao</a>, <a href="/profile?id=~Tao_Wang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Wang7">Tao Wang</a>, <a href="/profile?email=shiming.wang%40nlpr.ia.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="shiming.wang@nlpr.ia.ac.cn">shiming wang</a>, <a href="/profile?id=~Le_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Le_Xu2">Le Xu</a>, <a href="/profile?id=~Ruibo_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruibo_Fu1">Ruibo Fu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#oocV8W9ovR9-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oocV8W9ovR9-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">fake audio detecton, additive noise scenario, audio forensics, FAD dataset</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We design a Chinese fake audio detection dataset (FAD) for studying more generalized detection methods. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Fake audio detection is a growing concern and some relevant datasets have been designed for research. But there is no standard public Chinese dataset under additive noise conditions. In this paper, we aim to fill in the gap and design a Chinese fake audio detection dataset (FAD) for studying more generalized detection methods. Twelve mainstream speech generation techniques are used to generate fake audios. To simulate the real-life scenarios, three noise datasets are selected for noisy adding at five different signal noise ratios. FAD dataset can be used not only for fake audio detection, but also for detecting the algorithms of fake utterances for audio forensics. Baseline results are presented with analysis. The results that show fake audio detection methods with generalization remain challenging. The FAD dataset is publicly available. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=oocV8W9ovR9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ADDchallenge/FAD" target="_blank" rel="nofollow noreferrer">https://github.com/ADDchallenge/FAD</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://zenodo.org/record/6641573#.YqlN_U7YuUk" target="_blank" rel="nofollow noreferrer">https://zenodo.org/record/6641573#.YqlN_U7YuUk</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-ND 4.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="P14FNX0iotO" data-number="96">
        <h4>
          <a href="/forum?id=P14FNX0iotO">
              ReSPack: A Large-Scale Rectilinear Steiner Tree Packing Benchmark for Wire Routing
          </a>


            <a href="/pdf?id=P14FNX0iotO" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kanghoon_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kanghoon_Lee2">Kanghoon Lee</a>, <a href="/profile?id=~Youngjoon_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngjoon_Park1">Youngjoon Park</a>, <a href="/profile?id=~Han-Seul_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han-Seul_Jeong1">Han-Seul Jeong</a>, <a href="/profile?id=~Deunsol_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deunsol_Yoon1">Deunsol Yoon</a>, <a href="/profile?id=~Sunghoon_Hong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sunghoon_Hong2">Sunghoon Hong</a>, <a href="/profile?id=~Sungryull_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungryull_Sohn1">Sungryull Sohn</a>, <a href="/profile?id=~Minu_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minu_Kim1">Minu Kim</a>, <a href="/profile?id=~Hanbum_Ko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanbum_Ko1">Hanbum Ko</a>, <a href="/profile?id=~Moontae_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moontae_Lee1">Moontae Lee</a>, <a href="/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="/profile?id=~Kyunghoon_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyunghoon_Kim1">Kyunghoon Kim</a>, <a href="/profile?id=~Euihyuk_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Euihyuk_Kim1">Euihyuk Kim</a>, <a href="/profile?id=~Seonggeon_Cho2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seonggeon_Cho2">Seonggeon Cho</a>, <a href="/profile?id=~Jaesang_Min2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaesang_Min2">Jaesang Min</a>, <a href="/profile?id=~Woohyung_Lim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Woohyung_Lim1">Woohyung Lim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#P14FNX0iotO-details-656" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="P14FNX0iotO-details-656"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">combinatorial optimization, rectilinear Steiner tree packing problem, wire routing, synthetic data</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present ReSPack, a large-scale synthetic rectilinear Steiner tree packing problem (RSTPP) dataset and a benchmark for wire routing.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Combinatorial optimization (CO) has been studied as a useful tool for modeling industrial problems, but it still remains a challenge in complex domains because of the NP-hardness. With recent advances in machine learning, the field of CO is shifting to the study of neural combinatorial optimization using a large amount of data, showing promising results in some CO problems. Rectilinear Steiner Tree Packing Problem (RSTPP) is a well-known CO problem and is widely used in modeling wiring problem among components in a printed circuit board and an integrated circuit design. Despite the importance for a design automation, the lack of available data has restricted to fully leverage machine learning approaches. In this paper, we present ReSPack, a large-scale synthetic RSTPP dataset and a benchmark for wire routing. ReSPack includes a source code for generating RSTPP problems of various types with different sizes, data instances generated for the benchmark testing, and implementations of several baseline algorithms. We believe that ReSPack can contribute to solving circuit design problems as well as the development of CO with machine learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=P14FNX0iotO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/LG-AI-PAIRLab/ReSPack" target="_blank" rel="nofollow noreferrer">https://github.com/LG-AI-PAIRLab/ReSPack</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://drive.google.com/file/d/1YpnWb5fZHBoQ27fsyY6ZVGPIh484hbnp/view?usp=sharing" target="_blank" rel="nofollow noreferrer">https://drive.google.com/file/d/1YpnWb5fZHBoQ27fsyY6ZVGPIh484hbnp/view?usp=sharing</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC-SA 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="mJD_rFda76Z" data-number="95">
        <h4>
          <a href="/forum?id=mJD_rFda76Z">
              Prompt-Based Time Series Forecasting: A New Task and Dataset
          </a>


            <a href="/pdf?id=mJD_rFda76Z" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Hao_Xue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Xue1">Hao Xue</a>, <a href="/profile?id=~Flora_D._Salim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Flora_D._Salim1">Flora D. Salim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">02 Jun 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#mJD_rFda76Z-details-346" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mJD_rFda76Z-details-346"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The research of time series forecasting benefits a wide range of applications from weather forecasting to human mobility or traffic prediction. This paper studies the time series forecasting problem from a whole new perspective. In the existing methods, the forecasting models take a sequence of numerical values as input and yield numerical values as output. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to time series forecasting tasks. Thus, we propose a novel prompt-based time series forecasting (PromptCast) task. In this task, the numerical input and output are transformed into language sentence prompts. We frame the forecasting task in a sentence-to-sentence manner which makes it possible to directly apply language models for the forecasting purpose. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios in this paper. We evaluate different state-of-the-art numerical-based forecasting methods and language generation models such as Bart and Bigbird. The benchmark results demonstrate that the proposed prompt-based time series forecasting with language generation models is a promising research direction. In addition, in comparison to conventional numerical-based forecasting, prompt-based forecasting shows a better generalization ability. We believe that the proposed PromptCast benchmark task as well as our PISA dataset could provide novel insights and further lead to new research directions in the time series forecasting domain.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=mJD_rFda76Z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/HaoUNSW/PISA" target="_blank" rel="nofollow noreferrer">https://github.com/HaoUNSW/PISA</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The PISAdataset and codes for models reported in the benchmark are available at https://github.com/HaoUNSW/PISA.
        In this repository, we also show some generated examples of language models for the PromptCast task. Please note that only validation sets are provided as examples in the above repository during the submission period. After the acceptance decision notification, the full PISA dataset (including train/val/testing sets) will be uploaded to the same repository and publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The dataset is distributed under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Vhb-awTdHCh" data-number="93">
        <h4>
          <a href="/forum?id=Vhb-awTdHCh">
              PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets
          </a>


            <a href="/pdf?id=Vhb-awTdHCh" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Shuo_Sun2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuo_Sun2">Shuo Sun</a>, <a href="/profile?id=~Molei_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Molei_Qin1">Molei Qin</a>, <a href="/profile?id=~Xinrun_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinrun_Wang1">Xinrun Wang</a>, <a href="/profile?id=~Bo_An2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_An2">Bo An</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Vhb-awTdHCh-details-422" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Vhb-awTdHCh-details-422"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Evaluation, Reinforcement Learning, Finance, Benchmarking</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The financial markets, which involve more than \$90 trillion market capitals, attract the attention of innumerable investors around the world. Recently, reinforcement learning in financial markets (FinRL) emerges as a promising direction to train agents for making profitable investment decisions. However, the evaluation of most FinRL methods only focus on profit-related measures, which are far from satisfactory for practitioners to deploy these methods into real-world financial markets. Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability, Risk-control, Universality, Diversity, rEliability, and eXplainability, with a total of 16 measures for a systematic evaluation. Specifically, i) we propose AlphaMix+ as a strong FinRL baseline, which leverages Mixture-of-Experts (MoE) and risk-sensitive approaches to make diversified risk-aware investment decisions, ii) we evaluate 6 widely used FinRL methods in 4 long-term real-world datasets of influential financial markets on portfolio management to demonstrate the usage of our PRUDEX-Compass, iii) PRUDEX-Compass is released as public resources to facilitate the design and comparison of new FinRL methods. We hope that PRUDEX-Compass can shed light on future FinRL research to prevent untrustworthy results from stagnating FinRL into successful industry deployment.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Vhb-awTdHCh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/ai-gamer/PRUDEX-Compass" target="_blank" rel="nofollow noreferrer">https://github.com/ai-gamer/PRUDEX-Compass</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="TaARsI_Iio" data-number="92">
        <h4>
          <a href="/forum?id=TaARsI_Iio">
              A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction
          </a>


            <a href="/pdf?id=TaARsI_Iio" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Wonseok_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonseok_Hwang1">Wonseok Hwang</a>, <a href="/profile?id=~Dongjun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongjun_Lee1">Dongjun Lee</a>, <a href="/profile?email=kycho%40lbox.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="kycho@lbox.kr">Kyoungyeon Cho</a>, <a href="/profile?email=leehanuhl%40lbox.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="leehanuhl@lbox.kr">Hanuhl Lee</a>, <a href="/profile?id=~Minjoon_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjoon_Seo1">Minjoon Seo</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#TaARsI_Iio-details-837" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TaARsI_Iio-details-837"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Natural Legal Language Understanding, Legal AI, Legal Judgement Prediction, Legal Language Model</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A Korean Legal AI datasets  &amp; Language Model</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">
        The recent advances of deep learning have dramatically changed how machine learning, especially in the domain of natural language processing, can be applied to legal domain. However, this shift to the data-driven approaches calls for larger and more diverse datasets, which are nevertheless still small in number, especially in non-English languages. Here we present the first large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of one legal corpus, two classification tasks, two legal judgement prediction (LJP) tasks, and one summarization task. The legal corpus consists of 150k Korean precedents (264M tokens), of which 63k are sentenced in last 4 years and 96k are from the first and the second level courts in which factual issues are reviewed. The two classification tasks are case names (10k) and statutes (3k) prediction from the factual description of individual cases. The LJP tasks consist of (1) 10k criminal examples where the model is asked to predict fine amount, imprisonment with labor, and imprisonment without labor ranges for the given facts, and (2) 5k civil examples where the inputs are facts and claim of relief and outputs are the degrees of claim acceptance. The summarization task consists of the Supreme Court precedents and the corresponding summaries. We also release LCUBE, the first Korean legal language model trained on the legal corpus from this study. Given the uniqueness of the Law of South Korea and the diversity of the legal tasks covered in this work, we believe that LBOX OPEN contributes to the multilinguality of global legal research. LBOX OPEN and LCUBE are publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=TaARsI_Iio&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/lbox-kr/lbox-open" target="_blank" rel="nofollow noreferrer">https://github.com/lbox-kr/lbox-open</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/lbox-kr/lbox-open" target="_blank" rel="nofollow noreferrer">https://github.com/lbox-kr/lbox-open</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Attribution-NonCommercial-NoDerivatives 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="8x8qeOu1dgW" data-number="91">
        <h4>
          <a href="/forum?id=8x8qeOu1dgW">
              ClinTS-HII: A Clinical Time-series Benchmark Targeting Heterogeneity, Irregularity, and Interdependency
          </a>


            <a href="/pdf?id=8x8qeOu1dgW" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Chong_Wang14" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chong_Wang14">Chong Wang</a>, <a href="/profile?id=~Jiawen_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiawen_Zhang1">Jiawen Zhang</a>, <a href="/profile?id=~Shun_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shun_Zheng1">Shun Zheng</a>, <a href="/profile?id=~Xiaohan_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohan_Yi1">Xiaohan Yi</a>, <a href="/profile?id=~Wei_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Cao1">Wei Cao</a>, <a href="/profile?id=~Lijun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lijun_Wu1">Lijun Wu</a>, <a href="/profile?id=~Jia_Li4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia_Li4">Jia Li</a>, <a href="/profile?id=~Jiang_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiang_Bian1">Jiang Bian</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#8x8qeOu1dgW-details-881" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8x8qeOu1dgW-details-881"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">benchmarks, clinical therapeutics, electronic health records, time series</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent years witnessed a great deal of time-series learning approaches being applied to various scenarios of clinical therapeutics, such as early risk warning, clinical outcome prediction, etc. However, some intrinsic data characteristics in these scenarios, particularly including heterogeneity, irregularity, and interdependency, violate the basic assumptions of traditional learning algorithms and thus hinder the performance of learned models. In the meantime, most of the existing studies only partially considered some of these data properties, inevitably resulting in limited model applicability and extensibility. To bridge this gap, we establish a new clinical time-series benchmark based on MIMIC-III. To facilitate comprehensive evaluation, this benchmark consists of five widely recognized clinical tasks. More importantly, we design a specific data schema by considering all the three critical data characteristics. To further demonstrate the value of clinical time-series modeling by considering these characteristics, we build a straightforward yet effective baseline by coupling some basic strategies that tackle the domain-specific challenges and show that this simple model can easily outperform all state-of-the-art methods applicable to these tasks. We hope this benchmark can facilitate and inspire more future research on the comprehensive modeling of clinical time series. The scripts to build the benchmark and the source code to reproduce our experiments have been released at https://github.com/nullnullll/ClinTS_HII.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=8x8qeOu1dgW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/nullnullll/ClinTS_HII" target="_blank" rel="nofollow noreferrer">https://github.com/nullnullll/ClinTS_HII</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">Our original data source MIMIC-III follows the PhysioNet credentialing process, accessible at https://physionet.org/content/mimiciii/1.4/.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The MIMIC-III database is hosted and maintained on PhysioNet under PhysioNet Credentialed Health Data License 1.5.0. We maintain all the code and scripts of this benchmark under the MIT license. </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="k3462dQtQhg" data-number="90">
        <h4>
          <a href="/forum?id=k3462dQtQhg">
              A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks
          </a>


            <a href="/pdf?id=k3462dQtQhg" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ganqu_Cui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ganqu_Cui1">Ganqu Cui</a>, <a href="/profile?id=~Lifan_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lifan_Yuan1">Lifan Yuan</a>, <a href="/profile?id=~He_Bingxiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~He_Bingxiang1">He Bingxiang</a>, <a href="/profile?id=~Yangyi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangyi_Chen1">Yangyi Chen</a>, <a href="/profile?id=~Zhiyuan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyuan_Liu1">Zhiyuan Liu</a>, <a href="/profile?id=~Maosong_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maosong_Sun1">Maosong Sun</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#k3462dQtQhg-details-388" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="k3462dQtQhg-details-388"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/thunlp/OpenBackdoor" target="_blank" rel="nofollow noreferrer">https://github.com/thunlp/OpenBackdoor</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="i1bFPSw42W0" data-number="88">
        <h4>
          <a href="/forum?id=i1bFPSw42W0">
              Multi-view Bootstrapping in the Wild
          </a>


            <a href="/pdf?id=i1bFPSw42W0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Mosam_Dabhi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mosam_Dabhi1">Mosam Dabhi</a>, <a href="/profile?id=~Chaoyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaoyang_Wang1">Chaoyang Wang</a>, <a href="/profile?id=~Tim_Clifford1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Clifford1">Tim Clifford</a>, <a href="/profile?id=~Laszlo_Attila_Jeni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laszlo_Attila_Jeni1">Laszlo Attila Jeni</a>, <a href="/profile?id=~Ian_R._Fasel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_R._Fasel1">Ian R. Fasel</a>, <a href="/profile?id=~Simon_Lucey2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Lucey2">Simon Lucey</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#i1bFPSw42W0-details-223" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="i1bFPSw42W0-details-223"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Auto labeling, Multi-view 2D-3D, Keypoint Detection, Self-supervision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Obtaining high-fidelity 2D and 3D landmark labels from videos with only two or three uncalibrated, handheld cameras moving in the wild.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Labeling articulated objects in unconstrained settings have a wide variety of applications including entertainment, neuroscience, psychology, ethology, and many fields of medicine. Large offline labeled datasets do not exist for all but the most common articulated object categories (e.g., humans). Hand labeling these landmarks within a video sequence is a laborious task. Learned landmark detectors can help, but can be error-prone when trained from only a few examples. Multi-camera systems that train fine-grained detectors have shown significant promise in detecting such errors, allowing for self-supervised solutions that only need a small percentage of the video sequence to be hand-labeled. The approach, however, is based on calibrated cameras and rigid geometry, making it expensive, difficult to manage, and impractical in real-world scenarios. In this paper, we address these bottlenecks by combining a non-rigid 3D neural prior with deep flow to obtain high-fidelity landmark estimates from videos with only two or three uncalibrated, handheld cameras. With just a few annotations (representing 1-2\% of the frames), we are able to produce 2D results comparable to state-of-the-art fully supervised methods, and obtain 3D reconstructions that are impossible with other existing approaches. Our Multi-view Bootstrapping in the Wild (MBW) approach demonstrates impressive results on standard human datasets, as well as tigers, cheetahs, fish, colobus monkeys, chimpanzees, and flamingos from videos captured casually in a zoo. We release this challenging dataset consisting more than a thousand image frames of tail-end distribution categories with their corresponding 2D, 3D labels generated from minimal human intervention.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=i1bFPSw42W0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/mosamdabhi/MBW-Data" target="_blank" rel="nofollow noreferrer">https://github.com/mosamdabhi/MBW-Data</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/mosamdabhi/MBW-Data" target="_blank" rel="nofollow noreferrer">https://github.com/mosamdabhi/MBW-Data</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY-NC</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NEkh84G4Nuo" data-number="87">
        <h4>
          <a href="/forum?id=NEkh84G4Nuo">
              Benchmarking Constraint Inference in Inverse Reinforcement Learning
          </a>


            <a href="/pdf?id=NEkh84G4Nuo" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Guiliang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guiliang_Liu1">Guiliang Liu</a>, <a href="/profile?id=~Yudong_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yudong_Luo1">Yudong Luo</a>, <a href="/profile?id=~Ashish_Gaurav1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashish_Gaurav1">Ashish Gaurav</a>, <a href="/profile?id=~Kasra_Rezaee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kasra_Rezaee1">Kasra Rezaee</a>, <a href="/profile?id=~Pascal_Poupart2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pascal_Poupart2">Pascal Poupart</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#NEkh84G4Nuo-details-262" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NEkh84G4Nuo-details-262"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Constraint Inverse Reinforcement Learning, benchmark, variational inference</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We construct a benchmark for Constraint Inverse Reinforcement Learning (CIRL) and propose a baseline algorithm for it.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">When deploying Reinforcement Learning (RL) agents into a physical system, we must ensure that these agents are well aware of the underlying constraints. In many real-world problems, however, the constraints followed by expert agents (e.g., humans) are often hard to specify mathematically and unknown to the RL agents. To tackle these issues, Constraint Inverse Reinforcement Learning (CIRL) considers the formalism of Constrained Markov Decision Processes (CMDPs) and estimates constraints from expert demonstrations by learning a constraint function. As an emerging research topic, CIRL does not have common benchmarks, and previous works tested their algorithms with hand-crafted environments (e.g., grid worlds). In this paper, we construct a CIRL benchmark in the context of two major application domains: robot control and autonomous driving. We design relevant constraints for each environment and empirically study the ability of different algorithms to recover those constraints based on expert trajectories that respect those constraints.  To handle stochastic dynamics, we propose a variational approach that infers constraint distributions, and we demonstrate its performance by comparing it with other CIRL baselines on our benchmark. The benchmark, including the information for reproducing the performance of CIRL algorithms, is publicly available at https://github.com/Guiliang/CIRL-benchmarks-public.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NEkh84G4Nuo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Guiliang/CIRL-benchmarks-public" target="_blank" rel="nofollow noreferrer">https://github.com/Guiliang/CIRL-benchmarks-public</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://cs.uwaterloo.ca/~ppoupart/datasets/expert_data.zip
        This dataset is a component of our benchmark, so we also include the dataset URL in our benchmark repository. You can directly download it. The instruction related to this dataset is in the code repository and our supplementary materials.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our code is licensed under a BSD 3-Clause license. We have put the license in our repository.
        Our dataset is licensed under a CC BY-NC 4.0 license.  See official instructions [here](https://creativecommons.org/licenses/by-nc/4.0/).
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="A79jAS4MeW9" data-number="86">
        <h4>
          <a href="/forum?id=A79jAS4MeW9">
              Multi-modal Robustness Analysis Against Language and Visual Perturbations
          </a>


            <a href="/pdf?id=A79jAS4MeW9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Madeline_Chantry_Schiappa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Madeline_Chantry_Schiappa1">Madeline Chantry Schiappa</a>, <a href="/profile?id=~Yogesh_S_Rawat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yogesh_S_Rawat1">Yogesh S Rawat</a>, <a href="/profile?id=~Vibhav_Vineet5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vibhav_Vineet5">Vibhav Vineet</a>, <a href="/profile?id=~Hamid_Palangi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hamid_Palangi1">Hamid Palangi</a>, <a href="/profile?id=~Shruti_Vyas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shruti_Vyas1">Shruti Vyas</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 01 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#A79jAS4MeW9-details-678" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="A79jAS4MeW9-details-678"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">robustness, multimodal modeling, text-to-video retrieval, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Analysis of robustness on text-video multimodal models on text-to-video retrieval.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Joint visual and language modeling on large-scale datasets has recently shown a good progress in multi-modal tasks when compared to single modal learning. However, robustness of these approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of such models against various real-world perturbations focusing on video and language. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different textual perturbations. The study reveals some interesting findings: 1) The studied models are more robust when text is perturbed versus when video is perturbed 2) The transformer text encoder is more robust on non-semantic changing text perturbations and visual perturbations compared to word embedding approaches 3) Using two-branch encoders in isolation is typically more robust than when architectures use cross-attention. We hope this study will serve as a benchmark and guide future research in robust multimodal learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=A79jAS4MeW9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://mmvr-neurips.github.io/MultiModalRobustness/" target="_blank" rel="nofollow noreferrer">https://mmvr-neurips.github.io/MultiModalRobustness/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="10iA3OowAV3" data-number="85">
        <h4>
          <a href="/forum?id=10iA3OowAV3">
              Chartalist: Labeled Graph Datasets for UTXO and Account-based Blockchains
          </a>


            <a href="/pdf?id=10iA3OowAV3" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kiarash_Shamsi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiarash_Shamsi1">Kiarash Shamsi</a>, <a href="/profile?id=~Friedhelm_Victor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Friedhelm_Victor1">Friedhelm Victor</a>, <a href="/profile?id=~Murat_Kantarcioglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Murat_Kantarcioglu1">Murat Kantarcioglu</a>, <a href="/profile?id=~Yulia_Gel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulia_Gel1">Yulia Gel</a>, <a href="/profile?id=~Cuneyt_Gurcan_Akcora2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cuneyt_Gurcan_Akcora2">Cuneyt Gurcan Akcora</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#10iA3OowAV3-details-876" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="10iA3OowAV3-details-876"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We created the first blockchain ML-Ready dataset platform</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Machine learning on blockchain graphs is an emerging field with many applications
        such as ransomware payment tracking, price manipulation analysis, and money
        laundering detection. However, analyzing blockchain data requires domain expertise
        and computational resources, which pose a significant barrier and hinder
        advancement in this field. We introduce Chartalist, the first comprehensive platform
        to methodically access and use machine learning across a large selection of
        blockchains to address this challenge.
        Chartalist contains ML-ready datasets from unspent transaction output (UTXO)
        (e.g., Bitcoin) and account-based blockchains (e.g., Ethereum). We envision that
        Chartalist can facilitate data modeling, analysis, and representation of blockchain
        data and attract a wider community of scientists to analyze blockchains. Chartalist
        is an open-science initiative at https://github.com/cakcora/Chartalist.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value ">www.chartalist.org</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://www.chartalist.org" target="_blank" rel="nofollow noreferrer">https://www.chartalist.org</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KqgjSoreck" data-number="84">
        <h4>
          <a href="/forum?id=KqgjSoreck">
              SSL4EO-S12: A Large-scale Multimodal Multitemporal Dataset for Self-supervised Learning in Earth Observation
          </a>


            <a href="/pdf?id=KqgjSoreck" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yi_Wang31" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wang31">Yi Wang</a>, <a href="/profile?id=~Nassim_Ait_Ait_Ali_Braham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nassim_Ait_Ait_Ali_Braham1">Nassim Ait Ait Ali Braham</a>, <a href="/profile?id=~Conrad_M_Albrecht1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Conrad_M_Albrecht1">Conrad M Albrecht</a>, <a href="/profile?id=~Zhitong_Xiong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhitong_Xiong2">Zhitong Xiong</a>, <a href="/profile?id=~Chenying_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenying_Liu1">Chenying Liu</a>, <a href="/profile?id=~Xiao_Xiang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Xiang_Zhu1">Xiao Xiang Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#KqgjSoreck-details-423" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KqgjSoreck-details-423"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">self-supervised learning, unsupervised learning, Earth observation, remote sensing</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a large-scale multimodal, multitemporal dataset for self-supervised pre-training in Earth observation with extensive experiments.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Recent advances of self-supervised learning in computer vision have raised wide interest in the field of remote sensing (RS) and Earth observation (EO). With the advantage of learning generic representations without human annotation, self-supervised pre-training bears great potential towards efficient data mining from large-scale satellite imagery. While there have been various pre-training datasets for natural images, most works in EO still rely on ImageNet or labeled RS datasets which can be suboptimal for unsupervised pre-training. We thus propose an unlabeled dataset SSL4EO-S12: Self-Supervised Learning for Earth Observation - Sentinel-1/2. The dataset contains large-scale (global) multimodal (SAR/optical) and multitemporal (seasons) satellite images from European Space Agency's Sentinel mission. We show that the proposed dataset, targeted at RS and EO applications, can be used for self-supervised pre-training with various methods such as MoCo-v2, DINO, MAE and Data2vec. In the transfer learning setting, it yields comparable or better downstream performances than supervised learning on multiple applications. It also yields better performance than pre-training on other existing datasets. Dataset, codes and pre-trained models are available at https://github.com/zhu-xlab/SSL4EO-S12.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KqgjSoreck&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/zhu-xlab/SSL4EO-S12" target="_blank" rel="nofollow noreferrer">https://github.com/zhu-xlab/SSL4EO-S12</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/zhu-xlab/SSL4EO-S12" target="_blank" rel="nofollow noreferrer">https://github.com/zhu-xlab/SSL4EO-S12</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="bQPhtI9ZMo1" data-number="82">
        <h4>
          <a href="/forum?id=bQPhtI9ZMo1">
              The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning
          </a>


            <a href="/pdf?id=bQPhtI9ZMo1" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Henrique_Donancio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Henrique_Donancio1">Henrique Donancio</a>, <a href="/profile?email=laurent.vercouter%40insa-rouen.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="laurent.vercouter@insa-rouen.fr">Laurent Vercouter</a>, <a href="/profile?email=roclawsk%40mv.uni-kl.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="roclawsk@mv.uni-kl.de">Harald Roclawski</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#bQPhtI9ZMo1-details-703" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bQPhtI9ZMo1-details-703"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Deep Reinforcement Learning (DRL) has achieved remarkable success in scenarios such as games and has emerged as a potential solution for control tasks. That is due to its ability to leverage the scalability and handle complex dynamics. However, few works have targeted environments grounded in real-world settings. Indeed, real-world scenarios can be challenging especially when faced to high dimensionality of the state space and to unknown reward function. To facilitate research, we release a testbed consisting of an environment simulator and demonstrations of human operation concerning pump scheduling of a real-world water distribution facility. The pump scheduling problem can be viewed as a decision process to decide when to operate pumps to supply water while limiting electricity consumption and meeting system constraints. To provide a starting point, we release a well-documented codebase, present an overview of some challenges that can be addressed and provide a baseline representation of the problem. The code and dataset are available at https://gitlab.com/hdonancio/pumpscheduling</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=bQPhtI9ZMo1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://gitlab.com/hdonancio/pumpscheduling" target="_blank" rel="nofollow noreferrer">https://gitlab.com/hdonancio/pumpscheduling</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://gitlab.com/hdonancio/pumpscheduling" target="_blank" rel="nofollow noreferrer">https://gitlab.com/hdonancio/pumpscheduling</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="v3yM5zVzP4C" data-number="81">
        <h4>
          <a href="/forum?id=v3yM5zVzP4C">
              Natural Backdoor Datasets
          </a>


            <a href="/pdf?id=v3yM5zVzP4C" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Emily_Wenger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Wenger1">Emily Wenger</a>, <a href="/profile?id=~Roma_Bhattacharjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roma_Bhattacharjee1">Roma Bhattacharjee</a>, <a href="/profile?id=~Arjun_Nitin_Bhagoji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arjun_Nitin_Bhagoji1">Arjun Nitin Bhagoji</a>, <a href="/profile?email=josephinep%40uchicago.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="josephinep@uchicago.edu">Josephine Passananti</a>, <a href="/profile?email=andere%40uchicago.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="andere@uchicago.edu">Emilio Andere</a>, <a href="/profile?id=~Haitao_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haitao_Zheng2">Haitao Zheng</a>, <a href="/profile?id=~Ben_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Zhao1">Ben Zhao</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#v3yM5zVzP4C-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v3yM5zVzP4C-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">machine learning, security, backdoors</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We discover and validate the existence of natural backdoors in existing image datasets.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using "digital trigger patterns." In contrast, "physical backdoors" use physical objects as triggers, have only recently been identified, and are qualitatively different enough to resist all defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co-located with targets of classification. Building these  datasets is time- and labor-intensive.
        This works seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co-located objects already present in popular datasets such as ImageNet. Once identified, a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets, along with the specific classes they can poison. We call these naturally occurring trigger-class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely-available datasets, and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=v3yM5zVzP4C&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/uchicago-sandlab/naturalbackdoors" target="_blank" rel="nofollow noreferrer">https://github.com/uchicago-sandlab/naturalbackdoors</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The code to recreate the datasets in this paper can be found at: https://github.com/uchicago-sandlab/naturalbackdoors</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The code accompanying this paper is released under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="6JgPNswpdzq" data-number="80">
        <h4>
          <a href="/forum?id=6JgPNswpdzq">
              Which Model to Trust: Assessing the Influence of Models on the Performance of Reinforcement Learning Algorithms for Continuous Control Tasks
          </a>


            <a href="/pdf?id=6JgPNswpdzq" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Giacomo_Arcieri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giacomo_Arcieri1">Giacomo Arcieri</a>, <a href="/profile?id=~David_Woelfle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Woelfle1">David Woelfle</a>, <a href="/profile?id=~Eleni_Chatzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eleni_Chatzi1">Eleni Chatzi</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 07 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#6JgPNswpdzq-details-371" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6JgPNswpdzq-details-371"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, model-based Reinforcement Learning, Bayesian Neural Networks, Gaussian Processes, continuous control, benchmark</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper compares several model choices on popular RL continuous control tasks.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The need for algorithms able to solve Reinforcement Learning (RL) problems with few trials has motivated the advent of model-based RL methods. The reported performance of model-based algorithms has dramatically increased within recent years. However, it is not clear how much of the recent progress is due to improved algorithms or due to improved models. While different modeling options are available to choose from when applying a model-based approach, the distinguishing traits and particular strengths of different models are not clear. The main contribution of this work lies precisely in assessing the model influence on the performance of RL algorithms. A set of commonly adopted models is established for the purpose of model comparison. These include Neural Networks (NNs), ensembles of NNs, two different approximations of Bayesian NNs (BNNs), that is, the Concrete Dropout NN and the Anchored Ensembling, and Gaussian Processes (GPs). The model comparison is evaluated on a suite of continuous control benchmarking tasks. Our results reveal that significant differences in model performance do exist. The Concrete Dropout NN reports persistently superior performance. We summarize these differences for the benefit of the modeler and suggest that the model choice is tailored to the standards required by each specific application.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=6JgPNswpdzq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/giarcieri/Assessing-the-Influence-of-Models-on-the-Performance-of-Reinforcement-Learning-Algorithms" target="_blank" rel="nofollow noreferrer">https://github.com/giarcieri/Assessing-the-Influence-of-Models-on-the-Performance-of-Reinforcement-Learning-Algorithms</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="qgA8P9IfcwN" data-number="79">
        <h4>
          <a href="/forum?id=qgA8P9IfcwN">
              SEN12TS: Fusing Sentinel-1 Backscatter and InSAR Timeseries with Multispectral Sentinel-2 Imagery for Deep Learning
          </a>


            <a href="/pdf?id=qgA8P9IfcwN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Terence_M_Conlon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Terence_M_Conlon1">Terence M Conlon</a>, <a href="/profile?email=rose%40descarteslabs.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rose@descarteslabs.com">Rose M Rustowicz</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#qgA8P9IfcwN-details-216" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qgA8P9IfcwN-details-216"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Sentinel-1, Sentinel-2, deep learning, data fusion, image timeseries, self-supervised learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces the SEN12TS dataset, a novel public resource containing 1.69 TB of Sentinel-1, Sentinel-2, and labeled land cover image triplets for remote sensing and deep learning applications. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper introduces the SEN12TS dataset, a novel public resource containing Sentinel-1, Sentinel-2, and labeled land cover image triplets over six agro-ecologically diverse areas of interest: California, Iowa, Catalonia, Ethiopia, Uganda, and Sumatra. Using the Descartes Labs geospatial analytics platform, 246,400 triplets are produced at 10m resolution over 31,398 256-by-256-pixel unique spatial tiles for a total size of 1.69 TB. The image triplets include radiometric terrain corrected synthetic aperture radar (SAR) backscatter measurements; interferometric synthetic aperture radar (InSAR) coherence and phase layers; local incidence angle and ground slope values; multispectral optical imagery; and decameter-resolution land cover classifications. Moreover, sensed imagery is available in timeseries: Within an image triplet, radar-derived imagery is collected at four timesteps 12 days apart. For the same spatial extent, up to 16 image triplets are available across the calendar year of 2020. Two initial use cases are also demonstrated for the dataset. The first transforms radar imagery into enhanced vegetation indices by means of a generative adversarial network, and the second tests combinations of input imagery for cropland classification. The SEN12TS dataset is hosted by the Radiant Earth Foundation, where it is available for download at   https://mlhub.earth/data/sen12ts under a non-commercial CC BY-NC 4.0 license</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=qgA8P9IfcwN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://mlhub.earth/data/sen12ts" target="_blank" rel="nofollow noreferrer">https://mlhub.earth/data/sen12ts</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Please find additional information the SEN12TS dataset and this submission in the file "sen12ts_supplementary_information_for_submission.pdf" contained within the zipped supplementary materials.

        I am copying below one paragraph from that file due to its potential importance for the upcoming review process:

        Note to reviewers: I am currently working with Radiant Earth engineers to resolve the last steps associated with publishing the dataset. The Radiant Earth Team has assured me that the dataset will be published by mid-June. However, if reviewers need access to the dataset before this timeframe, please contact me at terence.conlon@columbia.edu and I can grant access to the Google Storage Bucket that’s currently also holding the full SEN12TS dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 4.0 (Attribution-NonCommercial 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_b7Rq4BU3ug" data-number="78">
        <h4>
          <a href="/forum?id=_b7Rq4BU3ug">
              FlyView: a bio-inspired optical flow truth dataset for visual navigation using panoramic stereo vision
          </a>


            <a href="/pdf?id=_b7Rq4BU3ug" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Alix_Leroy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alix_Leroy1">Alix Leroy</a>, <a href="/profile?email=graham.taylor%40zoo.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="graham.taylor@zoo.ox.ac.uk">Graham Keith Taylor</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#_b7Rq4BU3ug-details-183" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_b7Rq4BU3ug-details-183"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">optical flow, motion flow, self-motion, ego-motion, fly, drosophila, calliphora</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A dataset for motion flow and ego-motion inspired by the fly vision</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Flying at speed through complex environments is a challenging task that has been performed successfully by insects since the Carboniferous, but which remains a challenge for robotic and autonomous systems. Insects navigate the world using optical flow sensed by their compound eyes, which they process using a deep neural network weighing just a few milligrams. Deploying an insect-inspired network architecture in computer vision could therefore enable more efficient and effective ways of estimating structure and self-motion using optical flow. Training a bio-inspired deep network to implement these tasks requires biologically relevant training, test, and validation data. To this end, we introduce FlyView, a novel bio-inspired truth dataset for visual navigation. This simulated dataset is rendered using open source 3D scenes in which the observer's position is known at every frame, and is accompanied by truth data on depth, self-motion, and motion flow. This dataset comprising 42,475 frames has several key features that are missing from existing optical flow datasets, including: (i) panoramic cameras with a monocular and binocular field of view matched to that of a fly's compound eyes; (ii) dynamically meaningful self-motion modelled on motion primitives, or the 3D trajectories of drones and flies; and (iii) complex natural and indoor environments including reflective surfaces.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_b7Rq4BU3ug&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/Ahleroy/FlyView" target="_blank" rel="nofollow noreferrer">https://github.com/Ahleroy/FlyView</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Data will be uploaded to Zenodo and HuggingFace, and download links will be shared on the Github repository.

        https://github.com/Ahleroy/FlyView

        For now, only the sample scene is available on the Github repository,</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The project is funded by DSTL and therefore the dataset needs to be approved before release.
        We will make sure the dataset will be fully released by the date of the conference.
        In addition, the dataset is roughly 4TB of data. Therefore, the release of the dataset will be done in chunks on multiple hosting platforms.
        A sample is made available to the reviewers.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">3D assets are the propriety of their authors. Please refer to the supplementary materials for the Licence of individual assets.
        Trajectories and code are the propriety of the University of Oxford and are generously made available to the scientific community under the license CC-BY-NC-SA 4.0. Source code is made available under the MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Jq3uTzLg9se" data-number="76">
        <h4>
          <a href="/forum?id=Jq3uTzLg9se">
              ComMU: Dataset for Combinatorial Music Generation
          </a>


            <a href="/pdf?id=Jq3uTzLg9se" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Lee_Hyun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lee_Hyun1">Lee Hyun</a>, <a href="/profile?id=~Taehyun_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taehyun_Kim1">Taehyun Kim</a>, <a href="/profile?id=~Hyolim_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyolim_Kang1">Hyolim Kang</a>, <a href="/profile?id=~Minjoo_Ki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjoo_Ki1">Minjoo Ki</a>, <a href="/profile?id=~Hyeonchan_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyeonchan_Hwang1">Hyeonchan Hwang</a>, <a href="/profile?id=~Kwanho_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kwanho_Park1">Kwanho Park</a>, <a href="/profile?id=~Sharang_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sharang_Han1">Sharang Han</a>, <a href="/profile?id=~Seon_Joo_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seon_Joo_Kim2">Seon Joo Kim</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">01 Jun 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Jq3uTzLg9se-details-930" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jq3uTzLg9se-details-930"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Music generation</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose ComMU, a dataset for generating diverse and high-quality music with rich musical metadata.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/).</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Jq3uTzLg9se&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://pozalabs.github.io/ComMU/" target="_blank" rel="nofollow noreferrer">https://pozalabs.github.io/ComMU/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://pozalabs.github.io/ComMU/" target="_blank" rel="nofollow noreferrer">https://pozalabs.github.io/ComMU/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="93cqcWFpTex" data-number="74">
        <h4>
          <a href="/forum?id=93cqcWFpTex">
              A Dataset for Efforts Towards Achieving the Sustainable Development Goal of Safe Working Environments
          </a>


            <a href="/pdf?id=93cqcWFpTex" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eirik_Lund_Flogard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eirik_Lund_Flogard1">Eirik Lund Flogard</a>, <a href="/profile?id=~Ole_Jakob_Mengshoel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ole_Jakob_Mengshoel1">Ole Jakob Mengshoel</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">31 May 2022 (modified: 14 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#93cqcWFpTex-details-248" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="93cqcWFpTex-details-248"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Working Environment, Labour Inspections, Checklists, Long tailed Classification</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Poor health, safety and environment (HSE) conditions in workplaces is a widespread problem that can have many detrimental effects on the society. For instance, it has been estimated that there are more then 4000 deaths each year due to poor working conditions in EU alone. To deal with this problem, governmental agencies conduct labour inspections, which are important to globally achieve UN's SDG 8.8, "protect labour rights and promote safe working environments for all workers". However, since agencies have limited resources, it is essential that the inspections are carried out as efficiently as possible. Current research suggests that machine learning (ML) could be used to improve labour inspection tasks, such as selecting organisations for inspections more effectively. However, the research within this field is limited. In this paper we introduce a new dataset called the Labour Inspection Checklists dataset (LICD), which we will make publicly available to promote further research into this subject. LICD consists of 63634 instances where each instance is an inspection conducted by the Norwegian Labour Inspection Authority (NLIA). LICD also contains 575 features and two potential target variables: checklists and non-compliance. There are many opportunities for ML research on the dataset, so we provide two demonstration experiments. One of the experiments deals with the problem of selecting a relevant checklist for surveying a given target organisation. The other experiment deals with the problem of correctly predicting HSE violations, given a specific checklist and target organisation. The demonstrations show that achieving good ML classification performance for both problems can be very difficult, which is a strong motivation for future research into methods that could improve classification performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=93cqcWFpTex&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Both versions of the LICD data set introduced in the paper are included in the supplementary material. The data set will also be available online via an URL if the paper is accepted. The online page will be available long before the conference.

        The code from the demonstration experiments described in the paper is also included as a part of the supplementary material. The code must be opened in Jupyter Notebook.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The data set is released under a CCBY 4.0 License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="euli0I5CKvy" data-number="73">
        <h4>
          <a href="/forum?id=euli0I5CKvy">
              RL4RS: A Real-World Benchmark Dataset for Reinforcement Learning based Recommender System
          </a>


            <a href="/pdf?id=euli0I5CKvy" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kai_Wang18" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Wang18">Kai Wang</a>, <a href="/profile?email=zouzhene%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zouzhene@corp.netease.com">Zhene Zou</a>, <a href="/profile?email=shangyue%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shangyue@corp.netease.com">Yue Shang</a>, <a href="/profile?email=dengqilin%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dengqilin@corp.netease.com">Qiling Deng</a>, <a href="/profile?email=zhaominghao%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhaominghao@corp.netease.com">Minghao Zhao</a>, <a href="/profile?email=liangyile%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="liangyile@corp.netease.com">Yile Liang</a>, <a href="/profile?id=~Runze_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runze_Wu1">Runze Wu</a>, <a href="/profile?email=hztaojianrong%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hztaojianrong@corp.netease.com">Jianrong Tao</a>, <a href="/profile?email=hzshenxudong%40corp.netease.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hzshenxudong@corp.netease.com">Xudong Shen</a>, <a href="/profile?id=~Tangjie_Lv1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tangjie_Lv1">Tangjie Lv</a>, <a href="/profile?id=~Changjie_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changjie_Fan1">Changjie Fan</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">31 May 2022 (modified: 13 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#euli0I5CKvy-details-189" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="euli0I5CKvy-details-189"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Reinforcement Learning, Datasets, Recommender Systems</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce RL4RS: a new resource fully collected from industrial applications to train and evaluate RL algorithms with special concerns on RL-based RS issues.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Reinforcement learning based recommender systems (RL-based RS) aim at learning a good policy from a batch of collected data, by casting recommendations to multi-step decision-making tasks. However, current RL-based RS research commonly have a large reality gap. They involve artificial RL datasets or semi-simulated RS datasets, and the trained policies are directly evaluated in the simulation environments. Moreover, in real-world situations, not all recommendation problems are suitable to be transformed into reinforcement learning problems. Unlike previous academic RL research, RL-based RS also suffers from extrapolation error and the difficulties of being well-validated before deployment.
        In this paper, we introduce the RL4RS (Reinforcement Learning for Recommender Systems) benchmark dataset - a new resource fully collected from industrial applications to train and evaluate RL algorithms with special concerns on the above issues. It contains two datasets, tuned simulation environments, related advanced RL baselines, data understanding tools, and counterfactual policy evaluation algorithms. The RL4RS suite can be found at https://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender systems, we expect the resource to contribute to research in applied reinforcement learning and neural combinatorial optimization.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=euli0I5CKvy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/fuxiAIlab/RL4RS" target="_blank" rel="nofollow noreferrer">https://github.com/fuxiAIlab/RL4RS</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/fuxiAIlab/RL4RS
        All datasets are available for download from Zenodo https://zenodo.org/record/6622390#.YqBBpRNBxQK or Github https://github.com/fuxiAIlab/RL4RS
        The dataset DOI is 10.5281/zenodo.6622390.
        The dataset's meta-data page using Web standards is https://github.com/fuxiAIlab/RL4RS/blob/main/index.html
        The corresponding code for dataset processing and model training is maintained at https://github.com/fuxiAIlab/RL4RS</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our dataset is distributed under the CC BY-SA 4.0 license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="cSB_N-gHgu" data-number="72">
        <h4>
          <a href="/forum?id=cSB_N-gHgu">
              The Game of Hidden Rules: A New Kind of Benchmark Challenge for Machine Learning
          </a>


            <a href="/pdf?id=cSB_N-gHgu" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Eric_Pulick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Pulick1">Eric Pulick</a>, <a href="/profile?id=~Shubham_Kumar_Bharti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shubham_Kumar_Bharti1">Shubham Kumar Bharti</a>, <a href="/profile?id=~Yiding_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiding_Chen1">Yiding Chen</a>, <a href="/profile?id=~Vladimir_Menkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladimir_Menkov1">Vladimir Menkov</a>, <a href="/profile?id=~Yonatan_Dov_Mintz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonatan_Dov_Mintz1">Yonatan Dov Mintz</a>, <a href="/profile?id=~Paul_Kantor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Kantor1">Paul Kantor</a>, <a href="/profile?id=~Vicki_M._Bier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vicki_M._Bier1">Vicki M. Bier</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">30 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#cSB_N-gHgu-details-309" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cSB_N-gHgu-details-309"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">dataset, benchmark, rule learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We present a new machine learning benchmark environment which can be used to rigorously study how characteristics of learning tasks affect difficulty.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">As machine learning (ML) is more tightly woven into society, it is imperative that we better characterize ML's strengths and limitations if we are to employ it responsibly. Existing benchmark environments for ML, such as board and video games, offer well-defined benchmarks for progress, but constituent tasks are often complex, and it is frequently unclear how task characteristics contribute to overall difficulty for the machine learner. Likewise, without a systematic assessment of how task characteristics influence difficulty, it is challenging to draw meaningful connections between performance in different benchmark environments. We introduce a novel benchmark environment that offers an enormous range of ML challenges and enables precise examination of how task elements influence practical difficulty. The tool frames learning tasks as a ``board-clearing game,'' which we call the Game of Hidden Rules (GOHR). The environment comprises an expressive rule language and a captive server environment that can be installed locally. We propose a set of benchmark rule-learning tasks and plan to support a performance leader-board for researchers interested in attempting to learn our rules. GOHR complements existing environments by allowing fine, controlled modifications to tasks, enabling experimenters to better understand how each facet of a given learning task contributes to its practical difficulty for an arbitrary ML algorithm.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=cSB_N-gHgu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="http://sapir.psych.wisc.edu:7150/w2020/captive.html" target="_blank" rel="nofollow noreferrer">http://sapir.psych.wisc.edu:7150/w2020/captive.html</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">N/A, see public URL.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">N/A</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License, Version 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="NAYoSV3tk9" data-number="71">
        <h4>
          <a href="/forum?id=NAYoSV3tk9">
              VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation
          </a>


            <a href="/pdf?id=NAYoSV3tk9" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Kaizhi_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaizhi_Zheng1">Kaizhi Zheng</a>, <a href="/profile?id=~Xiaotong_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaotong_Chen2">Xiaotong Chen</a>, <a href="/profile?id=~Odest_Jenkins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Odest_Jenkins1">Odest Jenkins</a>, <a href="/profile?id=~Xin_Eric_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Eric_Wang2">Xin Eric Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">30 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#NAYoSV3tk9-details-163" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NAYoSV3tk9-details-163"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Vision and Language, Robotics, Multimodal Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">VLMbench is the first benchmark that compositional designs for vision-and-language reasoning and categorizes the manipulation tasks from the perspectives of task constraints.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents---object manipulation by following human guidance, e.g., “move the red mug next to the box while keeping it upright.” To this end, we introduce an Automatic Manipulation Solver (AMSolver) simulator and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=NAYoSV3tk9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/ucsc.edu/vlmbench/home" target="_blank" rel="nofollow noreferrer">https://sites.google.com/ucsc.edu/vlmbench/home</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://sites.google.com/ucsc.edu/vlmbench/home" target="_blank" rel="nofollow noreferrer">https://sites.google.com/ucsc.edu/vlmbench/home</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The codes and dataset are under MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="iJAwfSj6i8L" data-number="70">
        <h4>
          <a href="/forum?id=iJAwfSj6i8L">
              More Than Skin-Deep: Towards a Multidimensional Measure of Skin Color
          </a>


            <a href="/pdf?id=iJAwfSj6i8L" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~William_Thong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Thong1">William Thong</a>, <a href="/profile?id=~Przemyslaw_Joniak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Przemyslaw_Joniak1">Przemyslaw Joniak</a>, <a href="/profile?id=~Alice_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alice_Xiang1">Alice Xiang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">30 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#iJAwfSj6i8L-details-210" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iJAwfSj6i8L-details-210"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper strives to measure skin color, beyond a unidimensional scale on skin tone.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper strives to measure skin color, beyond a unidimensional scale on skin tone. In their seminal paper Gender Shades, Buolamwini and Gebru have shown how gender classification systems can be biased against women with darker skin tones. Since then, bias on the basis of skin tone has become one of the most common dimensions evaluated by fairness researchers and practitioners to benchmark bias in computer vision systems. While the Fitzpatick skin type classification is commonly used to measure skin color, it only focuses on the skin tone ranging from light to dark. Towards a more comprehensive measure of skin color, we introduce the hue angle ranging from red to yellow. When applied to facial images, the skin hue dimension reveals additional biases related to skin color in datasets, as well as in several computer vision models. We then recommend future fairness benchmarks, not only to measure the tone, but also the hue of the skin.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=iJAwfSj6i8L&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="u46CbCaLufp" data-number="68">
        <h4>
          <a href="/forum?id=u46CbCaLufp">
              Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models
          </a>


            <a href="/pdf?id=u46CbCaLufp" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Maribeth_Rauh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maribeth_Rauh1">Maribeth Rauh</a>, <a href="/profile?id=~John_F_J_Mellor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_F_J_Mellor1">John F J Mellor</a>, <a href="/profile?id=~Jonathan_Uesato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Uesato1">Jonathan Uesato</a>, <a href="/profile?id=~Po-Sen_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Po-Sen_Huang1">Po-Sen Huang</a>, <a href="/profile?id=~Johannes_Welbl2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Welbl2">Johannes Welbl</a>, <a href="/profile?id=~Laura_Weidinger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laura_Weidinger1">Laura Weidinger</a>, <a href="/profile?id=~Sumanth_Dathathri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sumanth_Dathathri1">Sumanth Dathathri</a>, <a href="/profile?id=~Amelia_Glaese1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amelia_Glaese1">Amelia Glaese</a>, <a href="/profile?id=~Geoffrey_Irving2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Geoffrey_Irving2">Geoffrey Irving</a>, <a href="/profile?id=~Iason_Gabriel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iason_Gabriel1">Iason Gabriel</a>, <a href="/profile?id=~William_Isaac1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Isaac1">William Isaac</a>, <a href="/profile?id=~Lisa_Anne_Hendricks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lisa_Anne_Hendricks1">Lisa Anne Hendricks</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">29 May 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#u46CbCaLufp-details-469" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="u46CbCaLufp-details-469"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">evaluation, NLP, language models, ethics, fairness, benchmark, toxicity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Large language models produce human-like text that drive a growing number of applications.
        However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful.
        Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward.
        To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks.
        We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks.
        Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=u46CbCaLufp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="SyoUVEyzJbE" data-number="66">
        <h4>
          <a href="/forum?id=SyoUVEyzJbE">
              MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control
          </a>


            <a href="/pdf?id=SyoUVEyzJbE" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xuehai_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuehai_Pan1">Xuehai Pan</a>, <a href="/profile?id=~Mickel_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mickel_Liu1">Mickel Liu</a>, <a href="/profile?id=~fangwei_zhong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~fangwei_zhong1">fangwei zhong</a>, <a href="/profile?id=~Yaodong_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaodong_Yang1">Yaodong Yang</a>, <a href="/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>, <a href="/profile?id=~Yizhou_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhou_Wang1">Yizhou Wang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">29 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#SyoUVEyzJbE-details-655" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SyoUVEyzJbE-details-655"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent Reinforcement Learning, Multi-Camera System, Asymmetric Game, Multi-agent Cooperative-Competitive Game</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">Agile transporters against a smart camera network competing in a multi-agent cooperative-competitive game</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We introduce the Multi-Agent Tracking Environment (MATE), a novel game environment that simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents--"cameras" and "targets"-- with opposing interests.  Specifically, "cameras", a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, "targets" are mobile agents that aim to transport cargoes between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. MATE provides OpenAI Gym API with progressive difficulty levels, and benchmarks the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. To showcase the practicality of MATE, we start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and additionally augmented with notable protocols for multi-agent communication (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious play) in the asymmetric competitive game, which is hardly considered in previous works. Such learned adversarial target policies can not only further evaluate the robustness of the cameras, but also improve the performance of cameras by jointly training the two competitive teams with PRSO. We also observe that different roles of targets are emergent while incorporating I2C in target-target communication. Our project is released at https://github.com/UnrealTracking/mate. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=SyoUVEyzJbE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/UnrealTracking/mate" target="_blank" rel="nofollow noreferrer">https://github.com/UnrealTracking/mate</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Source Code: https://github.com/UnrealTracking/mate
        Documentation: https://mate-gym.readthedocs.io</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">MIT License</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="iC-oKZHkp2d" data-number="65">
        <h4>
          <a href="/forum?id=iC-oKZHkp2d">
              Vision meets mmWave Radar: 3D Object Perception Benchmark for Robust Autonomous Driving
          </a>


            <a href="/pdf?id=iC-oKZHkp2d" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yizhou_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhou_Wang4">Yizhou Wang</a>, <a href="/profile?id=~Jen-Hao_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jen-Hao_Cheng1">Jen-Hao Cheng</a>, <a href="/profile?id=~Jui-Te_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jui-Te_Huang1">Jui-Te Huang</a>, <a href="/profile?id=~Qiqian_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiqian_Fu1">Qiqian Fu</a>, <a href="/profile?id=~Chiming_Ni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chiming_Ni1">Chiming Ni</a>, <a href="/profile?id=~Shengyu_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyu_Hao1">Shengyu Hao</a>, <a href="/profile?id=~Gaoang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaoang_Wang2">Gaoang Wang</a>, <a href="/profile?id=~Guanbin_Xing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guanbin_Xing1">Guanbin Xing</a>, <a href="/profile?id=~Hui_Liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hui_Liu4">Hui Liu</a>, <a href="/profile?id=~Jenq-Neng_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jenq-Neng_Hwang1">Jenq-Neng Hwang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">29 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#iC-oKZHkp2d-details-100" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iC-oKZHkp2d-details-100"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Object Detection, 3D Object Detection, Multi-Object Tracking, Autonomous Driving, Sensor Fusion</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This paper introduces a multi-modality sensing dataset with raw radio frequency (RF) images from the mmWave radar for 3D object detection and 3D multi-object tracking (MOT).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Sensor fusion is crucial for an accurate and robust perception system on autonomous vehicles. Most existing datasets and perception solutions focus on fusing cameras and LiDAR. However, the collaboration between camera and radar is significantly under-exploited. The incorporation of rich semantic information from the camera, and reliable 3D information from the radar can potentially achieve an efficient, cheap, and portable solution for 3D object perception tasks. It can also be robust to different lighting or all-weather driving scenarios due to the capability of mmWave radars. In this paper, we introduce the CRUW2022 dataset, including 66K synchronized and well-calibrated camera, radar, and LiDAR frames in various driving scenarios. Unlike other large-scale autonomous driving datasets, our radar data is in the format of radio frequency (RF) images that contain not only 3D location information but also spatio-temporal semantic information. This kind of radar format can enable machine learning models to generate more reliable object perception results after interacting and fusing the information or features between the camera and radar. To the best of our knowledge, the CRUW2022 dataset is the first public dataset with radar RF images to address 3D object perception by camera-radar fusion with 3D bounding box annotations.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=iC-oKZHkp2d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.cruwdataset.org/" target="_blank" rel="nofollow noreferrer">https://www.cruwdataset.org/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">Full access to the dataset will be available later at: https://www.cruwdataset.org/.
        We release a sample dataset for paper reviewing purposes for the current stage: https://drive.google.com/drive/folders/10WiHaw1u417a1CRaETtXHnFUT7LdY78g?usp=sharing.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The full dataset will be released around December 2022.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC
        Creative Commons Attribution-NonCommercial 4.0 International</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="QgTZ56-zJou" data-number="64">
        <h4>
          <a href="/forum?id=QgTZ56-zJou">
              PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding
          </a>


            <a href="/pdf?id=QgTZ56-zJou" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Minghao_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minghao_Xu1">Minghao Xu</a>, <a href="/profile?id=~Zuobai_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zuobai_Zhang1">Zuobai Zhang</a>, <a href="/profile?id=~Jiarui_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiarui_Lu2">Jiarui Lu</a>, <a href="/profile?id=~Zhaocheng_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaocheng_Zhu1">Zhaocheng Zhu</a>, <a href="/profile?id=~Yangtian_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangtian_Zhang1">Yangtian Zhang</a>, <a href="/profile?id=~Ma_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ma_Chang1">Ma Chang</a>, <a href="/profile?id=~Runcheng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Runcheng_Liu1">Runcheng Liu</a>, <a href="/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Tang1">Jian Tang</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">29 May 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#QgTZ56-zJou-details-129" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QgTZ56-zJou-details-129"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Protein Modeling Benchmark, Protein Sequence Understanding, Multi-Task Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">This work proposes a comprehensive and multi-task benchmark for protein sequence understanding, which studies both single-task and multi-task learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark will be open-sourced soon.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=QgTZ56-zJou&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">We are doing the final code refinement and website construction. All the source code of PEER benchmark and the corresponding website will be public by next month at latest. During the review process, we will provide a private URL through an official comment to let reviewers access our source code.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">All source code of the PEER benchmark will be licensed under the Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="WST-8SwyxnI" data-number="63">
        <h4>
          <a href="/forum?id=WST-8SwyxnI">
              ReCo: A Dataset for Residential Community Layout Planning
          </a>


            <a href="/pdf?id=WST-8SwyxnI" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Xi_Chen29" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_Chen29">Xi Chen</a>, <a href="/profile?id=~Yun_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun_Xiong1">Yun Xiong</a>, <a href="/profile?id=~Siqi_Wang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siqi_Wang5">Siqi Wang</a>, <a href="/profile?id=~Haofen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haofen_Wang1">Haofen Wang</a>, <a href="/profile?id=~Tao_Sheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Sheng2">Tao Sheng</a>, <a href="/profile?id=~Yao_Zhang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao_Zhang6">Yao Zhang</a>, <a href="/profile?id=~Yu_Ye2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Ye2">Yu Ye</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">28 May 2022 (modified: 12 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#WST-8SwyxnI-details-719" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WST-8SwyxnI-details-719"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Residential Community, Layout Planning, Intelligent Design, Generative Model</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We introduce a dataset for Residential Community Layout Planning, namely ReCo Dataset, to facilitate intelligent architecture design.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we introduce Residential Community Layout Planning (ReCo) Dataset, which is the first and largest open-source vector dataset related to real-world community to date. ReCo Dataset is presented in multiple data formats with 37,646 residential community layout plans, covering 598,728 residential buildings with height information. ReCo can be conveniently adapted for residential community layout related urban design tasks, e.g., generative layout design, morphological pattern recognition and spatial evaluation. To validate the utility of ReCo in automated residential community layout planning, a Generative Adversarial Network (GAN) based generative model is further applied to the dataset. We expect ReCo Dataset to inspire more creative and practical work in intelligent design and beyond. The ReCo Dataset is published at: https://www.kaggle.com/fdudsde/reco-dataset and related code can be found at: https://github.com/FDUDSDE/ReCo-Dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=WST-8SwyxnI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://www.kaggle.com/fdudsde/reco-dataset" target="_blank" rel="nofollow noreferrer">https://www.kaggle.com/fdudsde/reco-dataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The ReCo Dataset is published at: https://www.kaggle.com/fdudsde/reco-dataset, and related code can be found at: https://github.com/FDUDSDE/ReCo-Dataset.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Dataset is released under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License.
        Related code is under the MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="yWhuIjIjH8k" data-number="61">
        <h4>
          <a href="/forum?id=yWhuIjIjH8k">
              NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies
          </a>


            <a href="/pdf?id=yWhuIjIjH8k" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Arjun_Krishnakumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arjun_Krishnakumar1">Arjun Krishnakumar</a>, <a href="/profile?id=~Colin_White1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colin_White1">Colin White</a>, <a href="/profile?id=~Arber_Zela1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arber_Zela1">Arber Zela</a>, <a href="/profile?id=~Renbo_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renbo_Tu1">Renbo Tu</a>, <a href="/profile?id=~Mahmoud_Safari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mahmoud_Safari1">Mahmoud Safari</a>, <a href="/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">28 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#yWhuIjIjH8k-details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yWhuIjIjH8k-details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">AutoML, Neural Architecture Search, Zero-Cost Proxies</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We create a benchmark suite for zero-cost proxies, and we use it to show how to effectively combine them to improve performance.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Zero-cost proxies (ZC proxies) are a recent architecture performance prediction technique aiming to significantly speed up algorithms for neural architecture search (NAS). Recent work has shown that these techniques show great promise, but certain aspects, such as evaluating and exploiting their complementary strengths, are under-studied. In this work, we create NAS-Bench-Suite-Zero: we evaluate 13 ZC proxies across 28 tasks, creating by far the largest dataset (and unified codebase) for ZC proxies, enabling orders-of-magnitude faster experiments on ZC proxies, while avoiding confounding factors stemming from different implementations. To demonstrate the usefulness of NAS-Bench-Suite-Zero, we run a large-scale analysis of ZC proxies, including the first information-theoretic analysis, concluding that they capture substantial complementary information. Motivated by these findings, we show that incorporating all 13 ZC proxies into the surrogate models used by NAS algorithms can improve their predictive performance by up to 42%. Our code and datasets are available at https://github.com/automl/naslib/tree/zerocost.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=yWhuIjIjH8k&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/automl/naslib/tree/zerocost" target="_blank" rel="nofollow noreferrer">https://github.com/automl/naslib/tree/zerocost</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/automl/naslib/tree/zerocost" target="_blank" rel="nofollow noreferrer">https://github.com/automl/naslib/tree/zerocost</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Apache License 2.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="wK574FI7XdD" data-number="60">
        <h4>
          <a href="/forum?id=wK574FI7XdD">
              DVM-CAR: A Large-Scale Automotive Dataset for Visual Marketing Research and Applications
          </a>


            <a href="/pdf?id=wK574FI7XdD" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Jingmin_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingmin_Huang1">Jingmin Huang</a>, <a href="/profile?id=~Bowei_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bowei_Chen1">Bowei Chen</a>, <a href="/profile?email=lluo%40marshall.usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lluo@marshall.usc.edu">Lan Luo</a>, <a href="/profile?email=syue%40lincoln.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="syue@lincoln.ac.uk">Shigang Yue</a>, <a href="/profile?id=~Iadh_Ounis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iadh_Ounis1">Iadh Ounis</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">27 May 2022 (modified: 06 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#wK574FI7XdD-details-552" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wK574FI7XdD-details-552"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Dataset, Car, Automotive, Aesthetic, Deep Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">The paper introduces a large-scale automotive dataset that contains images, text, and sales information.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">We present our multidisciplinary initiative of creating a publicly available dataset to facilitate visual marketing research and applications in the automotive industry, such as automotive exterior design, consumer analytics, and sales modeling. This study is motivated by the fact that there is growing interest in product aesthetics analytics and design but there is no large-scale dataset available that covers a wide range of variables and information. Our dataset contains 1.4 million images from 899 car models and their corresponding model specification and sales information over more than ten years in the UK market. To the best of our knowledge, this is the first large-scale automotive dataset that contains images, text, and sales information from multiple sources over a long period. Our study contributes to big data, domain application, database design, and data fusion. Three application examples are discussed to illustrate the usage and value of our dataset. </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=wK574FI7XdD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://deepvisualmarketing.github.io/" target="_blank" rel="nofollow noreferrer">https://deepvisualmarketing.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">The DVM-CAR 2.0 is publicly available at: https://deepvisualmarketing.github.io. This GitHub webpage provides data overview, usage statement, download links, timeline, and contact information of dataset developers.

        Users can directly download the dataset from the provided Dropbox links. Besides this, this dataset is also hosted on Figshare with a persistent DOI https://doi.org/10.6084/m9.figshare.19586296.v1. </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">This dataset is under the CC BY-NC license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="JELKKMF2PDN" data-number="59">
        <h4>
          <a href="/forum?id=JELKKMF2PDN">
              End to End Software Engineering Research
          </a>


            <a href="/pdf?id=JELKKMF2PDN" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?email=dan.amit%40mail.huji.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="dan.amit@mail.huji.ac.il">Idan Amit</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">27 May 2022 (modified: 27 May 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#JELKKMF2PDN-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JELKKMF2PDN-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">software engineering, end to end learning, causality, code similarity. code difficulty, co-change</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">5M source code files extracted every 2 months for a year with process metrics to enable end-to-end learning in software engineering </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">End to end learning is machine learning starting in raw data and predicting a desired concept, with all steps done automatically.
        In software engineering context, we see it as starting from the source code and predicting process metrics.
        This framework can be used for predicting defects, code quality, productivity and more.
        End-to-end improves over features based machine learning by not requiring domain experts and being able to extract new knowledge.
        We describe a dataset of 5M files from 15k projects constructed for this goal.
        The dataset is constructed in a way that enables not only predicting concepts but also investigating their causes.</span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/evidencebp/e2ese/" target="_blank" rel="nofollow noreferrer">https://github.com/evidencebp/e2ese/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://github.com/evidencebp/e2ese/

        The datasets are separated into different repositories for different languages in different dates.
        For example, the Java dataset extracted in February 2022 is at
        https://github.com/evidencebp/e2ese-dataset-java-february-2022

        See links to repositories at
        https://github.com/evidencebp/e2ese/tree/main/data

        The June 2022 datasets are not ready yet.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="ml1NjI-ujzf" data-number="58">
        <h4>
          <a href="/forum?id=ml1NjI-ujzf">
              Active-Passive SimStereo - Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods
          </a>


            <a href="/pdf?id=ml1NjI-ujzf" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Laurent_Valentin_Jospin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurent_Valentin_Jospin1">Laurent Valentin Jospin</a>, <a href="/profile?id=~Allen_Antony1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Allen_Antony1">Allen Antony</a>, <a href="/profile?id=~Lian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lian_Xu1">Lian Xu</a>, <a href="/profile?id=~Hamid_Laga3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hamid_Laga3">Hamid Laga</a>, <a href="/profile?id=~Farid_Boussaid1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Farid_Boussaid1">Farid Boussaid</a>, <a href="/profile?id=~Mohammed_Bennamoun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammed_Bennamoun1">Mohammed Bennamoun</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">26 May 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#ml1NjI-ujzf-details-197" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ml1NjI-ujzf-details-197"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Active Stereo, Dataset, Deep Learning, Generalization, Stereo Vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose the first dataset of active+passive stereo images to evaluate the generalisation ability of stereo deep learning models.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of ten selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of two of the ten architectures (CascadeStereo and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.

        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=ml1NjI-ujzf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://ieee-dataport.org/open-access/active-passive-simstereo" target="_blank" rel="nofollow noreferrer">https://ieee-dataport.org/open-access/active-passive-simstereo</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://ieee-dataport.org/open-access/active-passive-simstereo" target="_blank" rel="nofollow noreferrer">https://ieee-dataport.org/open-access/active-passive-simstereo</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The dataset is already available under a CC-license</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Creative Commons Attribution [https://creativecommons.org/licenses/by/4.0/]</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="yCZRdI0Y7G" data-number="57">
        <h4>
          <a href="/forum?id=yCZRdI0Y7G">
              Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization
          </a>


            <a href="/pdf?id=yCZRdI0Y7G" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Wenhao_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhao_Gao1">Wenhao Gao</a>, <a href="/profile?id=~Tianfan_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianfan_Fu1">Tianfan Fu</a>, <a href="/profile?id=~Jimeng_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimeng_Sun3">Jimeng Sun</a>, <a href="/profile?id=~Connor_W._Coley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Connor_W._Coley1">Connor W. Coley</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">26 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#yCZRdI0Y7G-details-348" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yCZRdI0Y7G-details-348"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization---the number of molecules evaluated by the oracle---is rarely discussed, despite being an essential consideration for realistic discovery applications.

        To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 tasks with a particular focus on sample efficiency. Our results show that most ``state-of-the-art'' methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=yCZRdI0Y7G&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/wenhao-gao/mol_opt" target="_blank" rel="nofollow noreferrer">https://github.com/wenhao-gao/mol_opt</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">We used ZINC dataset from Sterling, Teague, and John J. Irwin. “ZINC 15–ligand discovery for everyone.” Journal of chemical information and modeling 55.11 (2015): 2324-2337, is free to use for everyone. Redistribution of significant subsets requires written permission from the authors.

        Our benchmark is under MIT license.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="eJhc_CPXQIT" data-number="56">
        <h4>
          <a href="/forum?id=eJhc_CPXQIT">
              MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing
          </a>


            <a href="/pdf?id=eJhc_CPXQIT" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Zelun_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zelun_Luo2">Zelun Luo</a>, <a href="/profile?id=~Zane_Durante1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zane_Durante1">Zane Durante</a>, <a href="/profile?id=~Linden_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linden_Li1">Linden Li</a>, <a href="/profile?id=~Wanze_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wanze_Xie1">Wanze Xie</a>, <a href="/profile?email=ruochenl%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruochenl@stanford.edu">Ruochen Liu</a>, <a href="/profile?id=~Emily_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emily_Jin1">Emily Jin</a>, <a href="/profile?id=~Zhuoyi_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuoyi_Huang1">Zhuoyi Huang</a>, <a href="/profile?email=tinally%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tinally@stanford.edu">Lun Yu Li</a>, <a href="/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>, <a href="/profile?id=~Juan_Carlos_Niebles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juan_Carlos_Niebles1">Juan Carlos Niebles</a>, <a href="/profile?id=~Ehsan_Adeli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ehsan_Adeli1">Ehsan Adeli</a>, <a href="/profile?id=~Li_Fei-Fei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Fei-Fei1">Li Fei-Fei</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">26 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">4 Replies</span>


        </div>

          <a href="#eJhc_CPXQIT-details-648" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eJhc_CPXQIT-details-648"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">activity recognition, video-language model, video understanding, fine-grained activity recognition, scene graph generation, temporal action detection</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose a new dataset and framework for evaluating video-language models on activity recognition at multiple levels of granularity</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, sub-activity, and atomic action level.  We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on few-shot activity parsing, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=eJhc_CPXQIT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/d1ngn1gefe1/momatools" target="_blank" rel="nofollow noreferrer">https://github.com/d1ngn1gefe1/momatools</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/d1ngn1gefe1/momatools" target="_blank" rel="nofollow noreferrer">https://github.com/d1ngn1gefe1/momatools</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our work is released under the license CC-BY.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="sWOdnSkB0qu" data-number="55">
        <h4>
          <a href="/forum?id=sWOdnSkB0qu">
              MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control
          </a>


            <a href="/pdf?id=sWOdnSkB0qu" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Nolan_Wagener1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nolan_Wagener1">Nolan Wagener</a>, <a href="/profile?id=~Andrey_Kolobov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Kolobov1">Andrey Kolobov</a>, <a href="/profile?id=~Felipe_Vieira_Frujeri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felipe_Vieira_Frujeri1">Felipe Vieira Frujeri</a>, <a href="/profile?id=~Ricky_Loynd1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ricky_Loynd1">Ricky Loynd</a>, <a href="/profile?id=~Ching-An_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ching-An_Cheng1">Ching-An Cheng</a>, <a href="/profile?id=~Matthew_Hausknecht1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Hausknecht1">Matthew Hausknecht</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">26 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">7 Replies</span>


        </div>

          <a href="#sWOdnSkB0qu-details-388" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sWOdnSkB0qu-details-388"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">mocap, motion capture, hierarchical reinforcement learning, humanoid control, task transfer, motion completion</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We release a dataset of experts and rollouts for tracking 3.5 hours of MoCap data in dm_control.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Control of simulated humanoid characters is a challenging benchmark for sequential decision-making methods, as it assesses a policy’s ability to drive an inherently unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can be used to generate high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct, a dataset of these expert agents and their rollouts containing proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn high-level other tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can perform natural motion completion given a motion prompt.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=sWOdnSkB0qu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://mhauskn.github.io/mocapact.github.io/" target="_blank" rel="nofollow noreferrer">https://mhauskn.github.io/mocapact.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://mhauskn.github.io/mocapact.github.io/" target="_blank" rel="nofollow noreferrer">https://mhauskn.github.io/mocapact.github.io/</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-SA</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="KGw8-SbULf6" data-number="54">
        <h4>
          <a href="/forum?id=KGw8-SbULf6">
              GoBigger: A Scalable Platform for Multi-Agent Interactive Simulation
          </a>


            <a href="/pdf?id=KGw8-SbULf6" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Ming_Zhang10" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming_Zhang10">Ming Zhang</a>, <a href="/profile?id=~Chao_Yang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Yang3">Chao Yang</a>, <a href="/profile?id=~Zhenjie_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenjie_Yang1">Zhenjie Yang</a>, <a href="/profile?id=~Jinliang_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinliang_Zheng1">Jinliang Zheng</a>, <a href="/profile?id=~Chuming_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuming_Li3">Chuming Li</a>, <a href="/profile?id=~Yazhe_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yazhe_Niu1">Yazhe Niu</a>, <a href="/profile?id=~Yu_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Liu2">Yu Liu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">3 Replies</span>


        </div>

          <a href="#KGw8-SbULf6-details-942" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KGw8-SbULf6-details-942"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Multi-agent Environment, MARL, Swarm intelligence</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">gobigger: the scalable platform for multi-agent interactive simulation</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The emergence of various multi-agent environments motivates powerful algorithms to explore agents' cooperation or competition separately.
        It significantly promotes the development of multi-agent reinforcement learning (MARL). However, fewer methods focus on the behavior of swarm intelligence between multiple teams. To fill the gap, we introduce GoBigger, the scalable platform for multi-agent interactive simulation. GoBigger is an expansion environment for the Agar-like game, enabling the simulation of multiple scales of agent intra-team coordination and inter-team confrontation. Compared with the existing multi-agent simulation environment, our platform supports multi-team online games with more than two teams simultaneously, which dramatically expands the way of agent cooperation and competition and can more effectively simulate the swarm intelligent agent behavior. By leveraging the full set of the visible range of entities, a simple state encoding method is presented to verify the performance of the baseline algorithm. A ladder evaluation system is delivered for users to verify the game level of the agent. We believe this platform can lead to various emerging research directions in MARL, swarm intelligence, and large-scale agent interactive learning.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=KGw8-SbULf6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/opendilab/GoBigger" target="_blank" rel="nofollow noreferrer">https://github.com/opendilab/GoBigger</a></span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="cIvjVCE1V3d" data-number="53">
        <h4>
          <a href="/forum?id=cIvjVCE1V3d">
              A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics
          </a>


            <a href="/pdf?id=cIvjVCE1V3d" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Qing_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qing_Li1">Qing Li</a>, <a href="/profile?id=~Siyuan_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyuan_Huang2">Siyuan Huang</a>, <a href="/profile?id=~Yining_Hong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yining_Hong1">Yining Hong</a>, <a href="/profile?id=~Yixin_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yixin_Zhu1">Yixin Zhu</a>, <a href="/profile?id=~Ying_Nian_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Nian_Wu1">Ying Nian Wu</a>, <a href="/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#cIvjVCE1V3d-details-499" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cIvjVCE1V3d-details-499"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Systematic Generalization, Concept Learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We take inspiration from arithmetic and present a new benchmark for studying systematic generalization of perception, syntax, and semantics.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Inspired by humans' remarkable ability to master arithmetic and generalize to unseen problems, we present a new dataset, HINT, to study machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. Learning agents are tasked to reckon how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. With a focus on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. We further design a few-shot learning split to test whether models could quickly learn new concepts and generalize them to more complex scenarios. To understand existing models' limitations, we conduct extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results suggest that current models still struggle in extrapolation to long-range syntactic dependency and semantics. Models show a significant gap toward human-level generalization when tested with new concepts in a few-shot setting. Moreover, we find that it is infeasible to solve HINT by simply scaling up the dataset and the model size; this strategy barely helps the extrapolation over syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting shows impressive results and significantly boosts the test accuracy. We believe the proposed dataset together with the experimental findings is of great interest to the community on systematic generalization.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=cIvjVCE1V3d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://liqing-ustc.github.io/HINT" target="_blank" rel="nofollow noreferrer">https://liqing-ustc.github.io/HINT</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://liqing-ustc.github.io/HINT" target="_blank" rel="nofollow noreferrer">https://liqing-ustc.github.io/HINT</a></span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC BY-NC 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="_vSn5XxGRnG" data-number="52">
        <h4>
          <a href="/forum?id=_vSn5XxGRnG">
              SCAMPS: Synthetics for Camera Measurement of Physiological Signals
          </a>


            <a href="/pdf?id=_vSn5XxGRnG" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Daniel_McDuff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_McDuff1">Daniel McDuff</a>, <a href="/profile?email=miah%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="miah@microsoft.com">Miah Wander</a>, <a href="/profile?id=~Xin_Liu8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Liu8">Xin Liu</a>, <a href="/profile?id=~Brian_L._Hill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_L._Hill1">Brian L. Hill</a>, <a href="/profile?id=~Javier_Hernandez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_Hernandez1">Javier Hernandez</a>, <a href="/profile?email=jlester%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jlester@microsoft.com">Jonathan Lester</a>, <a href="/profile?id=~Tadas_Baltrusaitis2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tadas_Baltrusaitis2">Tadas Baltrusaitis</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#_vSn5XxGRnG-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_vSn5XxGRnG-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Synthetics, Dataset, Health, Physiology, Computer Vision</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">SCAMPS is a dataset of high-fidelity synthetics containing 2,800 videos (1.68M frames) of avatars with aligned cardiac and respiratory signals and facial action intensities.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">The use of cameras and computational algorithms for noninvasive, low-cost and scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs is very attractive. However, diverse data representing a range of environments, body motions, illumination conditions and physiological states is laborious, time consuming and expensive to obtain. Synthetic data have proven a valuable tool in several areas of machine learning, yet are not widely available for camera measurement of physiological states. Synthetic data offer "perfect" labels (e.g., without noise and with precise synchronization), labels that may not be possible to obtain otherwise (e.g., precise pixel level segmentation maps) and provide a high degree of control over variation and diversity in the dataset.  We present SCAMPS, a dataset of synthetics containing 2,800 videos (1.68M frames) with aligned cardiac and respiratory signals and facial action intensities. The RGB frames are provided alongside segmentation maps. We provide precise descriptive statistics about the underlying waveforms, including inter-beat interval, heart rate variability, pulse arrival time. Finally, we present baseline results training on these synthetic data and testing on real-world datasets to illustrate generalizability.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=_vSn5XxGRnG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/danmcduff/scampsdataset" target="_blank" rel="nofollow noreferrer">https://github.com/danmcduff/scampsdataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value "><a href="https://github.com/danmcduff/scampsdataset" target="_blank" rel="nofollow noreferrer">https://github.com/danmcduff/scampsdataset</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">There is no dataset embargo.</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Research Use of Data Agreement v1.0

        This is the Research Use of Data Agreement, Version 1.0 (the “R-UDA”). Capitalized terms are defined in Section 5. Data Provider and you agree as follows:

        1. Provision of the Data
        1.1. You may use, modify, and distribute the Data made available to you by the Data Provider under this R-UDA for Research Use if you follow the R-UDA’s terms.
        1.2. Data Provider will not sue you or any Downstream Recipient for any claim arising out of the use, modification, or distribution of the Data provided you meet the terms of the R-UDA.
        1.3. This R-UDA does not restrict your use, modification, or distribution of any portions of the Data that are in the public domain or that may be used, modified, or distributed under any other legal exception or limitation.

        2. Restrictions
        2.1. You agree that you will use the Data solely for Computational Use for non-commercial research. This restriction means that you may engage in non-commercial research activities (including non-commercial research undertaken by or funded via a commercial entity), but you may not use the Data or any Results in any commercial offering, including as part of a product or service (or to improve any product or service) you use or provide to others.
        2.2. You may not receive money or other consideration in exchange for use or redistribution of Data.

        3. Redistribution of Data
        3.1. You may redistribute the Data, so long as:
        3.1.1. You include with any Data you redistribute all credit or attribution information that you received with the Data, and your terms require any Downstream Recipient to do the same; and
        3.1.2. You bind each recipient to whom you redistribute the Data to the terms of the R-UDA.

        4. No Warranty, Limitation of Liability
        4.1. Data Provider does not represent or warrant that it has any rights whatsoever in the Data.
        4.2. THE DATA IS PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
        4.3. NEITHER DATA PROVIDER NOR ANY UPSTREAM DATA PROVIDER SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE DATA OR RESULTS, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

        5. Definitions
        5.1. “Computational Use” means activities necessary to enable the use of Data (alone or along with other material) for analysis by a computer.
        5.2. “Data” means the material you receive under the R-UDA in modified or unmodified form, but not including Results.
        5.3. “Data Provider” means the source from which you receive the Data and with whom you enter into the R-UDA.
        5.4. “Downstream Recipient” means any person or persons who receives the Data directly or indirectly from you in accordance with the R-UDA.
        5.5. “Result” means anything that you develop or improve from your use of Data that does not include more than a de minimis portion of the Data on which the use is based. Results may include de minimis portions of the Data necessary to report on or explain use that has been conducted with the Data, such as figures in scientific papers, but do not include more. Artificial intelligence models trained on Data (and which do not include more than a de minimis portion of Data) are Results.
        5.6. “Upstream Data Providers” means the source or sources from which the Data Provider directly or indirectly received, under the terms of the R-UDA, material that is included in the Data.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="Zx3LIfB-v0K" data-number="51">
        <h4>
          <a href="/forum?id=Zx3LIfB-v0K">
              Dynamic stability of power grids - new datasets for Graph Neural Networks
          </a>


            <a href="/pdf?id=Zx3LIfB-v0K" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Christian_Nauck1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Nauck1">Christian Nauck</a>, <a href="/profile?email=mlindner%40pik-potsdam.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="mlindner@pik-potsdam.de">Michael Lindner</a>, <a href="/profile?id=~Konstantin_Sch%C3%BCrholt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantin_Schürholt1">Konstantin Schürholt</a>, <a href="/profile?id=~Frank_Hellmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hellmann1">Frank Hellmann</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 09 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#Zx3LIfB-v0K-details-942" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zx3LIfB-v0K-details-942"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">Graph Neural Networks, Dynamic Stability, Power Grids, Complex Systems, Nonlinear Dynamics, Energy Transition, Out-of-distribution generalization, Benchmarks</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We generate new datasets based on dynamical simulations of power grids as a challenge for Graph Neural Networks and include benchmark performances on different tasks including out-of-distribution generalization. </span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">One of the key challenges for the success of the energy transition towards renewable energies is the analysis of the dynamic stability of power grids. However, dynamic solutions are intractable and exceedingly expensive for large grids. Graph Neural Networks (GNNs) are a promising method to reduce the computational effort of predicting  dynamic stability of power grids, however datasets of appropriate complexity and size do not yet exist. We introduce two new datasets of synthetically generated power grids. For each grid, the dynamic stability has been estimated using Monte-Carlo simulations. The datasets have 10 times more grids than previously published. To evaluate the potential for real-world applications, we demonstrate the successful prediction on a Texan power grid model. We can significantly increase the performance to surprisingly high levels by training more complex models on more data. Furthermore, the investigated grids have different sizes, that enables the application of out-of-distribution evaluation and transfer learning from a small to a large domain. We invite the community to improve our benchmark models and thus aid the energy transition with better tools.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=Zx3LIfB-v0K&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/PIK-ICoNe/dynamic_stability_datasets_gnn_paper-companion.git" target="_blank" rel="nofollow noreferrer">https://github.com/PIK-ICoNe/dynamic_stability_datasets_gnn_paper-companion.git</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://zenodo.org/record/6572973

        https://github.com/PIK-ICoNe/dynamic_stability_datasets_gnn_paper-companion.git</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">The data is publicly available and licensed under the Creative Commons Attribution 4.0 International license (CC-BY 4.0)</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="F9ENmZABB0" data-number="50">
        <h4>
          <a href="/forum?id=F9ENmZABB0">
              Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time
          </a>


            <a href="/pdf?id=F9ENmZABB0" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Huaxiu_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huaxiu_Yao1">Huaxiu Yao</a>, <a href="/profile?id=~Caroline_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caroline_Choi1">Caroline Choi</a>, <a href="/profile?id=~Yoonho_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoonho_Lee1">Yoonho Lee</a>, <a href="/profile?id=~Pang_Wei_Koh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pang_Wei_Koh1">Pang Wei Koh</a>, <a href="/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chelsea_Finn1">Chelsea Finn</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#F9ENmZABB0-details-930" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F9ENmZABB0-details-930"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">temporal distribution shift, invariant learning, continual learning</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A new benchmark for in-the-wild distribution shift over time</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 7 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 9 approaches with various inductive biases. Specifically, we evaluate several domain-generalization and continual learning methods, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning methods. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 21% from in-distribution to out-of-distribution data.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=F9ENmZABB0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">Open Credentialized Access:</strong>
              <span class="note-content-value ">Our benchmark includes MIMIC dataset, which requires PhysioNet credentialing for use of human subject data. </span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">We will provide a private URL of the source code and the corresponding datasets through an official comment.</span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Embargo:</strong>
              <span class="note-content-value ">The source code will be public by next month at the latest. During the review process, we will provide a private URL of the source code and the corresponding datasets through an official comment.
        </span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Source code of the Wild-Time benchmark will use Apache License 2.0.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="MPrcwjMgL3d" data-number="48">
        <h4>
          <a href="/forum?id=MPrcwjMgL3d">
              Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery
          </a>


            <a href="/pdf?id=MPrcwjMgL3d" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Yoshitomo_Matsubara1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshitomo_Matsubara1">Yoshitomo Matsubara</a>, <a href="/profile?id=~Naoya_Chiba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Naoya_Chiba1">Naoya Chiba</a>, <a href="/profile?id=~Ryo_Igarashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryo_Igarashi1">Ryo Igarashi</a>, <a href="/profile?id=~Tatsunori_Taniai4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsunori_Taniai4">Tatsunori Taniai</a>, <a href="/profile?id=~Yoshitaka_Ushiku3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshitaka_Ushiku3">Yoshitaka Ushiku</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">25 May 2022 (modified: 16 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">6 Replies</span>


        </div>

          <a href="#MPrcwjMgL3d-details-858" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MPrcwjMgL3d-details-858"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">symbolic regression for scientific discovery, physics, datasets, benchmarks, Symbolic Transformer</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">We propose new datasets and evaluation metric to discuss the performance of symbolic regression for scientific discovery (SRSD).</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">This paper revisits datasets and evaluation criteria for Symbolic Regression, a task of expressing given data using mathematical equations, specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling range of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not a SR method con (re)discover physical laws from such datasets. As an evaluation metric, we also propose to use normalized edit distances between a predicted equation and the ground-truth equation trees. While existing metrics are either binary or errors between the target values and a SR model's predicted values for a given input, normalized edit distances evaluate a sort of similarity between the ground-truth and predicted equation trees. We have conducted experiments on our new SRSD datasets using five state-of-the-art SR methods in SRBench and a simple baseline based on a recent Transformer architecture. The results show that we provide a more realistic performance evaluation and open up a new machine learning-based approach for scientific discovery. Our datasets and code repository are publicly available.</span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=MPrcwjMgL3d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://github.com/omron-sinicx/srsd-benchmark" target="_blank" rel="nofollow noreferrer">https://github.com/omron-sinicx/srsd-benchmark</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_easy
        https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_medium
        https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_hard</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">Our datasets and code are published with MIT License.</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
    <li class="note " data-id="3lk54yE2tYJ" data-number="47">
        <h4>
          <a href="/forum?id=3lk54yE2tYJ">
              Geoclidean: Few-Shot Generalization in Euclidean Geometry
          </a>


            <a href="/pdf?id=3lk54yE2tYJ" class="pdf-link" title="Download PDF" target="_blank"><img src="/images/pdf_icon_blue.svg"></a>


        </h4>



        <div class="note-authors">
          <a href="/profile?id=~Joy_Hsu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joy_Hsu2">Joy Hsu</a>, <a href="/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>, <a href="/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>
        </div>

        <div class="note-meta-info">
          <span class="item date">24 May 2022 (modified: 15 Jun 2022)</span>
              <span class="item">NeurIPS 2022 Track Datasets and Benchmarks Submission</span>

            <span class="item readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>


            <span class="item">5 Replies</span>


        </div>

          <a href="#3lk54yE2tYJ-details-367" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3lk54yE2tYJ-details-367"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
            <li>
              <strong class="note-content-field">Keywords:</strong>
              <span class="note-content-value ">geometry, concept learning, few-shot generalization</span>
            </li>
            <li>
              <strong class="note-content-field">TL;DR:</strong>
              <span class="note-content-value ">A study of few-shot generalization of human and vision models in Euclidean geometry concepts.</span>
            </li>
            <li>
              <strong class="note-content-field">Abstract:</strong>
              <span class="note-content-value ">Euclidean geometry is among the earliest forms of mathematical thinking. While the geometric primitives underlying its constructions, such as perfect lines and circles, do not often occur in the natural world, humans rarely struggle to perceive and reason with them. Will computer vision models trained on natural images show the same sensitivity to Euclidean geometry? Here we explore these questions by studying few-shot generalization in the universe of Euclidean geometry constructions. We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. We find that humans are indeed sensitive to Euclidean geometry and generalize strongly from a few visual examples of a geometric concept. In contrast, low-level and high-level visual features from standard computer vision models pretrained on natural images do not support correct generalization. Thus Geoclidean represents a novel few-shot generalization benchmark for geometric concept learning, where the performance of humans and of AI models diverge. The Geoclidean framework and dataset are publicly available for download.
        </span>
            </li>
            <li>
              <strong class="note-content-field">Supplementary Material:</strong>
              <span class="note-content-value "><a href="/attachment?id=3lk54yE2tYJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;pdf</a></span>
            </li>
            <li>
              <strong class="note-content-field">URL:</strong>
              <span class="note-content-value "><a href="https://downloads.cs.stanford.edu/viscam/Geoclidean/geoclidean.zip" target="_blank" rel="nofollow noreferrer">https://downloads.cs.stanford.edu/viscam/Geoclidean/geoclidean.zip</a></span>
            </li>
            <li>
              <strong class="note-content-field">Dataset Url:</strong>
              <span class="note-content-value ">
        Framework: https://github.com/joyhsu0504/geoclidean_framework
        Dataset: https://downloads.cs.stanford.edu/viscam/Geoclidean/geoclidean.zip</span>
            </li>
            <li>
              <strong class="note-content-field">License:</strong>
              <span class="note-content-value ">CC-BY 4.0</span>
            </li>
            <li>
              <strong class="note-content-field">Author Statement:</strong>
              <span class="note-content-value ">Yes</span>
            </li>
        </ul>
        </div></div>




    </li>
</ul>